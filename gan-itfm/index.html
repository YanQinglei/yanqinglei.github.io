<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation | BulingQAQ的个人博客</title><meta name="author" content="BulingQAQ"><meta name="copyright" content="BulingQAQ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="0. Abstract  工作：  训练了不同的图像到图像转换模型，从语义分割图中合成带有或不带有卒中病灶的脑部磁共振图像。 训练了一个生成对抗网络来生成合成病灶掩码。 将两个组件结合起来，构建了一个大规模的合成卒中图像数据库。 使用U-Net对各种模型的性能进行评估，该网络在临床测试集上训练以分割卒中病灶。  结果：  这是首次基于图像到图像转换的合成数据增强对比分析，也是首次应用于缺血性卒中。">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation">
<meta property="og:url" content="https://yanqinglei.github.io/gan-itfm/">
<meta property="og:site_name" content="BulingQAQ的个人博客">
<meta property="og:description" content="0. Abstract  工作：  训练了不同的图像到图像转换模型，从语义分割图中合成带有或不带有卒中病灶的脑部磁共振图像。 训练了一个生成对抗网络来生成合成病灶掩码。 将两个组件结合起来，构建了一个大规模的合成卒中图像数据库。 使用U-Net对各种模型的性能进行评估，该网络在临床测试集上训练以分割卒中病灶。  结果：  这是首次基于图像到图像转换的合成数据增强对比分析，也是首次应用于缺血性卒中。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png">
<meta property="article:published_time" content="2025-03-05T06:24:58.000Z">
<meta property="article:modified_time" content="2025-03-05T06:27:39.591Z">
<meta property="article:author" content="BulingQAQ">
<meta property="article:tag" content="GAN">
<meta property="article:tag" content="ITM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png"><link rel="shortcut icon" href="/img/icon.png"><link rel="canonical" href="https://yanqinglei.github.io/gan-itfm/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-05 14:27:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/spy_family_avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png')"><nav id="nav"><span id="blog-info"><a href="/" title="BulingQAQ的个人博客"><span class="site-name">BulingQAQ的个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-05T06:24:58.000Z" title="发表于 2025-03-05 14:24:58">2025-03-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-05T06:27:39.591Z" title="更新于 2025-03-05 14:27:39">2025-03-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="0-abstract">0. Abstract</h1>
<hr>
<h3 id="工作：">工作：</h3>
<ol>
<li>训练了不同的图像到图像转换模型，从语义分割图中合成带有或不带有卒中病灶的脑部磁共振图像。</li>
<li>训练了一个生成对抗网络来生成合成病灶掩码。</li>
<li>将两个组件结合起来，构建了一个大规模的合成卒中图像数据库。</li>
<li>使用U-Net对各种模型的性能进行评估，该网络在临床测试集上训练以分割卒中病灶。</li>
</ol>
<h3 id="结果：">结果：</h3>
<ul>
<li>这是首次基于图像到图像转换的合成数据增强对比分析，也是首次应用于缺血性卒中。</li>
</ul>
<h1 id="1-introduction">1. INTRODUCTION</h1>
<hr>
<ul>
<li>ischemic stroke (IS)——缺血性脑卒中</li>
<li>image-to-image translation models (ITMs)</li>
<li>diffusion weighted magnetic resonance imaging (DWI)——弥散加权磁共振成像</li>
</ul>
<p>将包含<strong>大脑结构解剖标签</strong>和中风病变标签的语义分割图转换为大脑DWI图像。<br>
<strong>新的病变标签</strong>通过3D生成对抗网络生成。解剖标签将指导网络生成具有与真实DWI图像相当的组织对比度的清晰图像。<br>
<strong>训练集中的病变标签</strong>将提供足够的信息来学习病理特定的对比度修改。<br>
ITM应学会在病变标签==内==生成高信号，同时忽略任何（统计上代表性不足的）误分类，例如病变标签==外==的高信号或由人工阅读者误分类为病变的正常组织。</p>
<h2 id="1-1-related-work">1.1 Related work</h2>
<hr>
<p>参考文献[10]通过将真实的IS病变轮廓融合到健康脑的DWI中，并通过线性增加轮廓内的体素强度来模拟病变，生成了合成的IS体积。</p>
<p>参考文献[11–13]成功地使用生成对抗网络（GANs）生成合成磁共振图像（MRIs）以改善脑肿瘤分割。</p>
<p>参考文献[14]作者使用ITM从CT图像中获取DWI，从而提高了在CT上增强合成DWI的缺血性卒中核心组织分割质量。</p>
<p>参考文献[15]使用CT到MR的图像转换来改善肺肿瘤分割。</p>
<p>参考文献[16]提出了使用风格转换的进一步方法，其中使用图像到图像转换将健康MR图像转换为病变MR图像以进行脑肿瘤分割.</p>
<p>参考文献[17]将MR图像转换为CT图像以进行一般治疗计划。</p>
<p>参考文献[18]引入了一种ITM方法，该方法被用于涉及图像去噪、运动校正和领域适应等多种医学任务。</p>
<h2 id="1-2-贡献">1.2 贡献</h2>
<hr>
<p>提出了一种从语义分割图中合成IS病变标签和DWI的新方法。<br>
前者通过ITMs实现，而病变标签则由3D GAN生成。<br>
这使得我们能够构建包含病变标签的空间一致脑部图像。此外，这种方法使我们的流程具有可扩展性，因为GAN可以生成无限数量的合成且独特的病变标签。我们由此证明，<strong>基于深度学习的数据增强能够有效利用有限医学数据集中的信息，并优于传统的数据增强技术。</strong> 这一概念最初由[19]提出，并在[20]中进一步发展用于非医学环境下的分割任务。</p>
<p>通过比较在手动标记数据上训练的U-Net[21]与在合成数据上训练的U-Net在临床IS数据测试集上的DSC值来量化这一结论。</p>
<p>这是首次在IS病变分割中使用ITMs进行数据增强的比较研究。</p>
<h1 id="2-materials-and-methods">2. MATERIALS AND METHODS</h1>
<hr>
<p>我们提出了一种通过两个连续的生成模型生成显示IS病变的脑部标注DWI的方法，其中一个模型用于生成真实的卒中病变标签，另一个模型将脑部分割掩码转换为DWI。</p>
<ol>
<li>健康大脑的解剖分割掩码易于获取且无需医学训练，因为已有多种自动分割算法。</li>
<li>通过GAN生成卒中病变标签，假设其引入了通用变换以及对现有病变掩码潜在空间表示的逼真插值。</li>
<li>使用条件生成模型生成合成图像，与无条件潜在空间模型相比，显著改善了生成器的收敛行为。<br>
由此，我们构建了一个最小工具箱，可以通过利用GAN的插值能力合成更多病变掩码，进而生成越来越多的标注DWI。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png" alt="image.png"></p>
<h2 id="2-1-data">2.1 Data</h2>
<hr>
<p>在巴塞尔大学医院获取了804例出现IS症状患者的DWI数据库，并获得了机构审查委员会的批准。该数据库（以下简称stroke DB）包含449例DWI阳性病例，即诊断为IS病灶（患者平均年龄72±14岁；200例左侧IS，193例右侧IS，56例双侧IS；194例女性，255例男性）。DWI阳性病例被随机分为训练集（365例）和测试集（74例）。此外，测试集中还包括85例DWI阴性样本，总计159例测试样本。另有一个包含2027例健康DWI扫描的独立数据库（平均年龄38±24岁；1088例女性，939例男性）用于数据增强流程（以下简称normal DB）。stroke DB中的大多数图像是在1.5 T扫描仪上获取的（67% @ 1.5 T；23% @ 3 T），而normal DB中的大多数扫描使用了3 T扫描仪（68% @ 1.5 T；22% @ 3 T）。平均而言，stroke DB扫描的回波时间和恢复时间与normal DB相似（TE = 90±16 ms，TR = 7400±1300 ms [normal DB] vs. TE = 100±2 ms，TR = 7000±1500 ms [stroke DB]）。图像被配准到标准的蒙特利尔神经学研究所图谱，使用FSL的“脑提取工具”进行去颅骨处理，并使用ANTs重新采样至128×128×40体素的标准分辨率。为了在训练过程中提高稳定性，裁剪了顶部和底部的四层切片。体素强度在第99.5百分位数处截断，背景的绝对体素强度截断值为35。最终，信号强度被重新缩放到[-1, 1]范围。使用基于FreeSurfer获取的参考图像训练的3D U-Net进行大脑解剖分割，如参考文献所述。这将整个数据库的处理时间缩短至几小时。</p>
<h2 id="2-2-image-translation">2.2 Image translation</h2>
<hr>
<h3 id="2-2-1-pix2pix">2.2.1 Pix2Pix</h3>
<hr>
<p>原始的 Pix2Pix 基于 U-Net 架构；然而，在高分辨率衍生模型中使用了残差块 [29, 34]。我们训练了两种架构，但发现质量差异不大，因此使用了基于 U-Net 的版本，因其收敛速度更快。<br>
我们考虑了重构损失权重 λ 的不同值，因为我们发现 λ = 100（Pix2Pix100）比推荐值 λ = 10（Pix2Pix10）能产生质量上更具吸引力的结果。<br>
对于判别器架构，我们采用了 PatchGAN [28, 35, 36]。与普通判别器不同，PatchGAN 不是输出一个单一的数字来判断图像是真实的还是伪造的，而是对图像的局部区域进行判断，从而为生成器提供更精细的反馈。L1 损失足以捕捉大尺度或低频特征 [28]。</p>
<h3 id="2-2-2-cyclegan">2.2.2 cycleGAN</h3>
<hr>
<p>一种无配对图像翻译的方法由cycleGAN [37]提出。两个生成器-判别器对，每个图像域各一个，使得模型可以通过重建损失进行训练，使用第一个生成器将图像从域A翻译到域B，然后使用另一个生成器从域B翻译回域A，并计算生成图像与原始图像之间的逐像素（L1或L2范数）差异。这种循环一致性赋予了模型名称，并解决了无配对数据的问题。然而，需要训练的参数数量相当庞大，在我们的设置中达到了4200万。为了处理如此大量的自由参数，模型引入了身份损失，该损失惩罚生成器与身份映射的偏差。需要注意的是，cycleGAN已成功应用于医学图像生成中的配对数据 [38, 39]。</p>
<h3 id="2-2-3-spade">2.2.3 SPADE</h3>
<hr>
<p>SPADE 是本文讨论的最新 ITM 之一。尽管它在医学领域尚未广泛应用，但在环境图像上的结果令人期待 [40]。此外，SPADE 架构与标准 GAN 基本一致，非常经济，与其他 ITM 相比减少了需要训练的参数数量。与 GANs [9] 类似，该模型从一个随机潜在向量开始，随后通过上采样在生成过程中引入变化元素。分割掩码被注入到 SPADE 的归一化层中，取代了通常的批次或实例归一化。这有助于模型更有效地学习，并更快地收敛到视觉上吸引人的结果 [40]。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050940281.png" alt="image.png"></p>
<h3 id="2-2-4-training">2.2.4 Training</h3>
<hr>
<p>我们已将所有的ITM实现为TensorFlow [41]中的<strong>2D CNN</strong>，并在单个GPU（Nvidia Titan RTX 24GB）上对临床正常和卒中数据库的合并数据进行100个周期的训练。所有模型均使用批次大小为8，学习率为2·10−4，其余设置均按照前文所引用的建议进行配置。所有模型均基于相同的训练条件，使用可用的分割体积来获得条件生成模型，这些模型仅在架构上有所不同，而非训练过程。最后，我们在训练集的<strong>3D病变掩码</strong>上训练了一个GAN，作为病变标签生成器，训练周期为500。在后续的病变注入中，所有ITM均使用相同的病变生成器。</p>
<h2 id="2-3-lesion-injection-and-dwi-synthesis">2.3 Lesion injection and DWI synthesis</h2>
<hr>
<p>为了合成IS数据以增强现有的临床数据，我们使用了2027份健康大脑DWI的解剖分割图。<br>
这些健康的分割图随后通过注入一个假病变进行修改，该假病变由3D Wasserstein GAN生成[42]，该GAN在卒中数据库的病变掩码上进行训练，并通过将softmax输出在0.5的阈值处进行硬分割转换。<br>
注入是通过调整DWI分割图中实质的标签来实现的；然而，要求病变的最小体积为20个体素。如果未满足此要求，则生成新的病变掩码，直到生成至少20个体素的病变。<br>
生成的分割图随后被分解为2D切片并输入ITM以生成假DWI。</p>
<h2 id="2-4-evaluation-of-lesion-segmentation">2.4 Evaluation of lesion segmentation</h2>
<hr>
<p>为了定量评估各种ITM的性能，我们在临床、临床与合成以及单独合成的数据库上训练了一个3D U-Net[21]分割网络（针对上述讨论的每个ITM），随后在包含74名中风患者和85名健康患者的DWI测试集上评估生成的分割掩码，该测试集已从训练数据中分离出来。<br>
图3展示了这一过程的可视化表示。<br>
如果验证损失在100个epoch内未增加，则停止训练。为了定量比较模型的性能，我们在训练历史结束时选择100个epoch的范围，在测试集上评估每个模型。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051000060.png" alt="image.png"></p>
<p>模型通过交叉熵损失和DSC的组合进行训练，这些损失使用U-Net P的soft-max输出和真实标签G计算。<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051027127.png" alt="image.png"><br>
其中 | · | 表示集合的基数，第二个等式适用于基于像素/体素的二值分割图，且求和遍历所有条目。为了更好地比较，我们还报告了两位具有两年经验的人工读者之间的分割DSC。<br>
此外，我们通过一系列额外指标评估分割的质量。首先，我们评估相对体积差异，因为病变体积是患者治疗分诊的重要指标。此外，我们评估豪斯多夫距离（Hausdorff distance, HD）和平均对称表面距离（the average symmetric surface distance , ASSD），这两个指标常用于量化预测病变形状与真实值之间的质量。表面距离dA(b)定义为点b ∈ B到表面A的最小欧几里得距离，即b与任意点a ∈ A之间的最小距离。由此，ASSD构造为对称和。<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051030793.png" alt="image.png"><br>
（对称化后的）HD是最大的对称表面距离，<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051033639.png" alt="image.png"><br>
最后，我们对模型的召回率和精确率进行了评估。</p>
<h1 id="3-results">3. RESULTS</h1>
<hr>
<h2 id="3-1-synthetic-data-generation-and-qualitative-evaluation">3.1 Synthetic data generation and qualitative evaluation</h2>
<hr>
<p>图(a)采用了重建损失权重λ = 10，而图(b)则使用了λ = 100。结果质量良好，主观上逼真，并且如预期的那样，附带了高保真的病变标签。<br>
SPADE生成的病变强度[图4©]明显低于Pix2Pix生成的图像，但病变外的对比度也低得多：在合成大脑的后部，几乎不存在灰质/白质对比。虽然这乍一看似乎是一个缺点，但实际上可能有利于训练U-Net以可靠地检测信号增幅较低的病变。?<br>
cycleGAN未能识别并合成任何病变高信号，同时产生了极高的组织间对比度[图4(d)]。我们推测这与cycleGAN的循环一致性要求密切相关，即两个生成器将图像尽可能映射回自身，以及病变与非病变体素之间的类别不平衡。同样，文献[37]指出了“失败案例”，表明cycleGAN的泛化能力较差。<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051040844.png" alt="image.png"><br>
图5中展示了经过预处理并归一化到[0, 1]范围的合成和真实数据集的强度直方图。请注意，正常和临床（即中风）数据库在强度低于0.5的区域存在显著差异。合成图像的强度紧密跟随正常体积的强度。这并不令人意外，因为我们使用了正常分割掩码来合成中风图像。由此可以得出结论，这种差异可能是由解剖结构或图像采集过程中的某些系统性差异引起的。<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051048335.png" alt="image.png"></p>
<h2 id="3-2-model-selection-and-test-set-evaluation">3.2 Model selection and test set evaluation</h2>
<hr>
<p>结果如图6所示，并总结于表I中。<br>
基于临床数据训练的U-Net在测试集上的平均DSC为(63.2 ± 1.9)%，最大DSC为67.3%。<br>
基于Pix2Pix生成的合成数据与临床数据联合训练的U-Net表现优于仅基于临床数据的基线U-Net，其平均DSC分别为(70.3 ± 1.1)%（Pix2Pix；λ = 100）和(70.8 ± 1.0)%（Pix2Pix；λ = 10）。<br>
而SPADE的表现略差，为(62.9 ± 1.2)%。<br>
Pix2Pix（λ = 10）的最大DSC达到72.8%，其他模型的最大DSC分别为66.5%（SPADE）和72.5%（Pix2Pix；λ = 100）。<br>
这是本研究的一个关键发现，表明<strong>合成数据并非简单地复制训练数据中的信息，而是通过插值有效地泛化了训练数据</strong>。<br>
与此相反，仅基于合成数据训练的U-Net表现较差，且方差较大。其测试集DSC分别为(57.1±2.7)%（Pix2Pix；λ = 100）、(55.7±3.3)%（Pix2Pix；λ = 10）和(47.3 ± 2.2)%（SPADE）。<br>
有趣的是，与之前的结果不同，较大的重建损失在Pix2Pix中表现更好，这与我们的定性观察一致。为了更全面地理解这些结果，我们报告了人类阅片者的DSC，训练集为76.6 ± 13.9%，测试集为76.9 ± 13.5%。<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051054353.png" alt="image.png"><br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051054623.png" alt="image.png"></p>
<h2 id="3-3-model-fine-tuning">3.3 Model fine-tuning</h2>
<hr>
<p>这些结果表明，仅靠合成数据不足以替代临床数据集。<br>
这是否归因于生成图像的质量，或是用于图像合成和评估的数据库之间解剖特征的差异，将在接下来的部分中进行探讨。<br>
分别使用了10个或50个随机选择的临床训练样本，并额外训练了150个周期。<br>
图7中我们重复了之前的结果，即模型仅使用合成数据进行训练（橙色）。随后，我们展示了分别使用10个（红色）和50个（深红色）临床病例进行训练后的结果。<br>
对于<strong>SPADE</strong>，将微调集大小从10增加到50仅带来小幅提升：从10个样本的(51.0 ± 1.2)%提升到50个样本的(56.4 ± 1.5)%。在50个样本上微调后，SPADE的性能与临床基线模型的下限相当。<br>
<strong>Pix2Pix</strong>的情况则截然不同，使用10个微调样本时，其性能已与临床基线相当。使用50个样本进行微调后，Pix2Pix获得了足够的信息，以平均DSC为(66.4 ± 1.1)%（最大68.9%；λ = 10）和(66.8±0.7)%（最大68.3%；λ = 100）的表现超越了临床模型。在所有情况下，DSC的方差均远低于临床基线。如果我们在仅10个[50个]病例上训练U-Net，结果将会更差：我们发现平均DSC为(2.9 ± 1.6)% [(12.5 ± 7.0)%]。<br>
仅使用合成数据训练的模型无法达到基于临床数据集训练的模型性能。然而，这些模型只需少量临床训练案例即可进行微调，最终得到的结果与基于全部365例临床IS病例训练的基线模型相当，且方差显著降低。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051100795.png" alt="image.png"></p>
<h2 id="3-4-further-experiments">3.4 Further experiments</h2>
<hr>
<p>表II和图9展示了我们进行的Pix2Pix四种不同类型实验的结果。<br>
前四行包含了在10%/25%/50%的临床数据上训练U-Net并在完整测试集上评估的结果。注意到在所有情况下，误差都较大，且随着更多临床数据的加入仅略有减少，这表明数据集具有较高的多样性。随着训练集规模的增加，DSC迅速上升；然而，从50%到完整数据集时，仅观察到小幅度的提升。这种行为与相对体积损失形成对比，后者在完整训练集上实际上最大。<br>
进一步检查发现，体积损失较大的案例中病灶体积较小（参见图8），因此在研究不同大小的病灶时，<strong>体积损失似乎不是一个合适的指标</strong>。所有其他指标均证实，使用完整训练集能够获得最佳性能。<br>
在第二类实验中，我们利用完整训练集并采用了不同的数据增强技术，包括添加随机噪声和随机重映射图像强度，如参考文献[47]所建议。然而，没有任何指标表明其他数据增强技术比单独使用几何变换表现得更好。<br>
在前一种情况下，对于较小的部分训练数据集，可以取得显著改进，例如在少于40例DWI阳性临床病例（10%）的情况下，可以获得高于50%的DSC。其他指标也显示出相同的行为。有趣的是，相对体积损失随着临床病例数量的增加而单调递减，表明模型对于较小病变的预测更加稳定。<br>
人们可能会怀疑，使用来自stroke DB的分割掩码（由于个体年龄较大，平均具有较大的脑室）会进一步提高性能；然而事实并非如此。即使将分割掩码重复使用四次（生成的数据集大小与正常DB相当）并通过不同的病变标签进行增强，我们也没有发现任何改进。因此，我们可以得出结论，<strong>正常DB中增加的变异性对U-Net的性能是有益的</strong>。<br>
此外，我们得出结论，<strong>仅使用合成数据训练的模型的不足并不源于我们所使用的分割掩码的分布外性质，而是ITM本身的限制或训练集大小的限制</strong>。<br>
最后，我们在ISELS 2015 DWI图像上评估了训练好的模型[7]。我们发现，所考虑的模型一致地产生了约50%的DSC，模型之间的差异很小。然而，该测试集的差异较大，标准差约为30%。对部分病例的仔细检查表明，ISLES数据的分割较为粗糙，因此DSC实际上受到该测试集分割质量的限制，而不是模型的能力。从定性上看，结果质量很高，只有少数病例（如脑干梗死）被我们的模型遗漏。参见图10的示例，其中模型提供了非常详细的分割轮廓。而真实的分割则非常粗糙，甚至包括脑脊液（CSF）以及不属于脑实质的结构。这强烈支持了我们的假设，即<strong>数据质量限制了模型的得分，而不是模型性能本身</strong>。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051112398.png" alt="image.png"><br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051112718.png" alt="image.png"><br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051120125.png" alt="image.png"></p>
<h2 id="3-5-discussion">3.5 Discussion</h2>
<hr>
<p>较大的重建损失比推荐的λ = 10能生成更清晰的图像，但差异较为微小。<br>
中风数据库显示出可归因于患者高龄的解剖学特征，尤其是扩大的脑室体积。<br>
如果合成数据引入了额外的解剖学变异，则其是有益的。<br>
所有模型在较大病灶上都表现良好，包含多个较小病灶的案例并非所有模型都能同等程度地准确分割。</p>
<p>使用从卒中患者的真实扩散加权成像（DWI）中提取的掩码病变，将其植入健康患者的真实DWI中，从而创建了一个类似的合成图像数据库用于训练分割网络。虽然这种方法生成了完全逼真的3D图像（至少在病变区域外），但其方法在组合上存在局限性，而我们的方法原则上可以从（连续的）潜在空间表示（如SPADE和病变生成GAN）或分割掩码的连续变形中生成一系列新的、未见过的图像。<br>
仅通过ITM生成的合成图像训练的U-Net无法与基于临床数据训练的模型结果相媲美。然而，此类模型只需在少量临床病例上进行微调，即可获得与在纯临床环境中使用大量高质量分割数据相当的结果。<br>
针对3D一致性问题，我们还尝试训练了3D ITM。然而，训练3D生成模型较为困难，因为这些模型往往会迅速耗尽可用内存，导致图像分辨率或训练批量大小受限。此外，GAN在3D数据训练时通常难以收敛。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503051121340.png" alt="image.png"></p>
<h1 id="4-conclusions-outlook">4. CONCLUSIONS &amp; OUTLOOK</h1>
<hr>
<p>数据集规模的增加显著提高了分割网络的质量。此外，<br>
使用合成数据训练疾病检测算法（如本研究中针对IS病灶分割的情况）可以避免数据隐私问题。<br>
仅使用合成数据进行训练无法取得具有竞争力的结果。<br>
更精细的数据准备可以提供与临床数据更一致的对比度，但代价是需要对模型进行特定数据集的微调。<br>
所提出的流程可以直接推广到其他可以通过DWI识别的病理，甚至适用于整个人体的其他成像模式。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yanqinglei.github.io">BulingQAQ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yanqinglei.github.io/gan-itfm/">https://yanqinglei.github.io/gan-itfm/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yanqinglei.github.io" target="_blank">BulingQAQ的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GAN/">GAN</a><a class="post-meta__tags" href="/tags/ITM/">ITM</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/image-synthesis-review-dlaf/" title="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503021507800.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409182316153.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-18</div><div class="title">【论文阅读】Generative Adversarial Nets</div></div></a></div><div><a href="/gan-bmsg/" title="【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409092216234.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-10</div><div class="title">【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training</div></div></a></div><div><a href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409121843542.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-12</div><div class="title">【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</div></div></a></div><div><a href="/gan-itit/" title="【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072345976.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="title">【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks</div></div></a></div><div><a href="/gan-mmit/" title="【论文阅读】MedGAN:Medical image translation using GANs"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-21</div><div class="title">【论文阅读】MedGAN:Medical image translation using GANs</div></div></a></div><div><a href="/gan-sdat/" title="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-02</div><div class="title">【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/spy_family_avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">BulingQAQ</div><div class="author-info__description">不知归路，宁愿一世无悔追逐</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YanQinglei"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YanQinglei" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/OctYZ" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1819615836&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1819615836@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">请多多指教</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0-abstract"><span class="toc-text">0. Abstract</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%EF%BC%9A"><span class="toc-text">工作：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-text">结果：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-introduction"><span class="toc-text">1. INTRODUCTION</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-related-work"><span class="toc-text">1.1 Related work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E8%B4%A1%E7%8C%AE"><span class="toc-text">1.2 贡献</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-materials-and-methods"><span class="toc-text">2. MATERIALS AND METHODS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-data"><span class="toc-text">2.1 Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-image-translation"><span class="toc-text">2.2 Image translation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-pix2pix"><span class="toc-text">2.2.1 Pix2Pix</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-cyclegan"><span class="toc-text">2.2.2 cycleGAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-spade"><span class="toc-text">2.2.3 SPADE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-training"><span class="toc-text">2.2.4 Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-lesion-injection-and-dwi-synthesis"><span class="toc-text">2.3 Lesion injection and DWI synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-evaluation-of-lesion-segmentation"><span class="toc-text">2.4 Evaluation of lesion segmentation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-results"><span class="toc-text">3. RESULTS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-synthetic-data-generation-and-qualitative-evaluation"><span class="toc-text">3.1 Synthetic data generation and qualitative evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-model-selection-and-test-set-evaluation"><span class="toc-text">3.2 Model selection and test set evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-model-fine-tuning"><span class="toc-text">3.3 Model fine-tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-further-experiments"><span class="toc-text">3.4 Further experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-discussion"><span class="toc-text">3.5 Discussion</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-conclusions-outlook"><span class="toc-text">4. CONCLUSIONS &amp; OUTLOOK</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/gan-itfm/" title="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation"/></a><div class="content"><a class="title" href="/gan-itfm/" title="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation">【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation</a><time datetime="2025-03-05T06:24:58.000Z" title="发表于 2025-03-05 14:24:58">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/image-synthesis-review-dlaf/" title="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503021507800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review"/></a><div class="content"><a class="title" href="/image-synthesis-review-dlaf/" title="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review">【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review</a><time datetime="2025-03-03T07:43:01.091Z" title="发表于 2025-03-03 15:43:01">2025-03-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-sdat/" title="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis"/></a><div class="content"><a class="title" href="/gan-sdat/" title="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis">【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis</a><time datetime="2025-03-02T02:21:15.559Z" title="发表于 2025-03-02 10:21:15">2025-03-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/image-synthesis-review-03/" title="【论文阅读】Generating Synthetic Data for Medical Imaging"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202412031045821.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Generating Synthetic Data for Medical Imaging"/></a><div class="content"><a class="title" href="/image-synthesis-review-03/" title="【论文阅读】Generating Synthetic Data for Medical Imaging">【论文阅读】Generating Synthetic Data for Medical Imaging</a><time datetime="2024-12-03T03:25:53.000Z" title="发表于 2024-12-03 11:25:53">2024-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409301638643.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"/></a><div class="content"><a class="title" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合">【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合</a><time datetime="2024-10-10T13:52:22.000Z" title="发表于 2024-10-10 21:52:22">2024-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021053834.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By BulingQAQ</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>