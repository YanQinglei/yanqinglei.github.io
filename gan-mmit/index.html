<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】MedGAN:Medical image translation using GANs | BulingQAQ的个人博客</title><meta name="author" content="BulingQAQ"><meta name="copyright" content="BulingQAQ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Abstract 背景：图像到图像的转换被认为是医学图像分析领域的一个新的前沿领域。 工作：提出了一种新的医学图像到图像的转换框架MedGAN：将对抗性框架与非对抗性损失的新组合相结合；生成器——CasNet，该结构通过编码器-解码器对的逐步细化来增强翻译后的医学输出的清晰度；判别器——作为一个可训练的特征提取惩罚之间的差异转换医学图像和期望的模式；利用风格传递损失将目标图像的纹理和精细结构匹配到">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】MedGAN:Medical image translation using GANs">
<meta property="og:url" content="https://yanqinglei.github.io/gan-mmit/">
<meta property="og:site_name" content="BulingQAQ的个人博客">
<meta property="og:description" content="Abstract 背景：图像到图像的转换被认为是医学图像分析领域的一个新的前沿领域。 工作：提出了一种新的医学图像到图像的转换框架MedGAN：将对抗性框架与非对抗性损失的新组合相结合；生成器——CasNet，该结构通过编码器-解码器对的逐步细化来增强翻译后的医学输出的清晰度；判别器——作为一个可训练的特征提取惩罚之间的差异转换医学图像和期望的模式；利用风格传递损失将目标图像的纹理和精细结构匹配到">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png">
<meta property="article:published_time" content="2024-09-21T14:04:30.000Z">
<meta property="article:modified_time" content="2024-09-21T14:27:30.191Z">
<meta property="article:author" content="BulingQAQ">
<meta property="article:tag" content="GAN">
<meta property="article:tag" content="MedGAN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png"><link rel="shortcut icon" href="/img/icon.png"><link rel="canonical" href="https://yanqinglei.github.io/gan-mmit/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】MedGAN:Medical image translation using GANs',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-21 22:27:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/spy_family_avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png')"><nav id="nav"><span id="blog-info"><a href="/" title="BulingQAQ的个人博客"><span class="site-name">BulingQAQ的个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】MedGAN:Medical image translation using GANs</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-21T14:04:30.000Z" title="发表于 2024-09-21 22:04:30">2024-09-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-21T14:27:30.191Z" title="更新于 2024-09-21 22:27:30">2024-09-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【论文阅读】MedGAN:Medical image translation using GANs"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><hr>
<h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h3><p>图像到图像的转换被认为是医学图像分析领域的一个新的前沿领域。</p>
<h3 id="工作："><a href="#工作：" class="headerlink" title="工作："></a>工作：</h3><p>提出了一种新的医学图像到图像的转换框架MedGAN：<br>将对抗性框架与非对抗性损失的新组合相结合；<br>生成器——CasNet，该结构通过编码器-解码器对的逐步细化来增强翻译后的医学输出的清晰度；<br>判别器——作为一个可训练的特征提取惩罚之间的差异转换医学图像和期望的模式；<br>利用风格传递损失将目标图像的纹理和精细结构匹配到转换后的图像；<br>应用于三个不同的任务：PETCT翻译、MR运动伪影校正和PET图像去噪。</p>
<h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><p>MedGAN优于其他现有的转换方法。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><hr>
<p>由于可能引入不切实际的信息，将输入图像模态转换为输出模态的任务具有挑战性。这显然会使合成图像不可靠，无法用于诊断目的。</p>
<h2 id="1-1-Classical-approaches"><a href="#1-1-Classical-approaches" class="headerlink" title="1.1 Classical approaches"></a>1.1 Classical approaches</h2><hr>
<h2 id="1-2-Generative-models"><a href="#1-2-Generative-models" class="headerlink" title="1.2 Generative models"></a>1.2 Generative models</h2><hr>
<h2 id="1-3-Medical-image-translation"><a href="#1-3-Medical-image-translation" class="headerlink" title="1.3 Medical image translation"></a>1.3 Medical image translation</h2><hr>
<h2 id="1-4-Contributions"><a href="#1-4-Contributions" class="headerlink" title="1.4 Contributions"></a>1.4 Contributions</h2><hr>
<p>MedGAN的主要目的不是诊断，而是进一步增强需要全局一致图像属性的技术后处理任务。</p>
<ul>
<li><strong>MedGAN</strong>通过结合对抗性框架和一种新的非对抗性损失组合，捕捉所需目标模态的高频和低频成分。</li>
<li><strong>CasNet</strong>新生成器架构，将多个完全卷积的编码器-解码器网络通过跳跃连接链接成一个生成器网络。</li>
<li><strong>MedGAN</strong>在三项具有挑战性的医学成像任务中的应用：从PET图像生成合成CT图像、PET图像去噪以及MR运动伪影。</li>
<li><strong>MedGAN</strong>与其他对抗性翻译框架的定量和定性比较。</li>
<li>从医学角度研究翻译后的医学图像的主观性能和保真度。</li>
</ul>
<h1 id="2-Materials-and-Methods"><a href="#2-Materials-and-Methods" class="headerlink" title="2. Materials and Methods"></a>2. Materials and Methods</h1><hr>
<p>MedGAN架构：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png" alt="image.png"></p>
<h2 id="2-1-Preliminaries"><a href="#2-1-Preliminaries" class="headerlink" title="2.1 Preliminaries"></a>2.1 Preliminaries</h2><hr>
<h3 id="2-1-1-Generative-adversarial-networks"><a href="#2-1-1-Generative-adversarial-networks" class="headerlink" title="2.1.1 Generative adversarial networks"></a>2.1.1 Generative adversarial networks</h3><hr>
<p>GANs 由两个主要部分组成：生成器和判别器。<br>生成器 G 接受来自先验噪声分布 $p<em>{\text{noise}}$ （例如正态分布）的输入样本 z，并将其映射到数据空间 $\hat{x} = G(z)$，从而诱导出模型分布 $p</em>{\text{model}}$ 。<br>判别器 D 是一个二元分类器，其目标是将真实数据样本 $x \sim p<em>{\text{data}}$ 分类为真实（D(x)=1），而将生成的样本 $\hat{x} \sim p</em>{\text{model}}$ 分类为虚假（$D(\hat{x}) = 0$）。<br>这两个网络相互竞争：生成器试图生成与真实样本无法区分的样本，即使 $p<em>{\text{model}} \approx p</em>{\text{data}}$，从而欺骗判别器；与此同时，判别器的目标是通过学习更有意义的特征来避免被欺骗，更好地区分真实和生成的样本。</p>
<p>表现为：</p>
<script type="math/tex; mode=display">\min_G \max_D L_{\text{GAN}}</script><p>对抗损失：</p>
<script type="math/tex; mode=display">L_{\text{GAN}} = \mathbb{E}_{x \sim p_{\text{data}}} [\log D(x)] + \mathbb{E}_{z \sim p_{\text{noise}}} [\log (1 - D(G(z)))]</script><h3 id="2-1-2-Image-to-image-translation"><a href="#2-1-2-Image-to-image-translation" class="headerlink" title="2.1.2 Image-to-image translation"></a>2.1.2 Image-to-image translation</h3><hr>
<p>将对抗网络从图像生成应用于转换任务的基本原理是将生成器网络替换为条件生成对抗网络 (cGAN)。</p>
<p>生成器的目标是通过映射函数 $G(y, z) = \hat{x} \sim p<em>{\text{model}}$ 将源域图像 $y \sim p</em>{\text{source}}$ 映射到其对应的真实目标图像 $x \sim p_{\text{target}}$。这通常可以被视为两个共享相同底层结构但表面外观不同的域之间的回归任务。</p>
<p>cGAN 不是使用手动构建的损失函数来测量转换图像和目标图像之间的相似性，而是利用一个二元分类器——判别器来代替。<br>对抗损失：</p>
<script type="math/tex; mode=display">L_{\text{cGAN}} = \mathbb{E}_{x,y}[\log D(x, y)] + \mathbb{E}_{z,y}[\log (1 - D(G(y, z), y))]</script><p>判别器的目标是将源图像 y 和其对应的真实目标图像 x 的拼接视为真实（D(x, y) = 1），而将 y 和生成的图像 $\hat{x}$ 视为虚假（$D(\hat{x}, y) = 0$）。<br>仅依赖对抗损失函数的图像到图像翻译框架无法生成一致的结果。生成的图像可能不会与目标图像共享相似的全局结构。<br>为了解决这一问题，通常会加入像素重构损失，例如 L1 损失 。这可以通过计算目标图像与生成图像之间的平均绝对误差 (MAE) 来实现：</p>
<script type="math/tex; mode=display">L_{L1} = \mathbb{E}_{x,y,z} [\|x - G(y, z)\|_1]</script><p>最终训练目标：</p>
<script type="math/tex; mode=display">\min_G \max_D (L_{\text{cGAN}} + \lambda L_{L1})</script><h2 id="2-2-Perceptual-loss"><a href="#2-2-Perceptual-loss" class="headerlink" title="2.2 Perceptual loss"></a>2.2 Perceptual loss</h2><hr>
<p>利用这种损失函数的转换框架往往会在保持全局结构的同时，导致图像失真和细节丢失。<br>为了捕捉图像中高频成分之间的差异，额外引入了感知损失。<br>这种损失是基于使用判别器网络作为可训练的特征提取器，提取中间特征表示。目标图像 x 和翻译图像 $\hat{x}$ 的特征图之间的平均绝对误差 (MAE) 被计算为：</p>
<script type="math/tex; mode=display">P_i (G(y, z), x) = \frac{1}{h_i w_i d_i} \|D_i (G(y, z), y) - D_i (x, y)\|_1</script><p>其中 $D_i$ 表示从判别器网络的第 i 个隐藏层提取的特征表示，$h_i$ 、 $w_i$ 和 $d_i$ 分别表示特征空间的高度、宽度和深度。<br>感知损失：</p>
<script type="math/tex; mode=display">L_{\text{perceptual}} = \sum_{i=0}^{L} \lambda_{pi} P_i (G(y, z), x)</script><p>其中 L 是判别器的隐藏层数量，$\lambda_{pi} &gt; 0$ 是一个调节超参数，表示第 i 层的影响。</p>
<p>稳定判别器的对抗训练，采用了谱归一化正则化，通过对判别器中每一层 i 的权重矩阵 $\theta_{D,i}$ 进行归一化来实现：</p>
<script type="math/tex; mode=display">\theta_{D,i} = \frac{\theta_{D,i}}{\delta(\theta_{D,i})}</script><p>其中 $\delta(\theta<em>{D,i})$ 表示矩阵 $\theta</em>{D,i}$ 的谱范数。因此，判别器函数 D(x, y) 的 Lipschitz 常数将被限制为 1。<br>在实际操作中，为了计算谱范数，使用了功率迭代法的近似 $\hat{\delta}(W_i)$，而不是应用奇异值分解，以降低所需的计算复杂度。</p>
<h2 id="2-3-Style-transfer-losses"><a href="#2-3-Style-transfer-losses" class="headerlink" title="2.3 Style transfer losses"></a>2.3 Style transfer losses</h2><hr>
<p>用于损失计算的特征来自为图像分类任务预训练的特征提取器。<br>预训练网络的优势它能够从更大的感受野中提取丰富的特征，从而在增强转换图像的全局结构的同时，也增强精细的细节。<br>风格迁移损失可以分为两个主要组成部分：风格损失和内容损失。</p>
<h3 id="2-3-1-Style-loss"><a href="#2-3-1-Style-loss" class="headerlink" title="2.3.1 Style loss"></a>2.3.1 Style loss</h3><hr>
<p>风格损失惩罚转换图像与其对应目标图像之间风格表示的差异。</p>
<p>特征的相关性通过每个卷积块的Gram矩阵 $Gr_j(x)$ 表示，该矩阵的形状为 $d_j \times d_j$，其元素通过特征图在高度和宽度维度上的内积计算得出：</p>
<script type="math/tex; mode=display">Gr_j(x)_{m,n} = \frac{1}{h_j w_j d_j} \sum_{h=1}^{h_j} \sum_{w=1}^{w_j} V_j(x)_{h,w,m} V_j(x)_{h,w,n}</script><p>$V_{j,i}(x)$ 表示从输入图像 x 中通过特征提取网络的第 j 个卷积块和第 i 层提取的特征图。仅使用每个卷积块的第一层，并将在符号中省略。这些特征图的大小为 $h_j \times w_j \times d_j$，其中 $h_j$ 、 $w_j$ 、 $d_j$ 分别表示高度、宽度和空间深度。</p>
<p>风格损失计算为转换输出 $\hat{x}$ 和真实输入 x 之间的特征相关性差异的Frobenius范数平方：</p>
<script type="math/tex; mode=display">L_{\text{style}} = \sum_{j=1}^{B} \lambda_{sj} \frac{1}{4 d_j^2} \| Gr_j(G(y, z)) - Gr_j(x) \|_F^2</script><p>$\lambda_{sj} &gt; 0$ 是一个调整的超参数，表示第 j 个卷积块的贡献权重，B 是卷积块的总数。</p>
<h3 id="2-3-2-Content-loss"><a href="#2-3-2-Content-loss" class="headerlink" title="2.3.2 Content loss"></a>2.3.2 Content loss</h3><hr>
<p>内容损失惩罚从特征提取网络中提取的特征表示之间的差异，不捕捉风格或纹理的差异。<br>起到了类似像素重构损失的辅助作用，通过增强低频分量并确保转换图像的全局一致性。</p>
<p>内容损失：</p>
<script type="math/tex; mode=display">L_{\text{content}} = \sum_{j=1}^{B} \lambda_{cj} \frac{1}{h_j w_j d_j} \| V_j(G(y, z)) - V_j(x) \|_F^2</script><p>其中，$\lambda_{cj} &gt; 0$ 是一个超参数，表示第 j 个卷积块第一层的影响力。</p>
<h2 id="2-4-MedGAN-architecture"><a href="#2-4-MedGAN-architecture" class="headerlink" title="2.4 MedGAN architecture"></a>2.4 MedGAN architecture</h2><hr>
<p>CasNet架构：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409212128019.png" alt="image.png"></p>
<h3 id="2-4-1-U-blocks"><a href="#2-4-1-U-blocks" class="headerlink" title="2.4.1 U-blocks"></a>2.4.1 U-blocks</h3><hr>
<p>U-块是一个全卷积编码器-解码器网络。<br>编码路径将输入域中的图像（分辨率为256×256）映射到一个高级表示，该表示由8个卷积层堆叠而成，每个卷积层后面都跟着批量归一化和Leaky-ReLU激活函数。<br>卷积滤波器的数量分别为64、128、256、512、512、512、512和512，卷积核大小为4×4，步长为2。<br>解码路径与编码架构相对应，但使用分数步长反卷积，每层之后将分辨率放大两倍。<br>上采样路径的每层分别包含512、1024、1024、1024、1024、512、256和128个滤波器，这些层使用ReLU激活函数，但最后一层反卷积层使用Tanh激活函数。<br>U-块包含跳跃连接。</p>
<h3 id="2-4-2-CasNet"><a href="#2-4-2-CasNet" class="headerlink" title="2.4.2 CasNet"></a>2.4.2 CasNet</h3><hr>
<p>CasNet通过端到端地连接多个U-块来增强MedGAN的生成能力。<br>通过这种网络深度的损失梯度反向传播可能导致梯度消失问题，但是，由于单个U-块内部使用了跳跃连接，这个问题得到了缓解。</p>
<h3 id="2-4-3-Discriminator-architecture"><a href="#2-4-3-Discriminator-architecture" class="headerlink" title="2.4.3 Discriminator architecture"></a>2.4.3 Discriminator architecture</h3><hr>
<p>判别器采用了改进后的PatchGAN架构。<br>PatchGAN的设计不是为了将目标图像和输出图像分类为真实或虚假，而是具有减小的感受野，以便在分类之前将输入图像通过卷积分成更小的图像块，并对结果进行平均。<br>为了避免使用较小块大小时出现的典型平铺伪影，70x70块是常用的块大小。然而，实验发现，将较小的块与先前引入的非对抗性损失（如感知损失和风格迁移损失）结合使用，可以促进更清晰的结果，同时消除传统的平铺伪影。<br>通过结合两个卷积层（分别具有64和128个空间滤波器），随后是批量归一化和Leaky-ReLU激活函数，来实现16x16块的大小。<br>最后，为了输出所需的置信度概率图，使用了一个输出维度为1的卷积层和一个sigmoid激活函数。</p>
<h2 id="2-5-MedGAN-framework-and-training"><a href="#2-5-MedGAN-framework-and-training" class="headerlink" title="2.5 MedGAN framework and training"></a>2.5 MedGAN framework and training</h2><hr>
<p>MedGAN累计损失函数：</p>
<script type="math/tex; mode=display">L_{\text{MedGAN}} = L_{\text{cGAN}} + \lambda_1 L_{\text{perceptual}} + \lambda_2 L_{\text{style}} + \lambda_3 L_{\text{content}}</script><p>$\lambda_1$、$\lambda_2$ 和 $\lambda_3$ 是平衡不同损失组件的超参数。<br>最终选择了 $\lambda_1 = 20$ 和 $\lambda_2 = \lambda_3 = 0.0001$ 。</p>
<p>设置 $\lambda<em>{pi}$ 来使判别器的两层对损失的影响相等。<br>设置 $\lambda</em>{cj}$ 使除最深的卷积块外的所有卷积块对内容损失有影响。<br>对于风格损失，设置$\lambda_{sj}$ 被设定为仅包括预训练的VGG-19网络的第一个和最后一个卷积块的影响。</p>
<p>特征提取器使用一个深度的VGG-19网络，它是在ImageNet分类任务上预训练的。<br>使用ADAM优化器 ，动量值为0.5，学习率为0.0002。<br>使用实例归一化，批量大小为1。<br>每训练CasNet生成器三次时训练一次Patch判别器。<br>生成器的CasNet架构由N=6个U-block组成。</p>
<p>训练过程：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409212148328.png" alt="image.png"></p>
<h1 id="3-Experimental-evaluation"><a href="#3-Experimental-evaluation" class="headerlink" title="3. Experimental evaluation"></a>3. Experimental evaluation</h1><hr>
<h2 id="3-1-Datasets"><a href="#3-1-Datasets" class="headerlink" title="3.1 Datasets"></a>3.1 Datasets</h2><hr>
<h2 id="3-2-Experimental-setup"><a href="#3-2-Experimental-setup" class="headerlink" title="3.2 Experimental setup"></a>3.2 Experimental setup</h2><hr>
<h3 id="3-2-1-Analysis-of-loss-functions"><a href="#3-2-1-Analysis-of-loss-functions" class="headerlink" title="3.2.1 Analysis of loss functions"></a>3.2.1 Analysis of loss functions</h3><hr>
<h3 id="3-2-2-Comparison-with-state-of-the-art-techniques"><a href="#3-2-2-Comparison-with-state-of-the-art-techniques" class="headerlink" title="3.2.2 Comparison with state-of-the-art techniques"></a>3.2.2 Comparison with state-of-the-art techniques</h3><hr>
<h3 id="3-2-3-Perceptual-study-and-validation"><a href="#3-2-3-Perceptual-study-and-validation" class="headerlink" title="3.2.3 Perceptual study and validation"></a>3.2.3 Perceptual study and validation</h3><hr>
<h2 id="3-3-Evaluation-metrics"><a href="#3-3-Evaluation-metrics" class="headerlink" title="3.3 Evaluation metrics"></a>3.3 Evaluation metrics</h2><hr>
<ul>
<li>结构相似性指数（SSIM）</li>
<li>峰值信噪比（PSNR）</li>
<li>均方误差（MSE）</li>
<li>视觉信息保真度（VIF）</li>
<li>通用质量指数（UQI）</li>
<li>学习感知图像块相似性（LPIPS）</li>
</ul>
<h1 id="4-Results"><a href="#4-Results" class="headerlink" title="4. Results"></a>4. Results</h1><hr>
<h2 id="4-1-Analysis-of-loss-functions"><a href="#4-1-Analysis-of-loss-functions" class="headerlink" title="4.1 Analysis of loss functions"></a>4.1 Analysis of loss functions</h2><hr>
<h2 id="4-2-Comparison-with-state-of-the-art-techniques"><a href="#4-2-Comparison-with-state-of-the-art-techniques" class="headerlink" title="4.2 Comparison with state-of-the-art techniques"></a>4.2 Comparison with state-of-the-art techniques</h2><hr>
<h2 id="4-3-Perceptual-study-and-validation"><a href="#4-3-Perceptual-study-and-validation" class="headerlink" title="4.3 Perceptual study and validation"></a>4.3 Perceptual study and validation</h2><hr>
<h1 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5. Discussion"></a>5. Discussion</h1><hr>
<p>讨论MedGAN框架各个部分的性能，以及现有的不足。</p>
<h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><hr>
<p>MedGAN 将条件对抗框架与一种新型的非对抗性损失组合以及CasNET生成器架构相结合，以增强结果的全局一致性和高频细节。<br>未来的工作将致力于扩展MedGAN以支持3D多通道体积的处理，还将研究MedGAN在技术后处理任务中的性能。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yanqinglei.github.io">BulingQAQ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yanqinglei.github.io/gan-mmit/">https://yanqinglei.github.io/gan-mmit/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yanqinglei.github.io" target="_blank">BulingQAQ的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GAN/">GAN</a><a class="post-meta__tags" href="/tags/MedGAN/">MedGAN</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/gan-urlw/" title="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409191616851.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/gan-bmsg/" title="【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409092216234.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-10</div><div class="title">【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training</div></div></a></div><div><a href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409182316153.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-18</div><div class="title">【论文阅读】Generative Adversarial Nets</div></div></a></div><div><a href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409121843542.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-12</div><div class="title">【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</div></div></a></div><div><a href="/gan-itit/" title="【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072345976.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="title">【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks</div></div></a></div><div><a href="/gan-uiti/" title="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112008921.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-11</div><div class="title">【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</div></div></a></div><div><a href="/image-synthesis-review-02/" title="【论文阅读】深度学习图像数据增广方法研究综述"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409061017167.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-07</div><div class="title">【论文阅读】深度学习图像数据增广方法研究综述</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/spy_family_avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">BulingQAQ</div><div class="author-info__description">不知归路，宁愿一世无悔追逐</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YanQinglei"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YanQinglei" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/OctYZ" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1819615836&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1819615836@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">请多多指教</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%EF%BC%9A"><span class="toc-text">背景：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%EF%BC%9A"><span class="toc-text">工作：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-text">结果：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Classical-approaches"><span class="toc-text">1.1 Classical approaches</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Generative-models"><span class="toc-text">1.2 Generative models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-Medical-image-translation"><span class="toc-text">1.3 Medical image translation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-Contributions"><span class="toc-text">1.4 Contributions</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Materials-and-Methods"><span class="toc-text">2. Materials and Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Preliminaries"><span class="toc-text">2.1 Preliminaries</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-Generative-adversarial-networks"><span class="toc-text">2.1.1 Generative adversarial networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-Image-to-image-translation"><span class="toc-text">2.1.2 Image-to-image translation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Perceptual-loss"><span class="toc-text">2.2 Perceptual loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Style-transfer-losses"><span class="toc-text">2.3 Style transfer losses</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-Style-loss"><span class="toc-text">2.3.1 Style loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-Content-loss"><span class="toc-text">2.3.2 Content loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-MedGAN-architecture"><span class="toc-text">2.4 MedGAN architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-U-blocks"><span class="toc-text">2.4.1 U-blocks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-2-CasNet"><span class="toc-text">2.4.2 CasNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-3-Discriminator-architecture"><span class="toc-text">2.4.3 Discriminator architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-MedGAN-framework-and-training"><span class="toc-text">2.5 MedGAN framework and training</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Experimental-evaluation"><span class="toc-text">3. Experimental evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Datasets"><span class="toc-text">3.1 Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Experimental-setup"><span class="toc-text">3.2 Experimental setup</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Analysis-of-loss-functions"><span class="toc-text">3.2.1 Analysis of loss functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Comparison-with-state-of-the-art-techniques"><span class="toc-text">3.2.2 Comparison with state-of-the-art techniques</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-Perceptual-study-and-validation"><span class="toc-text">3.2.3 Perceptual study and validation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Evaluation-metrics"><span class="toc-text">3.3 Evaluation metrics</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Results"><span class="toc-text">4. Results</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Analysis-of-loss-functions"><span class="toc-text">4.1 Analysis of loss functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Comparison-with-state-of-the-art-techniques"><span class="toc-text">4.2 Comparison with state-of-the-art techniques</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Perceptual-study-and-validation"><span class="toc-text">4.3 Perceptual study and validation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Discussion"><span class="toc-text">5. Discussion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Conclusion"><span class="toc-text">6. Conclusion</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/gan-mmit/" title="【论文阅读】MedGAN:Medical image translation using GANs"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】MedGAN:Medical image translation using GANs"/></a><div class="content"><a class="title" href="/gan-mmit/" title="【论文阅读】MedGAN:Medical image translation using GANs">【论文阅读】MedGAN:Medical image translation using GANs</a><time datetime="2024-09-21T14:04:30.000Z" title="发表于 2024-09-21 22:04:30">2024-09-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-urlw/" title="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409191616851.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"/></a><div class="content"><a class="title" href="/gan-urlw/" title="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a><time datetime="2024-09-19T09:19:12.000Z" title="发表于 2024-09-19 17:19:12">2024-09-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409182316153.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Generative Adversarial Nets"/></a><div class="content"><a class="title" href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets">【论文阅读】Generative Adversarial Nets</a><time datetime="2024-09-18T15:14:36.000Z" title="发表于 2024-09-18 23:14:36">2024-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409121843542.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"/></a><div class="content"><a class="title" href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs">【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a><time datetime="2024-09-12T13:36:37.000Z" title="发表于 2024-09-12 21:36:37">2024-09-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-uiti/" title="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112008921.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"/></a><div class="content"><a class="title" href="/gan-uiti/" title="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks">【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a><time datetime="2024-09-11T14:30:23.000Z" title="发表于 2024-09-11 22:30:23">2024-09-11</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021053834.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By BulingQAQ</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>