<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis | BulingQAQ的个人博客</title><meta name="author" content="BulingQAQ"><meta name="copyright" content="BulingQAQ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Abstract  背景： AI在介入性图像分析中的潜在应用仍未被充分挖掘。在实时手术过程中收集的数据进行事后分析存在根本性和实践性的限制，包括伦理考虑、成本、可扩展性、数据完整性以及缺乏真实基准。 工作： 1、展示了一种从人体模型中创建逼真模拟图像的可行方案，作为大规模原位数据收集的替代和补充。 2、 创建X射线图像分析模型迁移范式——称为SyntheX。 贡献： 1、 证明通过在现实合成的数据">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis">
<meta property="og:url" content="https://yanqinglei.github.io/gan-sdat/">
<meta property="og:site_name" content="BulingQAQ的个人博客">
<meta property="og:description" content="Abstract  背景： AI在介入性图像分析中的潜在应用仍未被充分挖掘。在实时手术过程中收集的数据进行事后分析存在根本性和实践性的限制，包括伦理考虑、成本、可扩展性、数据完整性以及缺乏真实基准。 工作： 1、展示了一种从人体模型中创建逼真模拟图像的可行方案，作为大规模原位数据收集的替代和补充。 2、 创建X射线图像分析模型迁移范式——称为SyntheX。 贡献： 1、 证明通过在现实合成的数据">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png">
<meta property="article:published_time" content="2025-03-02T02:21:15.559Z">
<meta property="article:modified_time" content="2025-03-05T06:29:06.528Z">
<meta property="article:author" content="BulingQAQ">
<meta property="article:tag" content="GAN">
<meta property="article:tag" content="Sim2Real">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png"><link rel="shortcut icon" href="/img/icon.png"><link rel="canonical" href="https://yanqinglei.github.io/gan-sdat/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-05 14:29:06'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/spy_family_avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png')"><nav id="nav"><span id="blog-info"><a href="/" title="BulingQAQ的个人博客"><span class="site-name">BulingQAQ的个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-02T02:21:15.559Z" title="发表于 2025-03-02 10:21:15">2025-03-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-05T06:29:06.528Z" title="更新于 2025-03-05 14:29:06">2025-03-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="abstract">Abstract</h1>
<hr>
<h3 id="背景：">背景：</h3>
<p>AI在介入性图像分析中的潜在应用仍未被充分挖掘。在实时手术过程中收集的数据进行事后分析存在根本性和实践性的限制，包括伦理考虑、成本、可扩展性、数据完整性以及缺乏真实基准。</p>
<h3 id="工作：">工作：</h3>
<p>1、展示了一种从人体模型中创建逼真模拟图像的可行方案，作为大规模原位数据收集的替代和补充。<br>
2、 创建X射线图像分析模型迁移范式——称为SyntheX。</p>
<h3 id="贡献：">贡献：</h3>
<p>1、 证明通过在现实合成的数据上训练AI图像分析模型，并结合当代领域泛化技术，能够在真实数据上获得与精确匹配的真实数据训练集相当的表现。<br>
2、 SyntheX甚至能够超越基于真实数据训练的模型，显著加速基于X射线的智能系统的构思、设计和评估提供了机会。</p>
<ul>
<li>真实数据与仿真数据之间的特性差异通常被称为“领域差距”</li>
<li>AI模型在来自不同领域的数据上表现的能力，即与其训练数据存在领域差距的情况，被称为“领域泛化”。</li>
<li>迄今为止尚无研究使用精确匹配的跨领域数据集来孤立领域泛化的影响。</li>
</ul>
<blockquote>
<p>SyntheX框架，该框架旨在开发基于合成数据的通用人工智能算法，用于X射线图像分析，这些合成数据完全由标注的计算机断层扫描（CT）模拟生成。通过从CT中模拟真实的X射线图像生成过程，并利用领域随机化技术训练AI模型，SyntheX创建的AI模型能够在领域转移时保持性能，从而实现对真实世界临床X射线的评估与部署。</p>
</blockquote>
<h1 id="clinical-tasks">Clinical tasks</h1>
<hr>
<p>在三项X射线图像分析下游任务中展示了SyntheX的优势：髋部成像、手术机器人工具检测以及胸部X射线中的COVID-19病灶分割。</p>
<h2 id="hip-imaging-髋骨成像">Hip imaging——髋骨成像</h2>
<hr>
<p>实现空间对齐的一种有效方法是识别2D X射线图像中的已知结构和标志，然后用于推断姿态。<br>
在髋部成像中，我们定义了六个解剖结构和十四个标志作为最相关的已知结构。它们如图2a所示。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png" alt="image.png"></p>
<h2 id="surgical-robotic-tool-detection-手术机器人工具检测">Surgical robotic tool detection——手术机器人工具检测</h2>
<hr>
<p>从术中图像中自动检测手术工具是机器人辅助手术的重要步骤。<br>
由于训练检测模型需要足够且带有真实标签的图像数据，因此只有在手术机器人成熟并投入临床使用后，才能开发此类模型。<br>
我们以连续体机械臂（CM）为目标对象。通过使用SyntheX，我们解决了CM检测问题，包括分割CM主体并在X射线图像中预测特定标志点。语义分割掩码覆盖了27个交替的凹槽，以区分CM与其他手术工具；标志点定义为CM中心线的起点和终点。</p>
<h2 id="covid-19-lesion-segmentation-covid-19-病变分割">COVID-19 lesion segmentation——COVID-19 病变分割</h2>
<hr>
<p>胸部X光（CXR）已成为辅助COVID-19诊断和指导治疗的重要工具。<br>
我们考虑了COVID-19病变分割任务，该任务也可以从CXR中实现以进行比较。我们使用了ImagEng lab发布的开源COVID-19 CT数据集以及电子科技大学（UESTC）发布的CT扫描数据来生成合成CXR图像。<br>
使用自动病变分割方法COPLE-Net为每个CT创建了3D感染掩码。我们遵循了相同的真实X射线合成流程，并使用来自不同几何形状的配对CT扫描和分割掩码生成了合成图像和标签。病变标签按照相同的几何形状进行投影。</p>
<h1 id="precisely-controlled-investigations-on-hip-imaging-精准控制的髋关节影像研究">Precisely controlled investigations on hip imaging——精准控制的髋关节影像研究</h1>
<hr>
<p>针对一个独特的髋关节影像数据集进行了实验，以分离邻域差距对Sim2Real AI模型迁移的影响。<br>
在髋关节X射线的解剖标志点检测和解剖分割任务中，我们研究了最常用的邻域泛化技术，即域随机化和域适应，并进一步考虑了不同的X射线模拟器、图像分辨率和训练数据集大小。</p>
<h2 id="precisely-matched-hip-dataset">Precisely matched hip dataset</h2>
<hr>
<p>对于每张真实的X光图像，我们使用全面的2D/3D图像配准流程精确估计了X光摄像机的姿态。<br>
随后，我们生成了合成的X光图像（数字重建放射影像，DRR），这些图像精确再现了真实X光图像的空间配置和解剖结构，仅在模拟的真实感上有所不同（图3a）。<br>
我们研究了三种不同的X光图像模拟技术：naive DRR generation, xreg DRR10 and DeepDRR，分别称为基础、启发式和真实感模拟。它们在模拟真实X光成像物理效应方面的考虑有所不同。由于合成图像与真实数据集精确匹配，所有2D和3D标签均同等适用。<br>
图3b展示了不同模拟器生成的图像外观与对应真实X光图像的对比。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011654184.png" alt="image.png"></p>
<h1 id="model-and-evaluation-paradigm">Model and evaluation paradigm</h1>
<hr>
<p>TransUNet是一种最先进的医学图像分割框架，用于所有任务。<br>
所有临床应用的分割网络均经过训练以最小化Dice损失（Lseg），该损失评估预测分割标签与真实分割标签之间的重叠。<br>
对于髋关节图像分析和手术工具检测任务，我们对TransUNet架构进行了调整（如扩展数据图2所示），使其能够同步预测解剖标志点位置。真实的标志点位置被表示为以实际坐标为中心对称的高斯分布（当标志点不可见时值为零）。通过引入均方误差损失函数（Lld），对网络预测的热图与参考标志点热图之间的差异进行约束。<br>
在评估阶段，我们通过预测标志点与真实位置之间的l2距离来衡量定位精度。同时，使用Dice分数定量评估髋关节图像和手术工具的分割质量。<br>
对于所有三项任务，我们均报告了Sim2Real（模拟到现实迁移）和Real2Real（现实到现实迁移）的性能表现。Sim2Real性能通过所有测试集真实X射线数据计算得出。Real2Real实验采用k折交叉验证方法开展，最终性能报告为所有测试折结果的平均值。<br>
采用了一种专门设计的评估曲线图进行结果呈现，该方法能够全面反映算法需具备的两个关键性能指标：(1) 标志点检测的完整性；(2) 检测结果的精确性。网络对每个标志点的直接输出为热图强度图像（I）。为量化预测置信度，我们计算预测热图I与高斯基准热图Igauss的归一化互相关（normalized cross-correlation, ncc），即ncc(I, Igauss)¹²。当ncc值高于置信度阈值φ时（即ncc(I, Igauss) &gt; φ），判定该标志点为有效激活状态。</p>
<p>基于TransUNet的并发分割与关键点检测网络架构，适用于多任务学习，扩展数据图2：<br>
<img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011902095.png" alt="image.png"></p>
<h1 id="result">Result</h1>
<hr>
<h2 id="primary-findings">Primary findings</h2>
<hr>
<p>使用SyntheX Sim2Real模型迁移范式训练的模型在真实数据上的表现与直接在真实数据上训练的模型相当，甚至更好。</p>
<h2 id="hip-imaging">Hip imaging</h2>
<hr>
<p>Sim2Real预测的稳定性更高，其标准偏差较小。<br>
标志点检测平均误差与在366张西门子真实X射线图像上报告的性能相似。<br>
单个解剖标志点和结构大多数标志点上的检测精度优于或与Real2Real相当，但耻骨联合上缘和下缘除外。<br>
Sim2Real在所有六个结构中的分割性能均优于Real2Real。</p>
<h2 id="surgical-robotic-tool-detection">Surgical robotic tool detection</h2>
<hr>
<p>Sim2Real误差的标准差明显更小。<br>
在分割Dice得分方面，Sim2Real大幅优于Real2Real。</p>
<h2 id="covid-19-lesion-segmentation">COVID-19 lesion segmentation</h2>
<hr>
<p>在敏感性和特异性方面，Sim2Real的表现与Real2Real相似，但在其他指标上稍逊一筹。</p>
<h2 id="sim2real-benchmark-findings">Sim2Real benchmark findings</h2>
<hr>
<p>基于我们精确控制的髋部成像消融研究，包括对（1）模拟环境、（2）域随机化和域适应效果、（3）图像分辨率的比较，我们观察到，使用强域随机化的真实模拟进行训练的效果与使用真实数据训练的模型或使用域适应的合成数据训练的模型相当，且在训练时无需任何真实数据。</p>
<h2 id="the-effect-of-domain-randomization">The effect of domain randomization</h2>
<hr>
<p>在所有实验中，我们观察到使用强域随机化训练的神经网络始终比使用常规域随机化训练的网络表现更好。这是预期的，因为强域随机化引入了更剧烈的增强，采样了更广泛的图像外观谱，并促进了发现更具鲁棒性的特征，从而减少了过拟合的风险。</p>
<h2 id="the-effect-of-domain-adaptation">The effect of domain adaptation</h2>
<hr>
<p>无论是realistic-CycleGAN还是naive-CycleGAN，其表现均与Real2Real相当（图4d和4f）。<br>
与仅在各自合成域上训练相比（图4a和4c），性能的提升证实了CycleGAN在域泛化方面的有效性。<br>
与常规域随机化相比，强域随机化的ADDA表现有所下降（图4e和4i）。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503012054669.png" alt="image.png"></p>
<h2 id="scaling-up-the-training-data">Scaling up the training data</h2>
<hr>
<p>随着训练数据和几何多样性的增加，我们发现所有扩大规模的实验在基准数据集上均优于Real2Real基线（图4g,h）。<br>
图5展示了该合成数据训练模型在应用于真实数据时的检测性能的定性可视化结果。这一结果表明，在大规模真实合成数据上进行强领域随机化和/或适应训练，是替代真实数据训练的一种可行方案。</p>
<p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503012059788.png" alt="image.png"></p>
<h1 id="discussion">Discussion</h1>
<hr>
<p>使用SyntheX训练的模型在真实数据上的性能达到或超过了基于真实数据训练的模型。</p>
<ul>
<li>使用DeepDRR框架进行基于物理的真实模拟生成训练数据，相比基于简单或启发式模拟范式训练的模型，能够更好地泛化到真实数据领域。</li>
<li>真实模拟与强领域随机化（SyntheX）相结合，在匹配数据集上训练时，表现与最佳领域适应方法（CycleGAN结合领域随机化）和真实数据训练相当。然而，由于SyntheX在训练时不需要任何真实数据，这一范式相比领域适应具有明显优势。</li>
</ul>
<p>Sim2Real模型迁移在真实数据和相应标注特别难以获取的场景中表现最佳。而在真实数据丰富的场景中，Sim2Real几乎无法与Real2Real的性能相媲美。</p>
<h1 id="conclusion">Conclusion</h1>
<hr>
<p>本文展示了结合领域泛化或适应技术的人体模型图像生成真实模拟是大规模真实数据收集的可行替代方案。<br>
结合真实合成训练和强领域随机化的方法（我们称之为 SyntheX）尤为有前景。</p>
<h1 id="methods">Methods</h1>
<hr>
<h2 id="domain-randomization">Domain randomization</h2>
<hr>
<p><strong>常规域随机化</strong>（每轮训练均应用）：</p>
<ol>
<li><strong>高斯噪声注入</strong>：x + N(0, σ)，其中N为标准正态分布，σ从(0.005, 0.1)区间均匀采样后乘以图像强度范围。</li>
<li><strong>伽马变换</strong>：norm(x)^γ，x经最大-最小值归一化后，γ从(0.7, 1.3)区间均匀采样。</li>
<li><strong>随机裁剪</strong>：以原图尺寸90%的方形区域随机裁剪x。</li>
</ol>
<p><strong>强域随机化</strong>（每轮训练随机选择至多两种方法组合应用，同时保留基础增强）：</p>
<ol>
<li><strong>反色</strong>：max(x) − x，将图像像素值取反。</li>
<li><strong>脉冲/椒盐噪声注入</strong>：随机选择10%的像素替换为脉冲、胡椒或盐噪声。</li>
<li><strong>仿射变换</strong>：施加包含平移、旋转、剪切和缩放的随机二维仿射形变。</li>
<li><strong>对比度调整</strong>：采用线性、对数或Sigmoid对比度变换处理图像。</li>
<li><strong>模糊化</strong>：应用高斯模糊（N(μ=0, σ=3.0)）或均值模糊（核尺寸2×2至7×7）。</li>
<li><strong>区域噪声破坏</strong>：随机添加含强噪声的矩形框区域。</li>
<li><strong>像素丢弃</strong>：随机将1–10%的像素置零，或以矩形区域丢弃图像2–5%的像素。</li>
<li><strong>锐化与浮雕化</strong>：锐化通过原图与锐化版本（混合系数α∈[0,1]）叠加实现，浮雕化则直接叠加锐化版本。</li>
<li><strong>池化操作</strong>：随机应用平均池化、最大池化、最小池化或中值池化（核尺寸2×2至4×4）。</li>
<li><strong>亮度调整</strong>：通过像素值乘以50–150%的系数改变亮度。</li>
<li><strong>局部形变</strong>：对图像局部施加随机分段仿射变换。</li>
</ol>
<h2 id="domain-adaptation">Domain adaptation</h2>
<hr>
<p>本研究选取两种广泛应用的域适应方法进行对比分析：CycleGAN与ADDA。</p>
<p><strong>CycleGAN</strong>：在任务网络训练前，使用未配对的合成图像与真实图像训练CycleGAN模型。随后，所有合成图像均通过训练后的CycleGAN生成器进行处理，使其外观与真实数据对齐。为确保数据独立性，我们严格遵循任务模型训练时的数据划分原则，确保测试集图像在CycleGAN训练和任务网络训练中均不参与。</p>
<p><strong>ADDA</strong>：该方法通过引入对抗性判别器分支作为附加损失函数，以区分源自合成图像与真实图像的特征差异。我们参考文献的设计思路，针对语义分割任务构建ADDA的判别器结构。</p>
<p><strong>实验设置</strong>：两种方法（CycleGAN与ADDA）均在经优化的仿真图像（realistic simulation）与未经优化的基础仿真图像（naive simulation）上进行性能测试，以全面评估其域适应能力。</p>
<h2 id="clinical-tasks-experimental-details">Clinical tasks experimental details</h2>
<hr>
<p>图像分辨率为1,536 × 1,536像素，各向同性像素间距为0.194毫米/像素，源到探测器距离为1,020毫米，主点位于图像中心。</p>
<h3 id="hip-imaging">Hip imaging</h3>
<p>仿真中，CT体积的旋转角度在[−45°, 45°]内均匀采样，平移范围分别为左右方向[−50 mm, 50 mm]、上下方向[−20 mm, 20 mm]、前后方向[−100 mm, 100 mm]。共生成18,000张训练图像和2,000张验证图像。<br>
通过投影几何将3D分割掩膜及解剖标志点投影至2D平面作为真值标签。</p>
<h3 id="robotic-surgical-tool-detection">Robotic surgical tool detection</h3>
<p>通过从高斯分布N(μ=0°, σ=2.5°)采样曲率控制点角度，生成100种不同构型的CM（手术工具）体素化模型。<br>
CM基座位姿在左/右前斜位（LAO/RAO）[−30°, 30°]、头/尾位（CRAN/CAUD）[−10°, 10°]范围内均匀采样，源到等中心距离为[600 mm, 900 mm]，x/y轴平移服从N(μ=0 mm, σ=10 mm)。</p>
<h3 id="covid-19-lesion-segmentation">COVID-19 lesion segmentation</h3>
<p>CT视角位姿在三个轴向旋转[−5°, 5°]、源到等中心距离[350 mm, 650 mm]内均匀采样，生成18,000张224×224像素训练图像和1,800张验证图像。CT施加[−30°, 30°]随机剪切变换，分割阈值设为0.5。</p>
<h3 id="benchmark-hip-imaging-investigation">Benchmark hip-imaging investigation</h3>
<p>本研究采用三种DRR（数字重建放射影像）仿真器生成合成数据：</p>
<ol>
<li><strong>基础仿真（Naive DRR）</strong>
<ul>
<li>仅进行简单射线投射，未考虑任何成像物理效应。</li>
<li>假设条件：单能射线源、单一材质物体、无噪声或散射等图像退化因素。</li>
</ul>
</li>
<li><strong>启发式仿真（xreg DRR）</strong>
<ul>
<li>在射线投射前，对CT亨氏单位进行线性阈值处理以区分空气与解剖组织材质。</li>
<li>虽能提升组织对比度使DRR更逼真，但仍未建模成像物理效应。</li>
</ul>
</li>
<li><strong>真实物理仿真（DeepDRR）</strong>
<ul>
<li>完整模拟X射线物理过程：考虑X射线源全频谱能量分布，基于机器学习实现材质分解与散射估计。</li>
<li>引入信号相关噪声、读出噪声及探测器饱和效应，高度还原真实成像链特性。</li>
</ul>
</li>
</ol>
<h2 id="network-training-details">Network training details</h2>
<hr>
<p>网络训练采用随机梯度下降法，初始学习率为0.1，Nesterov动量为0.9，权重衰减为0.00001，批量大小为5张图像。训练过程中，学习率每10个epoch以0.5的gamma值衰减。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://yanqinglei.github.io">BulingQAQ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://yanqinglei.github.io/gan-sdat/">https://yanqinglei.github.io/gan-sdat/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yanqinglei.github.io" target="_blank">BulingQAQ的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GAN/">GAN</a><a class="post-meta__tags" href="/tags/Sim2Real/">Sim2Real</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/image-synthesis-review-dlaf/" title="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503021507800.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review</div></div></a></div><div class="next-post pull-right"><a href="/image-synthesis-review-03/" title="【论文阅读】Generating Synthetic Data for Medical Imaging"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202412031045821.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【论文阅读】Generating Synthetic Data for Medical Imaging</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409182316153.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-18</div><div class="title">【论文阅读】Generative Adversarial Nets</div></div></a></div><div><a href="/gan-itfm/" title="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-05</div><div class="title">【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation</div></div></a></div><div><a href="/gan-bmsg/" title="【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409092216234.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-10</div><div class="title">【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training</div></div></a></div><div><a href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409121843542.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-12</div><div class="title">【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</div></div></a></div><div><a href="/gan-itit/" title="【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072345976.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="title">【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks</div></div></a></div><div><a href="/gan-mmit/" title="【论文阅读】MedGAN:Medical image translation using GANs"><img class="cover" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-21</div><div class="title">【论文阅读】MedGAN:Medical image translation using GANs</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/spy_family_avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">BulingQAQ</div><div class="author-info__description">不知归路，宁愿一世无悔追逐</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YanQinglei"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YanQinglei" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/OctYZ" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1819615836&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1819615836@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">请多多指教</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#abstract"><span class="toc-text">Abstract</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%EF%BC%9A"><span class="toc-text">背景：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%EF%BC%9A"><span class="toc-text">工作：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE%EF%BC%9A"><span class="toc-text">贡献：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#clinical-tasks"><span class="toc-text">Clinical tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hip-imaging-%E9%AB%8B%E9%AA%A8%E6%88%90%E5%83%8F"><span class="toc-text">Hip imaging——髋骨成像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#surgical-robotic-tool-detection-%E6%89%8B%E6%9C%AF%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%B7%A5%E5%85%B7%E6%A3%80%E6%B5%8B"><span class="toc-text">Surgical robotic tool detection——手术机器人工具检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#covid-19-lesion-segmentation-covid-19-%E7%97%85%E5%8F%98%E5%88%86%E5%89%B2"><span class="toc-text">COVID-19 lesion segmentation——COVID-19 病变分割</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#precisely-controlled-investigations-on-hip-imaging-%E7%B2%BE%E5%87%86%E6%8E%A7%E5%88%B6%E7%9A%84%E9%AB%8B%E5%85%B3%E8%8A%82%E5%BD%B1%E5%83%8F%E7%A0%94%E7%A9%B6"><span class="toc-text">Precisely controlled investigations on hip imaging——精准控制的髋关节影像研究</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#precisely-matched-hip-dataset"><span class="toc-text">Precisely matched hip dataset</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#model-and-evaluation-paradigm"><span class="toc-text">Model and evaluation paradigm</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#result"><span class="toc-text">Result</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#primary-findings"><span class="toc-text">Primary findings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hip-imaging"><span class="toc-text">Hip imaging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#surgical-robotic-tool-detection"><span class="toc-text">Surgical robotic tool detection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#covid-19-lesion-segmentation"><span class="toc-text">COVID-19 lesion segmentation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sim2real-benchmark-findings"><span class="toc-text">Sim2Real benchmark findings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-effect-of-domain-randomization"><span class="toc-text">The effect of domain randomization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-effect-of-domain-adaptation"><span class="toc-text">The effect of domain adaptation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scaling-up-the-training-data"><span class="toc-text">Scaling up the training data</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#discussion"><span class="toc-text">Discussion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#conclusion"><span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#methods"><span class="toc-text">Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#domain-randomization"><span class="toc-text">Domain randomization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#domain-adaptation"><span class="toc-text">Domain adaptation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#clinical-tasks-experimental-details"><span class="toc-text">Clinical tasks experimental details</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hip-imaging"><span class="toc-text">Hip imaging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#robotic-surgical-tool-detection"><span class="toc-text">Robotic surgical tool detection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#covid-19-lesion-segmentation"><span class="toc-text">COVID-19 lesion segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#benchmark-hip-imaging-investigation"><span class="toc-text">Benchmark hip-imaging investigation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#network-training-details"><span class="toc-text">Network training details</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/gan-itfm/" title="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503050919044.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation"/></a><div class="content"><a class="title" href="/gan-itfm/" title="【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation">【论文阅读】Image Translation for Medical Image Generation：Ischemic Stroke Lesion Segmentation</a><time datetime="2025-03-05T06:24:58.000Z" title="发表于 2025-03-05 14:24:58">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/image-synthesis-review-dlaf/" title="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503021507800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review"/></a><div class="content"><a class="title" href="/image-synthesis-review-dlaf/" title="【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review">【论文阅读】Deep Learning Approaches for Data Augmentation in Medical Imaging：A Review</a><time datetime="2025-03-03T07:43:01.091Z" title="发表于 2025-03-03 15:43:01">2025-03-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-sdat/" title="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202503011627746.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis"/></a><div class="content"><a class="title" href="/gan-sdat/" title="【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis">【论文阅读】Synthetic data accelerates the development of generalizable learning-based algorithms for X-ray image analysis</a><time datetime="2025-03-02T02:21:15.559Z" title="发表于 2025-03-02 10:21:15">2025-03-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/image-synthesis-review-03/" title="【论文阅读】Generating Synthetic Data for Medical Imaging"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202412031045821.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Generating Synthetic Data for Medical Imaging"/></a><div class="content"><a class="title" href="/image-synthesis-review-03/" title="【论文阅读】Generating Synthetic Data for Medical Imaging">【论文阅读】Generating Synthetic Data for Medical Imaging</a><time datetime="2024-12-03T03:25:53.000Z" title="发表于 2024-12-03 11:25:53">2024-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409301638643.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"/></a><div class="content"><a class="title" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合">【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合</a><time datetime="2024-10-10T13:52:22.000Z" title="发表于 2024-10-10 21:52:22">2024-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021053834.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By BulingQAQ</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>