<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>BulingQAQ的个人博客</title><meta name="author" content="BulingQAQ"><meta name="copyright" content="BulingQAQ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="不知归路，宁愿一世无悔追逐">
<meta property="og:type" content="website">
<meta property="og:title" content="BulingQAQ的个人博客">
<meta property="og:url" content="https://yanqinglei.github.io/">
<meta property="og:site_name" content="BulingQAQ的个人博客">
<meta property="og:description" content="不知归路，宁愿一世无悔追逐">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yanqinglei.github.io/img/spy_family_avatar.jpg">
<meta property="article:author" content="BulingQAQ">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yanqinglei.github.io/img/spy_family_avatar.jpg"><link rel="shortcut icon" href="/img/icon.png"><link rel="canonical" href="https://yanqinglei.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BulingQAQ的个人博客',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-10-10 21:54:53'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/spy_family_avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/index_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="BulingQAQ的个人博客"><span class="site-name">BulingQAQ的个人博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-camera"></i><span> 生活</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">BulingQAQ的个人博客</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/YanQinglei" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/OctYZ" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1819615836&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1819615836@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409301638643.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合">【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-10-10T13:52:22.000Z" title="发表于 2024-10-10 21:52:22">2024-10-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88/">图像融合</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/VGG/">VGG</a></span></div><div class="content">3. 基于协同学习机制的 CNN 肺肿瘤图像融合

3.1 网络架构


三部分：

两个独立的编码器
协同学习与融合模块
图像重建部分

3.1.1 特定模态编码器

编码器：
（卷积层×2+最大池化层）×3
每一 次卷积后都进行以 0 为均值，单位方差分布的归一化。
归一化之后再使用 LeakyReLU 函数进行激活。
卷积层输出：
F=LeakyReLU(W∗X+b)F = LeakyReLU(W*X+b)
F=LeakyReLU(W∗X+b)

3.1.2 协同学习CNN激活函数

ReLU
LeakyReLU
3.1.3 协同学习CNN损失函数

交叉熵损失函数
e=−[ylog(p)+(1−y)log(1−p)]e = -[ylog(p)+(1-y)log(1-p)]
e=−[ylog(p)+(1−y)log(1−p)]
3.1.4 多模态特征协同学习和融合模块

（1）协同学习
两部分：
协同学习单元
融合操作
FCT:w∗h∗cF_{CT}: w*h*cFCT​:w∗h∗c
FPET:w∗h∗cF_{PET}: w*h*cFPET​:w∗h∗c
穿插堆叠：Xmulti: ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/trans-aiay/" title="【论文阅读】Attention is All you Need"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202410041958010.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Attention is All you Need"></a></div><div class="recent-post-info"><a class="article-title" href="/trans-aiay/" title="【论文阅读】Attention is All you Need">【论文阅读】Attention is All you Need</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-10-06T08:38:17.000Z" title="发表于 2024-10-06 16:38:17">2024-10-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">Abstract

贡献：
提出了Transformer，完全基于注意力机制，摒弃了循环和卷积网络。
结果：
本模型在质量上优于现有模型，同时具有更高的并行性，并且显著减少了训练时间。
1. Introduction


long short-term memory（LSTM）——长短期记忆网络
gated recurrent neural networks——门控循环神经网络

循环模型通常沿着输入和输出序列的符号位置来分解计算。通过将位置与计算时间步骤对齐，它们生成一系列隐藏状态 ht，作为前一个隐藏状态 ht−1 和位置 t 的输入的函数。
Transformer完全摒弃循环网络、完全依赖注意力机制来捕捉输入和输出之间全局依赖关系的模型架构。
显著允许增加并行化。
2. Background

减少序列计算。

Extended Neural GPU
ByteNet
ConvS2S
在这些模型中，关联任意两个输入或输出位置信号所需的操作次数随着位置之间的距离而增加。

Self-attention，是一种通过关联同一序列中不同位置来计算该序列表示的注意力机制。
End-to-en ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/gan-pccm/" title="【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409291040191.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-pccm/" title="【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN">【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-29T14:02:52.000Z" title="发表于 2024-09-29 22:02:52">2024-09-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88/">图像融合</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DCIF-GAN/">DCIF-GAN</a></span></div><div class="content">摘要

背景：
基于GAN的融合方法存在训练不稳定，提取图像的局部和全局上下文语义信息能力不足，交互融合程度不够等问题
贡献：
提出双耦合交互式融合GAN（Dual-Coupled Interactive Fusion GAN，DCIF-GAN）：

设计了双生成器双鉴别器GAN，通过权值共享机制实现生成器之间和鉴别器之间的耦合，通过全局自注意力机制实现交互式融合；
设计耦合CNN-Transformer的特征提取模块（Coupled CNN-T ransformer Feature Extraction Module, CC-TFEM）和特征重构模块（CNN-T ransformer F eature Reconstruction Module, C-TFRM），提升了对同一模态图像内部的局部和全局特征信息提取能力；
设计跨模态交互式融合模块（Cross Model Intermodal Fusion Module, CMIFM），通过跨模态自注意力机制，进一步整合不同模态间的全局交互信息。

结果：
在肺部肿瘤PET/CT医学图像数据集上进行实验，模型能够突出病变区域信息，融合图像 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/image-fusion-review-01/" title="【论文阅读】多模态医学图像融合方法的研究进展"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409291905818.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】多模态医学图像融合方法的研究进展"></a></div><div class="recent-post-info"><a class="article-title" href="/image-fusion-review-01/" title="【论文阅读】多模态医学图像融合方法的研究进展">【论文阅读】多模态医学图像融合方法的研究进展</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-29T11:00:32.000Z" title="发表于 2024-09-29 19:00:32">2024-09-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88/">图像融合</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/CNN/">CNN</a></span></div><div class="content">摘要

背景：
多模态融合技术可将多模态的医学图像融合到单模态的图像中，且单模态图像具有多种模态图像间的互补信息， 从而在单一图像中得到充足的便于临床诊断的信息。
贡献：
本文将多模态医学图像融合方法整理为两种，分别为传统融合方法和基于深度学习的融合方法。
0. 引言

图像融合是图像处理的子领域。
多模态医学图像：MRI、CT、PET、SPECT、US。
1. 传统融合方法

多尺度变换（MST）、稀疏表示（SR）、基于子空间、基于显著特征、混合模型
MST：
基于MST的方法包括多尺度分解、多尺度融合和多尺度重建等步骤。

具有多个特征的联合拉普拉斯金字塔方法
使用高斯滤波技术提高图像质量，然后使用离散小波变换增强融合图像的效果
从一组结合一系列稀疏系数的训练图像中学习到的过完整字典中生成融合图像
将已配准的医学图像按照块的几何方向划分为分类块
使用稀疏表示和邻域能量活动算子将源图像分为基础层和细节层
具有组稀疏性和图正则化的字典学习（DL-GSGR）

基于子空间：
主要包括主成分分析（PCA）、独立成分分析（ICA）和非负矩阵分解（NMF）。

将强度-色调-饱和度变换和主成分 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/gan-wgan/" title="【论文阅读】Wasserstein GAN"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409291311952.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Wasserstein GAN"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-wgan/" title="【论文阅读】Wasserstein GAN">【论文阅读】Wasserstein GAN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-29T06:26:18.000Z" title="发表于 2024-09-29 14:26:18">2024-09-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/WGAN/">WGAN</a></span></div><div class="content">贡献：

彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度
基本解决了collapse mode的问题，确保了生成样本的多样性
训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高 
以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到

改进：

判别器最后一层去掉sigmoid
生成器和判别器的loss不取log
每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

1. 原始GAN的问题

1.1 判别器越好，生成器梯度消失越严重

原始GAN训练有一个trick，就是别把判别器训练得太好，否则在实验中生成器会完全学不动（loss降不下去）。
根据原始GAN定义的判别器loss，可以得到最优判别器的形式；而在最优判别器的下，可以把原始GAN定义的生成器loss等价变换为最小化真实分布Pr与生成分布Pg之间的JS散度。
越训练判别器，它就越接近最优，最小化生成器的 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/gan-mmit/" title="【论文阅读】MedGAN:Medical image translation using GANs"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409202307724.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】MedGAN:Medical image translation using GANs"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-mmit/" title="【论文阅读】MedGAN:Medical image translation using GANs">【论文阅读】MedGAN:Medical image translation using GANs</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-21T14:04:30.000Z" title="发表于 2024-09-21 22:04:30">2024-09-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/MedGAN/">MedGAN</a></span></div><div class="content">Abstract

背景：
图像到图像的转换被认为是医学图像分析领域的一个新的前沿领域。
工作：
提出了一种新的医学图像到图像的转换框架MedGAN：
将对抗性框架与非对抗性损失的新组合相结合；
生成器——CasNet，该结构通过编码器-解码器对的逐步细化来增强翻译后的医学输出的清晰度；
判别器——作为一个可训练的特征提取惩罚之间的差异转换医学图像和期望的模式；
利用风格传递损失将目标图像的纹理和精细结构匹配到转换后的图像；
应用于三个不同的任务：PETCT翻译、MR运动伪影校正和PET图像去噪。
结果：
MedGAN优于其他现有的转换方法。
1. Introduction

由于可能引入不切实际的信息，将输入图像模态转换为输出模态的任务具有挑战性。这显然会使合成图像不可靠，无法用于诊断目的。
1.1 Classical approaches

1.2 Generative models

1.3 Medical image translation

1.4 Contributions

MedGAN的主要目的不是诊断，而是进一步增强需要全局一致图像属性的技术后处理任务。

MedGA ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/gan-urlw/" title="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409191616851.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-urlw/" title="【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">【论文阅读】Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-19T09:19:12.000Z" title="发表于 2024-09-19 17:19:12">2024-09-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DCGAN/">DCGAN</a></span></div><div class="content">Abstract

背景：
希望能缩小CNN在监督学习和无监督学习之间成功应用的差距。
贡献：
引入了一类称为深度卷积生成对抗网络（DCGAN）的CNN。
结果：
DCGAN 在生成器和判别器中都能从对象到场景学习表示层次结构。
1. Introduction

贡献：

提出DCGAN
用于图像分类任务，展示其性能
对滤波器进行了可视化，证明特定滤波器已经学会了绘制特定对象
生成器具有向量算术特性，允许轻松操作生成样本的许多语义特征

2. Related Work

2.1 Representation Learning From Unlabeled Data——基于未标记数据的表示学习


聚类
自动编码器

2.2 Generating Natural Images——生成自然图像

生成图像模型分为两类：参数和非参数。
2.3 Visualizing The Internals Of CNNs——CNN内部可视化

使用输入上的梯度下降，可以检查激活某些滤波器子集的理想图像。
3. Approach And Model Architecture


DCGAN的架构：

用带 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409182316153.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Generative Adversarial Nets"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-gan/" title="【论文阅读】Generative Adversarial Nets">【论文阅读】Generative Adversarial Nets</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-18T15:14:36.000Z" title="发表于 2024-09-18 23:14:36">2024-09-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a></span></div><div class="content">Abstract

本文贡献：
提出GAN：
生成模型 G ，生成模型用来捕获数据的分布；
辨别模型 D ，辨别模型用来判断样本是来自于训练数据还是生成模型生成的。
在任意函数空间里，存在唯一解，G 能找出训练数据的真实分布，而 D 的预测概率为 12\frac{1}{2}21​ 。
结果：
该框架是可行的。
1. Introduction

生成模型问题：在最大似然估计时会遇到很多棘手的近似概率计算。
对抗网络：生成模型与判别模型相比较，学习确定样本是来自模型分布还是来自数据分布。
生成模型可以通过多层感知机来实现，输入为一些随机噪声，可以通过反向传播来训练。
2. Related work


Boltzmann machine（玻尔兹曼机）：似然函数难以处理，需要多次近似
Generative stochastic networks（生成式随机网络）：用精确的反向传播进行训练
Markov chains（马尔可夫链）：本文通过消除生成随机网络中的马尔可夫链，扩展了生成模型的概念
variational autoencoders（VAE，变分自编码器）：将可微生成网络与执行近似推理 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409121843542.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-hris/" title="【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs">【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-12T13:36:37.000Z" title="发表于 2024-09-12 21:36:37">2024-09-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a></span></div><div class="content">Abstract

方法：
一种新的对抗性损失函数以及新颖的多尺度生成器和判别器架构，生成了2048×1024分辨率的图像。
加入了两个用于交互式视觉操作的新功能。

整合了对象实例分割信息，使得能够进行对象的操作，例如移除/添加对象以及更改对象类别。
提出了一种方法，在相同输入下生成多样化的结果，允许用户交互式地编辑对象外观。

1. Introduction

讨论了一种新的方法，产生高分辨率图像的语义标签映射。
首先仅通过对抗性训练获得结果，而不依赖任何手工制作的损失或预先训练的网络进行感知损失。
然后，如果预训练网络可用，则添加来自预训练网络的感知损失可以在某些情况下略微改善结果。
利用实例级对象分割信息，将同一类别内的不同对象实例进行分割。
提出了一种方法来生成不同的结果，给定相同的输入标签映射，允许用户交互编辑相同对象的外观。
2. Related Work

Generative adversarial networks（生成对抗网络）：
旨在通过强制生成的样本与自然图像不可区分来模拟自然图像分布。
Image-to-image translation（图像到图像的转换） ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/gan-uiti/" title="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"><img class="post-bg" src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112008921.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"></a></div><div class="recent-post-info"><a class="article-title" href="/gan-uiti/" title="【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks">【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-09-11T14:30:23.000Z" title="发表于 2024-09-11 22:30:23">2024-09-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/">图像生成</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GAN/">GAN</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/CycleGAN/">CycleGAN</a></span></div><div class="content">Abstract

背景：
成对训练数据缺乏。
方法：
学习一个映射 G:X→YG:X→YG:X→Y，使得来自G(X)的图像分布与使用对抗性损失的分布Y是不可区分的。
由于该映射是高度欠约束的，将其与逆映射 F:Y→XF:Y→XF:Y→X 耦合，并引入循环一致性损失提出 F(G(X))≈XF(G(X))≈XF(G(X))≈X。
1. Introduction

提出系统：在没有任何成对的训练例子，捕捉一个图像采集的特殊特征，找出如何将这些特征转化为其他图像采集。
尽管缺乏成对示例形式的监督，但可以利用集合级别的监督：给定领域 X 中的一组图像和领域 Y 中的另一组图像。
训练一个映射 G:X→YG:X→YG:X→Y，使得 x∈Xx∈Xx∈X 的输出 y^=G(x)\hat{y} = G(x)y^​=G(x) ，对于对抗器来说 y^\hat{y}y^​ 与领域 Y 中的图像 yyy 无法区分。
理论上，这一目标可以在 y^\hat{y}y^​​ 上引导出一个输出分布，使其与经验分布 pY(y)p_Y(y)pY​(y) 相匹配（通常需要 G 是随机的）。
因此，最优的 G 将领域 X 转换 ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/spy_family_avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">BulingQAQ</div><div class="author-info__description">不知归路，宁愿一世无悔追逐</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YanQinglei"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YanQinglei" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/OctYZ" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1819615836&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1819615836@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">请多多指教</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409301638643.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合"/></a><div class="content"><a class="title" href="/gan-vgg19/" title="【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合">【论文阅读】基于深度学习的肺肿瘤PET-CT图像融合</a><time datetime="2024-10-10T13:52:22.000Z" title="发表于 2024-10-10 21:52:22">2024-10-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/trans-aiay/" title="【论文阅读】Attention is All you Need"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202410041958010.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Attention is All you Need"/></a><div class="content"><a class="title" href="/trans-aiay/" title="【论文阅读】Attention is All you Need">【论文阅读】Attention is All you Need</a><time datetime="2024-10-06T08:38:17.000Z" title="发表于 2024-10-06 16:38:17">2024-10-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-pccm/" title="【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409291040191.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN"/></a><div class="content"><a class="title" href="/gan-pccm/" title="【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN">【论文阅读】PET/CT Cross-modal medical image fusion of lung tumors based on DCIF-GAN</a><time datetime="2024-09-29T14:02:52.000Z" title="发表于 2024-09-29 22:02:52">2024-09-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/image-fusion-review-01/" title="【论文阅读】多模态医学图像融合方法的研究进展"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409291905818.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】多模态医学图像融合方法的研究进展"/></a><div class="content"><a class="title" href="/image-fusion-review-01/" title="【论文阅读】多模态医学图像融合方法的研究进展">【论文阅读】多模态医学图像融合方法的研究进展</a><time datetime="2024-09-29T11:00:32.000Z" title="发表于 2024-09-29 19:00:32">2024-09-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/gan-wgan/" title="【论文阅读】Wasserstein GAN"><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409291311952.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】Wasserstein GAN"/></a><div class="content"><a class="title" href="/gan-wgan/" title="【论文阅读】Wasserstein GAN">【论文阅读】Wasserstein GAN</a><time datetime="2024-09-29T06:26:18.000Z" title="发表于 2024-09-29 14:26:18">2024-09-29</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="card-category-list-name">论文阅读</span><span class="card-category-list-count">15</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%88%86%E5%89%B2/"><span class="card-category-list-name">分割</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"><span class="card-category-list-name">图像生成</span><span class="card-category-list-count">11</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88/"><span class="card-category-list-name">图像融合</span><span class="card-category-list-count">3</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/BoneMetastasis/" style="font-size: 1.1em; color: #999">BoneMetastasis</a> <a href="/tags/WGAN/" style="font-size: 1.1em; color: #999">WGAN</a> <a href="/tags/GAN/" style="font-size: 1.5em; color: #99a9bf">GAN</a> <a href="/tags/DCGAN/" style="font-size: 1.1em; color: #999">DCGAN</a> <a href="/tags/CycleGAN/" style="font-size: 1.1em; color: #999">CycleGAN</a> <a href="/tags/Transformer/" style="font-size: 1.1em; color: #999">Transformer</a> <a href="/tags/DCIF-GAN/" style="font-size: 1.1em; color: #999">DCIF-GAN</a> <a href="/tags/MedGAN/" style="font-size: 1.1em; color: #999">MedGAN</a> <a href="/tags/CNN/" style="font-size: 1.1em; color: #999">CNN</a> <a href="/tags/VGG/" style="font-size: 1.1em; color: #999">VGG</a> <a href="/tags/SPECT/" style="font-size: 1.1em; color: #999">SPECT</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><span class="card-archive-list-count">13</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">16</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">42k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-10-10T13:54:53.699Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021053834.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By BulingQAQ</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生","虽千万人吾往矣","长风破浪会有时，直挂云帆济沧海","三军可夺帅也，匹夫不可夺志也","俱往矣，数风流人物，还看今朝","问汝平生功业，黄州惠州儋州"])
  } else {
    document.getElementById("subtitle").textContent = "竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生"
  }
}
typedJSFn.run(subtitleType)</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>