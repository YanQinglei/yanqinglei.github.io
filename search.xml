<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks</title>
      <link href="/gans-itit/"/>
      <url>/gans-itit/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="本文贡献：">本文贡献：</h3><ul><li>研究条件对抗网络作为一个通用的解决图像到图像的翻译问题的方案。</li><li>不仅学习从输入图像到输出图像的映射，而且学习一个损失函数来训练这种映射。</li></ul><h3 id="结果：">结果：</h3><ul><li>使得对传统上需要不同的损失公式的问题采用相同的普遍方法成为可能。</li></ul><h1 id="1-Introduction">1. Introduction</h1><hr><ul><li>将自动的图像到图像翻译定义为，在给定足够训练数据的情况下，将场景的一种可能表示转换为另一种可能表示的问题。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072308137.png" alt="image.png"></p><ul><li>predict pixels from pixels</li><li>CNN 学习最小化损失函数，即对结果质量进行评分的目标——尽管学习过程是自动的，但仍需要大量手动工作来设计有效的损失。迫使 CNN 做我们真正想要的损失函数——例如，输出清晰、逼真的图像。</li><li>GANs学习一个损失，试图分类输出图像是真是假，同时训练生成模型，以尽量减少这种损失。由于GANs学习的是与数据相适应的损失，因此它们可以应用于传统上需要不同类型的损失函数的大量任务。</li><li>cGANs学习条件生成模型。这使得cGAN适合于图像到图像的转换任务，其中对输入图像进行条件处理并生成相应的输出图像。</li></ul><h1 id="2-Related-work">2. Related work</h1><hr><h3 id="Structured-losses-for-image-modeling——图像建模中的结构损失：">Structured losses for image modeling——图像建模中的结构损失：</h3><ul><li>传统Image-to-image translation方法将输出空间视为“非结构化”的，即每个输出像素被视为有条件地独立于所有其他像素。</li><li>cGANs是结构化损失。结构化损失对输出的联合配置进行惩罚。</li><li>大量文献考虑了此类损失，方法包括条件随机场、SSIM度量、特征匹配、非参数损失、卷积伪先验和基于匹配协方差统计的损失。</li><li>cGANs的不同之处在于，损失是学习的，理论上，GAN可以惩罚输出和目标之间任何可能存在差异的结构。</li></ul><h3 id="Conditional-GANs——条件GANs：">Conditional GANs——条件GANs：</h3><ul><li>本文框架的不同之处在于没有特定于应用程序的内容。这使得设置比大多数其他设置简单得多。</li><li>本文的生成器使用了基于“U-Net”的架构 ，判别器则使用了卷积“PatchGAN”分类器，它只对图像块级别的结构进行惩罚。</li><li>类似的PatchGAN架构曾被提出，旨在捕捉局部风格统计信息。</li><li>本文展示了这种方法在更广泛的问题上是有效的，并探讨了改变块大小的影响。</li></ul><h1 id="3-Method">3. Method</h1><hr><ul><li>GANs 学习从随机噪声向量z到输出图像y的映射，表示为 $G: z \rightarrow y$ 。</li><li>cGANs 学习从观测到的图像x和随机噪声向量z到图像y的映射，表示为 $G: {x, z} \rightarrow y$ 。</li><li>生成器G被训练生成的输出不能被对抗性训练的判别器D区分为“假”图像，而判别器D则被训练尽可能准确地检测出生成器的“伪造”图像。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072345976.png" alt="image.png"></p><h2 id="3-1-Objective——目标">3.1 Objective——目标</h2><hr><p>cGANs的目标函数：<br>$$ L_{cGAN}(G, D) = \mathbb{E}<em>{x,y}[\log D(x, y)] + \mathbb{E}</em>{x,z}[\log(1 - D(x, G(x, z)))] $$</p><ul><li>判别器 <strong>D(x,y)</strong>：输出的值表示它认为这个图像对是否真实的概率。</li><li>生成器 <strong>G(x,y)</strong>：试图让 D(x,G(x,z)) 尽可能接近1。</li><li>判别器 D 的目标是最大化该损失函数，它希望尽量分清楚真实图像 y 和生成的伪造图像 G(x,z)。</li><li>生成器 G 的目标是最小化该损失函数，它希望生成的图像尽可能逼真，欺骗判别器。</li><li><strong>z</strong> 表示<strong>随机噪声向量</strong>，它是从某个预定义的概率分布（如高斯分布或均匀分布）中采样得到的。</li><li><strong>D(x, y) 值大</strong>：判别器认为输入的图像是真实的。</li><li><strong>D(x, G(x, z)) 值大</strong>：生成器生成的图像质量高，足以欺骗判别器。</li><li><strong>D(x, G(x, z)) 值小</strong>：生成器的图像质量差，判别器成功识别出它是伪造的。</li><li><strong>$L_{cGAN}(G, D)$越大</strong>： 判别器 D 在识别生成器 G 的假图像方面做得更好；生成器 G 还需要进一步改进，以生成更逼真的图像。</li><li><strong>$L_{cGAN}(G, D)$越小</strong>：生成器 G 的图像质量足够好，以至于判别器 D 很难区分真实图像和生成图像。</li><li>$\mathbb{E}$ 代表 <strong>期望值（Expectation）</strong>，即随机变量取不同值的加权平均。</li></ul><p>其中，生成器 G 尝试最小化这个目标函数，而判别器 D 尝试最大化它，即：<br>$$G^* = \arg\min_G \max_D L_{cGAN}(G, D)$$</p><ul><li>$\arg$ 表示取得某个函数最优值（最小值或最大值）时的输入参数。</li></ul><p>无条件GANs：<br>$$L_{GAN}(G, D) = \mathbb{E}<em>{y}[\log D(y)] + \mathbb{E}</em>{x,z}[\log(1 - D(G(x, z)))]$$</p><p>将GAN的目标与更传统的损失（如L2距离）结合起来是有益的。生成器不仅需要欺骗判别器，还需要在L2意义上接近真实输出：<br>$$L_{L1}(G) = \mathbb{E}<em>{x,y,z}[|y - G(x, z)|<em>1]$$<br>最终目标：<br>$$G^* = \arg \min_G \max_D \left( L</em>{cGAN}(G, D) + \lambda L</em>{L1}(G) \right)$$</p><p>没有随机变量 $z$ 会生成确定性输出，但生成器很容易学会忽略噪声。<br>本文模型中，噪声仅通过 dropout 的形式引入，应用于生成器的多个层级，且在训练和测试时都使用。<br>尽管存在 dropout 噪声，但网络输出的随机性较小。</p><h2 id="3-2-Network-architectures——网络体系结构">3.2 Network architectures——网络体系结构</h2><hr><p>生成器和判别器都使用卷积BatchNorm ReLu形式的模块。</p><h3 id="3-2-1-Generator-with-skips——带有跳跃连接的生成器">3.2.1 Generator with skips——带有跳跃连接的生成器</h3><hr><ul><li>借鉴 U-Net</li></ul><p>许多图像转换问题，输入和输出之间共享大量低级信息，因此直接传递这些信息会更为理想。<br>例如，在图像上色的情况下，输入和输出共享显著边缘的位置。</p><p>本文添加了跳跃连接，在每一层 i 和第 n - i 层之间添加了跳跃连接，其中 n 是总层数。<br>每个跳跃连接简单地将层 i 的所有通道与层 n - i 的通道进行级联。</p><h3 id="3-2-2-Markovian-discriminator-PatchGAN-——马尔可夫鉴别器（PatchGAN）">3.2.2 Markovian discriminator (PatchGAN)——马尔可夫鉴别器（PatchGAN）</h3><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409082343453.png" alt="image.png"></p><p>L2 损失以及 L1 损失（见图 3），能准确捕捉低频信息，无法捕捉高频信息。<br>所以，对于低频的正确性，L1 损失已经足够了。<br>这促使本文将 GAN 判别器限制为仅建模高频结构，并依赖 L1 项来强制低频的正确性（公式 5）。</p><p>对于高频信息，限制关注局部图像块中的结构是足够的。<br>本文设计了一种判别器架构——PatchGAN，它仅对图像块级别的结构进行惩罚。<br>这个判别器试图判别图像中的每个 N×N 块是否真实。在图像上以卷积方式运行这个判别器，对所有响应进行平均以提供最终的 D 输出。<br>N 可以比图像的全尺寸小得多，仍能产生高质量的结果。较小的 PatchGAN 参数更少，运行速度更快。</p><p>判别器有效地将图像建模为马尔可夫随机场，将图像的复杂性简化为局部结构的依赖。</p><blockquote><p><strong>马尔可夫性</strong>：图像中的像素只与其邻近像素直接相关，而与较远的像素是条件独立的。即一个像素的状态（如颜色或强度）只依赖于其局部邻域（Patch）中的像素，而不直接依赖于远处的像素。</p></blockquote><blockquote><p>局部依赖性和条件独立性</p></blockquote><h2 id="3-3-Optimization-and-inference——优化与推理">3.3 Optimization and inference——优化与推理</h2><hr><h3 id="优化：">优化：</h3><p>在判别器 D 上进行一次梯度下降步骤，然后在生成器 G 上进行一次该步骤。<br>用小批量的随机梯度下降（SGD）并应用 Adam 优化器 。</p><h3 id="推理：">推理：</h3><p>以与训练阶段完全相同的方式运行生成器网络。<br>在测试时应用了 dropout，并使用测试批次的统计数据进行批量归一化，而不是使用训练批次的汇总统计数据。<br>当批量大小设置为1时，这种批量归一化方法被称为“实例归一化”。<br>本文实验使用了1到10之间的批量大小。</p><h1 id="4-Experiments">4. Experiments</h1><hr><p>在所有情况下，输入和输出仅为1-3通道图像。</p><h2 id="4-1-Evaluation-metrics——评估指标">4.1 Evaluation metrics——评估指标</h2><hr><p>首先，本文在 Amazon Mechanical Turk (AMT) 上进行“真实 vs 假”感知研究。该方法更符合人类观察者的合理性。<br>其次，衡量合成的城市景观是否足够逼真，使得现成的识别系统能够识别其中的对象。</p><h3 id="AMT-perceptual-studies——AMT知觉研究">AMT perceptual studies——AMT知觉研究</h3><hr><p>Turkers 会被呈现一系列对比试验，展示由我们的算法生成的“假”图像与“真实”图像。<br>在每次试验中，每张图像显示1秒，之后图像消失，Turkers 会有无限时间来选择哪张是假的。每个会话的前10张图像是练习，Turkers 会收到反馈。在正式实验的40次试验中不再提供反馈。每次会话只测试一种算法，且 Turkers 不允许完成多个会话。<br>所有图像都以 256 × 256 分辨率展示。<br>没有设置警觉性试验。</p><h3 id="FCN-score——FCN评分">FCN-score——FCN评分</h3><hr><p>采用 FCN-8s 语义分割架构，并在 cityscapes 数据集上对其进行训练。<br>通过生成照片的分类准确率，来对这些照片根据其来源标签进行评分。</p><h2 id="4-2-Analysis-of-the-objective-function——目标函数分析">4.2 Analysis of the objective function——目标函数分析</h2><hr><ol><li>比较 L1 项和 GAN 项的影响；</li><li>比较基于输入条件的判别器与非条件判别器的影响。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409082343453.png" alt="image.png"></p><p>单独使用 L1 会得到合理但模糊的结果。<br>单独使用 cGAN（设置λ=0）会得到更清晰的结果，但在某些应用中会引入视觉伪影。<br>将这两个项相加（λ=100）可减少这些伪影。</p><p>在 cityscapes 标签→照片任务上使用 FCN 评分量化观察结果：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091106240.png" alt="image.png"></p><p>基于 GAN 的目标取得了更高的分数，表明合成的图像包含了更多可识别的结构。<br>测试去除判别器条件的效果（标记为 GAN）。在这种情况下，损失loss不会惩罚输入和输出之间的不匹配；它只关心输出是否看起来真实。这种变体表现非常差，结果显示，生成器在输入照片不同的情况下几乎生成相同的输出。<br>显然，损失衡量输入和输出之间的匹配质量非常重要，确实，cGAN的表现比GAN好得多。<br>添加 L1 项也鼓励输出符合输入，因为 L1 损失会惩罚 ground truth 输出（正确匹配输入）与 synthesis 输出（可能不匹配）的距离。因此，L1+GAN 在创建符合输入标签图的真实渲染方面也很有效。<br>结合所有项，L1+cGAN 的表现也同样优秀。</p><h3 id="Colorfulness——色彩度：">Colorfulness——色彩度：</h3><p>cGANs 的一个显著效果是，它们能生成清晰的图像，即使在输入标签图中不存在空间结构时也会进行“幻觉”生成。<br>人们可能会认为 cGANs 在光谱维度上也有类似的“锐化”效果——使图像更具色彩感。<br>对抗损失原则上可以意识到灰色输出不现实，并鼓励匹配真实的颜色分布。</p><p>L1 损失在不确定边缘具体位置时会鼓励模糊，当不确定像素应采用哪个可能的颜色值时，它也会鼓励选择一种平均的、灰色的颜色。L1 损失会通过选择可能颜色的条件概率密度函数的中位数来最小化。</p><p>图表显示了 Lab 色彩空间中输出颜色值的边际分布：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091641593.png" alt="image"><br>Ground truth 分布用虚线表示。<br>L1 损失导致的分布比真实分布更窄，确认了 L1 鼓励平均灰色颜色的假设。<br>cGANs 能将输出分布推向更接近真实分布。</p><h2 id="4-3-Analysis-of-the-generator-architecture——生成器结构分析">4.3 Analysis of the generator architecture——生成器结构分析</h2><hr><p>图 4 比较了 U-Net 和编码器-解码器在城市景观生成中的表现：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091121513.png" alt="image.png"></p><h2 id="4-4-From-PixelGANs-to-PatchGans-to-ImageGANs——从像素到PatchGans再到ImageGANs">4.4 From PixelGANs to PatchGans to ImageGANs——从像素到PatchGans再到ImageGANs</h2><hr><p>判别器接收场 patch 大小 N 的变化效果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091550758.png" alt="image.png"></p><p>FCN 评分量化了这些效果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091108553.png" alt="image.png"></p><p>本文中所有实验均使用 70 × 70 的 PatchGAN，而在本节中所有实验都使用了 L1+cGAN 损失。<br>PixelGAN 对空间清晰度没有影响，但确实增加了结果的色彩丰富性。<br>颜色直方图匹配是图像处理中的一个常见问题，PixelGAN 可能是一个有前景的轻量级解决方案。</p><h3 id="Fully-convolutional-translation——全卷积平移：">Fully-convolutional translation——全卷积平移：</h3><p>卷积式地应用生成器，然后在比训练时更大的图像上进行测试。<br>在 256×256 图像上训练生成器后，在 512 × 512 图像上进行了测试：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091601190.png" alt="image.png"></p><h2 id="4-5-Perceptual-validation——感知验证">4.5 Perceptual validation——感知验证</h2><hr><p>地图↔照片任务上的 AMT 实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091610772.png" alt="image.png"></p><p>地图↔照片任务本模型显著高于 L1 基线。<br>照片→地图任务偏低的原因可能是在地图中，轻微的结构错误由于其严格的几何形状更加明显，而在更加混乱的航拍照片中则不太显眼。</p><p>彩色化任务实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091614552.png" alt="image.png"></p><p>cGANs 的得分与 L2 变体相似，但未能达到完整方法的水平，后者是专门为彩色化任务设计的。</p><h2 id="4-6-Semantic-segmentation——语义分割">4.6 Semantic segmentation——语义分割</h2><hr><p>在 cityscape 数据集上训练了一个 cGANs（有/无 L1 损失），用于从照片生成标签：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091619370.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091620265.png" alt="image.png"></p><p>未使用 L1 损失训练的 cGANs 在一定程度上能够解决该问题。<br>这是第一次展示 GAN 成功生成“标签”而非“图像”，前者几乎是离散的，而后者具有连续值的变化。</p><h2 id="4-7-Community-driven-Research——社区推动研究">4.7 Community-driven Research——社区推动研究</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091627825.png" alt="image.png"></p><h1 id="5-Conclusion">5. Conclusion</h1><hr><p>cGANs 对于许多图像到图像转换任务来说是一个很有前景的方法，尤其是涉及高度结构化图形输出的任务。<br>这些网络学习了一种适应特定任务和数据的损失函数，这使得它们可以应用于多种场景。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GANs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】深度学习图像数据增广方法研究综述</title>
      <link href="/image-synthesis-review-02/"/>
      <url>/image-synthesis-review-02/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><hr><h3 id="背景：">背景：</h3><p>充足的训练数据不仅可以缓解模型在训练时的过拟合问题，而且可以进一步扩大参数搜索空间，帮助模型进一步朝着全局最优解优化。<br>然而，在许多领域或任务中，获取到充足训练样本的难度和代价非常高。因此，数据增广成为一种常用的增加训练样本的手段。</p><h3 id="本文贡献：">本文贡献：</h3><p>按照方法本质原理的不同，将其分为单数据变形、多数据混合、学习数据分布和学习增广策略等 4 类方法。<br>单数据变形方法主要可以分为几何变换、色域变换、清晰度变换、噪声注入和局部擦除等 5 种；<br>多数据混合可按照图像维度的混合和特征空间下的混合进行划分；<br>学习数据分布的方法主要基于生成对抗网络和图像风格迁移的应用进行划分；<br>学习增广策略的典型方法可以按照基于元学习和基于强化学习进行分类</p><h3 id="前景：">前景：</h3><p>根据数据和任务<br>基于强化学习探索最优的组合策略，<br>基于元学习自适应地学习最优数据变形和混合方式，<br>基于生成对抗网络进一步拟合真实数据分布以采样高质量的未知数据，<br>基于风格迁移探索多模态数据互相转换的应用</p><h1 id="0-引言">0. 引言</h1><hr><p>在许多研究领域，受限于数据获取难度大、标注成本高等原因，往往难以获得充足的训练数据，这样训练得到的深度学习模型往往存在过拟合的问题，进而导致模型泛化能力差、测试精度不高等，难以满足应用需求。</p><p><strong>数据增广</strong>，又称<strong>数据增强</strong>(data augmentation)，是一种增加有限数据的数量和多样性的策略，旨在从有限的数据中提炼出更多有用的信息，产生等价于更多数据的价值。数据增广方法试图从过拟合问题的根源———训练样本不足，去解决该问题。</p><p>数据增广可以分为<strong>数据变形</strong> (data warping) 和 <strong>数据过采样</strong> (oversampling)两种方法。</p><p><strong>数据变形类</strong>：LeNet-5、AlexNet、VGGNet、GoogleNet、ResNet、DenseNet中都有用到。</p><p><strong>多幅图像信息混合</strong>：SamplePairing、mixup、SMOTE等，这类方法本质上属于<strong>数据过采样</strong>。</p><p><strong>GAN</strong>：Frid-Adar 等</p><p><strong>元学习和强化学习的思想</strong>：训练一个模型去自适应地选用最优的数据增广策略，来实现模型性能提升的最大化。AutoAugment 和 RandAugment</p><p>本文从另外的角度，即从数据增广的生成方式综述，将数据扩增方法分为单数据变形、多数据混合、学习数据分布规律生成新数据和学习增广策略等 4 类方法。</p><h1 id="1-单数据变形">1. 单数据变形</h1><hr><h2 id="1-1-几何变换">1.1 几何变换</h2><hr><ul><li>几何变换 (geometric transformations)是最常见的图像数据增广方法，通过旋转、镜像、平移、裁剪、缩放和扭曲等变换方式生成新样本。</li><li>在实际任务中，需要根据数据的特点选择合适的几何变换方法才能进一步带来模型性能的提升，否则可能适得其反。</li><li>虽然几何变换的方式简单易操作，但也存在对数据重复记忆、增加的信息量有限等缺点，这也导致几何变换在实际应用中为模型带来的精度提升十分有限。</li></ul><h2 id="1-2-色域变换">1.2 色域变换</h2><hr><ul><li>色域变换(color space transformations)是一种在图像各通道上进行亮度变换的新样本生成方式。</li><li>基于色域变换的数据增广本质上是通过对数据集增加各种各样的光照亮度偏差，增强模型在不同光照条件下的鲁棒性。</li><li>对于图像分类任务，空间几何信息相比色彩信息更加重要。色域变换与几何变换存在着同样的缺点，同时还可能丢失一些重要的颜色信息，进而改变图像原有的语义信息，这也使得该方式的应用存在较大的局限性。</li></ul><p>示例：</p><ul><li>颜色抖动（color jittering），通过几种颜色组合模拟出大范围内多色彩模式的图像增广方式</li><li>PCA抖动（fancy PCA），对原图像进行主成分分析(PCA)，求得协方差矩阵，然后对主成分的特征值施加一个均值为 0 的随机扰动，然后再反变换回去。</li><li>高斯抖动，本质上通过给协方差矩阵增加噪音实现一种图像在视觉表现上的滤镜效果。</li><li>实际应用中，甚至可以使用图像编辑软件进行颜色变换。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409061017167.png" alt="image.png"></p><h2 id="1-3-清晰度变换">1.3 清晰度变换</h2><hr><ul><li>清晰度变换是一种改变图像视觉清晰度的新样本生成方式。</li><li>“核滤波器(kernel filters)”，核滤波器通过滑动的 n × m 矩阵对图像进行卷积操作，对图像进行锐化和模糊处理，实现图像的清晰度变换。</li><li>采用这种滤波方式对数据集进行增强，不如将其作为网络的一层，还可以训练获得最优的滤波操作。</li></ul><p>示例：</p><ul><li>高斯模糊滤波器(Gaussian blur filter)</li><li>边缘滤波器(edge filter)</li><li>PatchShuffle 正则化</li></ul><h2 id="1-4-噪声注入">1.4 噪声注入</h2><hr><ul><li>噪声注入(noise injection)是一种在图像上叠加噪声的新样本生成方式，噪声可表示为一个服从某分布的随机矩阵。</li><li>通过人为地为图像施加噪声干扰，可为数据集引入冗余和干扰信息，模拟不同成像质量的图像，增强模型对噪声干扰和冗余信息的过滤能力，提高模型对不同质量图像的识别能力。</li><li>常见的噪声种类有高斯噪声、瑞利噪声、伽马噪声、均匀噪声和椒盐噪声等</li><li>在图像上增加噪声可以帮助 CNNs 学到更加鲁棒的特征</li><li>但对于更加复杂的数据集以及多分类问题，模型训练本质是在欠拟合的情况下，噪声注入的图像扩增方式并不能带来新的有效信息，因此不能为模型带来提升效果</li><li>对抗训练：为防御对抗攻击，采用对抗样本进行训练，可以视做一种数据增广方法，用以弥补模型自身的弱点</li></ul><p>示例：</p><ul><li>前向噪声调整方案(forward noise adjustment scheme)</li><li>30 类遥感图像场景数据集</li><li>噪声叠加在图像上，产生“对抗样本(adversarial examples)”</li><li>DisturbLabel</li></ul><h2 id="1-5-局部擦除">1.5 局部擦除</h2><hr><ul><li>不同于噪声是对图像离散的像素值信息的干扰，局部擦除则是图像局部区域所有像素值信息的丢失。</li><li>迫使模型去学习图像中更宽广的具有描述性质的特征，从而防止模型过拟合于特定的视觉特征。</li><li>根据数据和任务的不同，这种方法有时需要人为干预以保证其有效性。</li></ul><p>示例：</p><ul><li>随即擦除（random erasing），可以视为一种在数据空间的dropout</li><li>Cutout 正则化 、Hide-and-Seek 、GridMask</li><li>不规则区域的局部擦除</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062140038.png" alt="image.png"></p><h1 id="2-多数据混合">2. 多数据混合</h1><hr><p>多数据混合的方式希望将多幅图像的信息进行混合以产生新的训练数据，可以从图像空间或特征空间进行信息混合。</p><h2 id="2-1-图像空间的数据混合">2.1 图像空间的数据混合</h2><hr><ul><li>在图像空间进行数据混合的数据增广方法，可以分为对多幅图像的线性叠加和非线性混合，是一类与人类直觉不一致的数据增广方式</li><li>虽然这类混合图像的方法看似不合常理，缺乏可解释性，但是对于提升模型的分类精度却十分有效，可以取得非常具有竞争力的结果。</li></ul><p>示例：</p><ul><li>基于线性混合图像：SamplePairing、mixup和Between-Class Learning</li><li>SamplePairing 对两幅图像求平均值的方式可以看做是在两个数据的中点进行插值，mixup 可以看做是拓展到线性插值得到新样本的版本</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062239130.png" alt="image.png"></p><ul><li>mixup 数据增广方法不仅可以提高深度神经网络模型的泛化能力，而且可以有效减少模型对错误标签的记忆，增加模型对于对抗样本的鲁棒性，甚至可以稳定生成对抗网络的训练。</li><li>“CNN 中的输入数据可以被视为波形”，“波形混合”的角度解释了图像线性叠加数据增广方法的原理，类间学习方法(between-class learning，BC)应用到图像上，随机的比例混合两幅图像，这类线性叠加图像的方法相当于一个正则项，希望模型尽可能向线性函数去拟合，以防止强非线性导致的过拟合问题<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062314809.png" alt="image.png"></li><li>非线性图像混合<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062316532.png" alt="image.png"></li><li>多图随机裁剪拼接混合<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062317706.png" alt="image.png"></li></ul><h2 id="2-2-特征空间数据混合">2.2 特征空间数据混合</h2><hr><ul><li>借助 CNN 提取的图像特征，在特征空间进行数据增广。</li><li>针对图像数据，在特征空间进行数据混合的方法很少被采用。</li></ul><p>示例：</p><ul><li>SMOTE 方法，一种在特征空间上进行插值生成新样本的方法</li><li>在特征空间外插值</li><li>在数据空间进行图像变换的效果要优于特征空间变换</li></ul><h1 id="3-学习数据分布">3. 学习数据分布</h1><hr><p>机器学习中的生成式方法，可以通过训练，学习数据集的潜在概率分布，在数据分布中进行过采样生成新数据，由于将整个数据集作为先验知识，这种数据增广方法在理论上是一种更加优秀的方法。</p><h2 id="3-1-生成对抗网络">3.1 生成对抗网络</h2><hr><ul><li>GAN 的核心思想源自博弈论的二人零和博弈(zero-sum game)</li><li>在 GAN 中，博弈的双方是生成器 G 和判别器 D，优化过程是一个极小极大博弈(min-max game)问题，使生成器和判别器在不断优化中各自提高自己的生成能力和判别能力。</li><li>生成器的目标是学习真实数据的潜在分布，并生成新的数据样本，使其看起来和真的一样，达到欺骗判别器的目的</li><li>判别器是一个二分类器，其目标是找到生成出的样本和真实数据分布之间的差异，判别输入的是真实数据还是生成的样本，并且计算并输出一个样本是否来自于真实数据分布的概率值或者标量</li><li>GAN 生成的样本用于数据增广的有效性，并且相比图像变换这类经典的数据增广方法，可以取得更好的效果</li><li>需要较为大量的数据来训练 GAN 模型，样本并不是真实世界存在的，不能将生成的样本当做真实的样本来对待</li></ul><p>示例：</p><ul><li>PG-GANs</li><li>BigGANs</li><li>DCGAN</li><li>conditional GAN</li><li>SiftingGAN<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062337018.png" alt="image.png"></li><li>真假样本混合加权训练的数据增广方法</li></ul><h2 id="3-2-图像风格迁移">3.2 图像风格迁移</h2><hr><ul><li>风格迁移，或称为“图到图翻译(image-to-image translation)”，可以视为一种广义上的图像变换，是一类针对图像的领域迁移(domain transfer)问题。</li><li>本质上是建立一种不同数据分布之间的相互映射。</li></ul><p>示例：</p><ul><li>基于 conditional GAN 提出 pix2pix 方法</li><li>循环一致性生成对抗网络(cycle-consistent adversarial networks，CycleGAN)）<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062342910.png" alt="image.png"></li><li>人体器官MR(magnetic resonance)影像和CT影像之间的转换</li><li>同一遥感场景下 SAR (synthetic aperture rader)和红外影像与可见光影像之间的转换</li><li>神经风格迁移（neural style transfer），类似于颜色空间的光照变换</li></ul><h1 id="4-学习增广策略">4. 学习增广策略</h1><hr><p>借助元学习(meta-learning)和强化学习(reinforcement learning)训练一个模型去自适应地选用最优的数据增广策略，来实现模型性能提升的最大化。</p><h2 id="4-1-基于元学习的策略">4.1 基于元学习的策略</h2><hr><ul><li>元学习的基本思想是希望模型像人一样学会“如何学习”，即基于过去学习的知识和经验总结学习方法，进而可以快速学习新知识、适应新任务和新环境。</li><li>元学习最直接的一种理解为“用神经网络去优化神经网络”，而在数据增广方面，可以用神经网络去替代确定的数据增广方法，训练模型去学习更好的增广策略。</li></ul><p>示例：</p><ul><li>输入随机选取的两幅同一类的图像，希望通过神经网络学习两幅图像共同的内容信息或者风格信息，进而得到一幅“增强图像”，再与原始图像一同输入到分类网络中进行分类模型的训练</li></ul><h2 id="4-2-基于强化学习的策略">4.2 基于强化学习的策略</h2><hr><ul><li>从给定的图像变换和混合方法中，搜索最优的组合策略</li><li>如何对给定任务定制一组图像变换策略，以进一步提高给定模型的预测性能，仍然是一个悬而未决的问题</li></ul><p>示例：</p><ul><li>离散搜索问题</li><li>AutoAugment，选用了强化学习作为搜索算法，搜索最优策略</li><li>RandAugment，主要思想是随机选择变换并调解变换的强度</li></ul><h1 id="5-方法分析与研究展望">5. 方法分析与研究展望</h1><hr><h2 id="5-1-不同数据增广方法选用分析">5.1 不同数据增广方法选用分析</h2><hr><ul><li>采用不合适的变换方法则可能带来负面的效果。因此，方法的适用性成为使用数据增广时首先需要考虑的问题。</li><li>虽然在选用数据增广方法时需要考虑不同种类、不同领域图像各自的特点，但是都需要具备一个核心原则: 在不改变图像原有语义信息的同时尽可能多地增加变化。</li><li>由于自然图像和遥感图像在内容理解上都经常受到遮挡因素的影响，如自然场景前景对背景的遮挡、遥感场景中云对地物的遮挡，裁剪和局部擦除的方法可以提高模型对遮 挡的鲁棒性，而对于医疗影像其成像方式的不同，不存在遮挡的问题，使用这类数据增广方法的有效性还有待验证。</li><li>虽然在一些研究工作中已经证明使用 GAN 进行数据增广可以更有效地提高模型的精度，但是训练 GAN 模型需要一定数量的样本，对于数据量非常小的任务，不适合采用这类基于学习的方法。<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062359953.png" alt="image.png"></li></ul><h2 id="5-2-未来研究展望">5.2 未来研究展望</h2><hr><ul><li>由于图像的维度很高，同时训练 GAN 的样本也非常有限，许多情况下 GAN 对图像数据的概率分布的拟合效果并不好，导致采样生成的图像质量难以保证，限制了 GAN 作为理论上最佳数据增广方法的发展。</li><li>对于 GAN 风格迁移方面的研究和应用，本质上是建立一种不同数据分布之间的相互映射，对于现实生活中普遍存在的跨场景、跨模态的多领域分布的数据，可以通过构建这种映射来实现数据的互补。</li></ul><h1 id="6-结语">6. 结语</h1><hr><ul><li>数据增广作为从数据层面提高机器学习模型性能的一项重要手段，广泛应用于各个领域，尤其是那些样本获取成本高、标注难度大的领域。</li><li>按照增广数据的生成方式划分为四类：单数据变形、多数据混合、学习数据分布和学习增广策略</li><li>基于学习的方法对于数据增广同样具有广阔的发展前景，主要在于以下几个方面: 根据数据和任务基于强化学习探索最优的组合策略; 基于元学习自适应地学习最优数据变形和混合方式；基于生成对抗网络进一步拟合真实数据分布以采样高质量的未知数据；基于风格迁移探索多模态数据互相转换的应用</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GANs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】医学图像数据集扩充方法研究进展</title>
      <link href="/image-synthesis-review-01/"/>
      <url>/image-synthesis-review-01/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><hr><h3 id="背景：">背景：</h3><ul><li>计算机辅助诊断（CAD）</li><li>训练样本受成像成本、标记成本和涉及患者隐私等因素的影响，导致训练图像多样性不足且难以获取。</li></ul><h3 id="本文贡献：">本文贡献：</h3><p>对医学图像数据集扩充方法的研究进展进行综述。</p><ol><li>对比分析基于几何变换和基于生成对抗网络的扩充方法；</li><li>介绍基于生成对抗网络扩充方法的改进及其适用场景；</li><li>讨论医学图像数据集扩充领域的一些亟待解决的问题并对其未来发展趋势进行展望。</li></ol><h1 id="0-引言">0. 引言</h1><hr><p>医学图像成像模态：</p><ul><li>磁共振成像（magnetic resonance imaging，MRI）</li><li>计算机断层扫描成像（computed tomography，CT）</li><li>正电子发射断层扫描成像（positron emission computed tomography，PET）</li></ul><p>诊断难点：医学图像信息量庞大以及部分疾病的病变部位细小</p><p>医学图像数据集扩充方法：基于几何变换和基于生成对抗网络（generative adversarial network，GAN）的扩充方法</p><h1 id="1-基于几何变换的医学图像数据集扩充方法">1. 基于几何变换的医学图像数据集扩充方法</h1><hr><p>两种操作方式：</p><ol><li>针对图像中像素点的灰度值进行操作，通过一系列变换函数的映射，改变像素点位置信息，使其纹理细节与原图保持一致；</li><li>通过将图像内容变形重组，使其病变区域、感兴趣区域产生形变进而使该图像拥有更多样化的特征信息。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409051723906.png" alt="image.png"></p><p>该扩充方法对数据集多样性的提升较少。</p><h1 id="2-基于GAN的医学图像数据集扩充方法">2. 基于GAN的医学图像数据集扩充方法</h1><hr><p><strong>GAN</strong> 是一种生成式模型，其目的是构建一个从真实图像到潜在特征分布的显式映射关系，构建过程中不需要额外构造复杂的概率密度函数即可实现该映射关系。<br>GAN的训练过程可以概括为一个零和博弈的过程，其生成器希望生成的图像尽可能真实从而欺骗鉴别器，鉴别器则尽力分辨出真实图像和生成图像。</p><p>基于GAN的医学图像数据集扩充方法可以分为四类，分别为：</p><ol><li>无条件数据集扩充方法；</li><li>条件数据集扩充方法；</li><li>跨模态数据集扩充方法；</li><li>与几何变换方法结合的数据集扩充方法</li></ol><h2 id="2-1-无条件数据集扩充方法">2.1 无条件数据集扩充方法</h2><hr><p><strong>无条件数据集扩充方法</strong>是指在没有任何额外信息的情况下，仅利用高斯噪声或者均匀噪声作为GAN的输入而生成医学图像的一类方法。<br>该方法早期生成的图像存在分辨率低、图像模糊、图像特征单一的问题，但可优化解决，其性能逐步上升。</p><p>案例：</p><ul><li>肝脏病变CT图像</li><li>3D头部MRI图像</li><li>CT肺结节图像</li><li>大脑切片MRI图像</li><li>脑部MRI图像</li></ul><p>问题：<br>图像常出现一些与生理学相违背的现象，针对无条件数据集扩充方法的改进应考虑优化生成图像的结构。</p><h2 id="2-2-条件数据集扩充方法">2.2 条件数据集扩充方法</h2><hr><p><strong>条件数据集扩充方法</strong>是指在生成新的医学图像时，一些先验信息如标签、文字、图片等跟随噪声信息一同输入到GAN的生成器。<br><strong>先验信息</strong>能够起到指导模型生成的作用，在先验信息的约束之下生成的医学图像更符合人体的生理学构造。</p><p>案例：</p><ul><li>新冠感染者的CT肺部图像</li><li>肺结节数据</li><li>高分辨率的彩色视网膜图像</li><li>皮肤图像生成与皮肤病变生成</li></ul><p>优点：<br>在继承了无条件数据集扩充方法优点的同时，能够生成特定类型的图像。</p><p>问题：<br>该方法缩小了GAN的生成器样本空间从而对生成图像的多样性起到了限制作用，若想要获得更多样化的医学图像需要训练多个网络，这增加了额外的资源消耗。</p><h2 id="2-3-跨模态数据集扩充方法">2.3 跨模态数据集扩充方法</h2><hr><p>在跨模态数据集扩充方法中，使用最多的是<strong>有监督的像素到像素GAN</strong>（pixel-to-pixel GAN， Pix2PixGAN）和<strong>无监督的循环GAN</strong>（cycle GAN， CycleGAN）。<br><strong>Pix2PixGAN</strong> 在训练过程中需要成对按像素值对齐的图像，图像的获取成本高昂；<br><strong>CycleGAN</strong> 能够适用于非对齐的医学图像，但其生成效果不如Pix2PixGAN。</p><p>案例：</p><ul><li>未对齐的目标图像被视为噪声并使用附加的配准网络进行训练以自适应地拟合未对齐的噪声分布</li><li>基于迭代的多尺度特征融合GAN，有效降低图像的对齐损失</li><li>联邦域翻译新基准方法，缓解图像域移</li><li>有效地去除图像中的噪声分布，有效地去除图像中的噪声分布</li><li>互信息约束GAN，减少模态迁移过程中医学图像细节的丢失</li><li>边缘感知GAN，整合了边缘信息并优化生成图像的内容及结构纹理信息</li><li>两阶段的模态迁移方法</li><li>以CycleGAN为基准生成二维超声心动图的方法</li></ul><p>优点：<br>该方法在成像清晰度、图像多样性、特征多样性、网络收敛速度等方面都具有明显优势</p><p>问题：<br>该方法需要消耗更多的计算机算力资源以及对训练数据集有更高的要求</p><h2 id="2-4-与几何变换结合的数据集扩充方法">2.4 与几何变换结合的数据集扩充方法</h2><hr><p><strong>GAN 的生成器</strong>对输入图像生成变形场、强度变换、仿射变换等<strong>模拟几何变换方法</strong>的操作，可避免生成图像缺乏特征多样性。</p><p>案例：</p><ul><li>两步的基于GAN方法，生成脑部MRI图像的纹理</li><li>对抗性数据集扩充方法</li><li>任务驱动型数据集扩充方法，三个医学数据集（心脏、前列腺和胰腺）</li><li>新的任务驱动数据集扩充方法，改变使用GAN模型随机增强训练示例却提升有限的现状，心脏MRI图像</li><li>新型联合强化学习方法，使用弱监督的GAN模型充当代理，并在给定样本作实例的情况下输出图像掩码，肌肉骨骼X光片</li><li>正则化对抗学习方法，人工指定生成范围和双层优化预定义操作，2D皮肤癌分类和3D腹部器官分割扩充</li></ul><p>优点：<br>既能产生如基于几何变换扩充方法的真实性又能兼顾GAN生成图像的多样化</p><p>问题：<br>基于GAN的扩充方法不稳定、难训练、缺乏可解释性<br>基于几何变换扩充方法生成的图像数据分布过于一致</p><h1 id="3-总结与展望">3. 总结与展望</h1><hr><p>问题：</p><ol><li>没有提出广泛接受的生成图像质量评价标准，多用：峰值信噪比（peak signal to noise ratio， PSNR）、IS、FID等来衡量图像质量</li><li>2D医学图像无法完整表达人体器官的结构特性</li><li>仍致力于研究单模态医学图像生成，未充分利用医学图像的多模态信息</li><li>GAN生成的图像用于医学图像分析领域可能会带来不可预知的问题</li><li>对其他领域的优秀模型的吸收和借鉴非常有限</li><li>带标注医学图像数据难以获取</li></ol><p>展望：</p><ol><li>建立广泛接受的定性评价标准</li><li>医学图像分析对高维度的图像需求也将进一步增加</li><li>从多模态图像出发生成综合性单模态图像的研究</li><li>GAN扩充的医学图像缺乏可解释性</li><li>医学图像数据集扩充领域可以吸收其它领域的优秀成果</li><li>解决标注数据集获取困难的问题</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GANs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】Automated diagnosis of bone metastasis based on multi-view bone scans using attention-augmented deep neural networks</title>
      <link href="/spect-adob/"/>
      <url>/spect-adob/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="背景：">背景：</h3><p>手动分析骨显像图像需要丰富的经验，已有骨显像图像的自动或半自动诊断方法步骤复杂且在小数据集上验证不足，准确性和可靠性较低。</p><h3 id="本文贡献：">本文贡献：</h3><p>描述了一个深度卷积神经网络的方法，该方法有两个主要创新点：首先，通过联合分析前后视图进行诊断，从而获得较高的准确性。其次，提出了一种空间注意特征聚集算子来增强空间位置信息。</p><h3 id="结果：">结果：</h3><p>高分类准确率证明了所提出的体系结构对骨显像图像诊断的有效性，可作为临床决策支持工具应用。</p><h1 id="1-Introduction">1. Introduction</h1><hr><h3 id="背景：-2">背景：</h3><p>全身骨扫描（WBS）在骨转移的鉴别诊断中与MRI具有相似的性能，但其成本远低于MRI</p><ul><li>磁共振成像（magnetic resonance imaging，MRI）</li><li>计算机断层扫描（computed tomography，CT）</li><li>全身骨扫描（whole-body bone scan，WBS）</li></ul><p>WBS图像中的异常称为热点（hot spot），通常表现为比周围环境更高的信号强度。但没有骨转移的患者也可以在WBS图像上显示热点。</p><p>骨转移的自动诊断方法的发展面临以下几个障碍：</p><ol><li>各种非肿瘤性疾病在影像学表现上也表现出异常，导致高灵敏度和低特异性。</li><li>为了在不同场景下获得较强的泛化能力，需要一个大的带注释数据集来学习骨转移的特征。然而，以往骨扫描相关研究中使用的数据集均不能满足这一要求。</li><li>每个WBS检查包含两个显示前后视图的图像。要分析是否存在骨转移，模型必须将两种观点作为一个单一的检查进行联合分析。</li></ol><h3 id="解决方法：">解决方法：</h3><ol><li>使用深度卷积神经网络（CNNs）从数据中自动提取高级特征。</li><li>构建了一个由专业核医学医师注释的大规模WBS图像数据集。</li><li>提出了一种接收多个输入的新结构，用于联合分析来自前后视图的图像。</li></ol><h3 id="贡献：">贡献：</h3><ol><li>构建了一个包含15474个核医学专业医师标记的大规模WBS图像数据集。</li><li>提出了一种基于多视点图像的骨转移瘤自动诊断模型。</li><li>提出了一种由深度神经网络参数化的特征聚合算子，用于约束检查前后视图的特征。</li><li>分类和可视化结果表明，所提出的方法成功地掌握了骨转移瘤的WBS图像特征。</li></ol><h1 id="2-Related-work">2. Related work</h1><hr><ul><li>计算机辅助诊断系统（computer-aided diagnosis systems ，CAD）</li></ul><h2 id="2-1-Traditional-methods-for-bone-scan-analyzing——传统的骨扫描分析方法">2.1 Traditional methods for bone scan analyzing——传统的骨扫描分析方法</h2><hr><h3 id="主要内容：">主要内容：</h3><ol><li>WBS图像的分析主要集中在三个方面：骨扫描指数（BSI）的自动计算、骨转移瘤的自动诊断和热点分割。然而，BSI的计算只是半自动的，并且需要费力的手动过程。</li><li>这些方法的分类性能在很大程度上依赖于热点分割，这意味着分割错误可能导致后续分类失败。此外，传统方法依赖于手工特征和阈值，因此缺乏鲁棒性。此外，人工选择特征是一种累人的、主观的、难以提高性能的方法。</li></ol><h2 id="2-2-Deep-neural-networks-for-bone-scan-analysis——深度神经网络在骨扫描分析中的应用">2.2 Deep neural networks for bone scan analysis——深度神经网络在骨扫描分析中的应用</h2><hr><h3 id="主要内容：-2">主要内容：</h3><ol><li>与传统的图像处理方法相比，Deep-cnn具有许多优点：它们以数据驱动的方式自动提取不同层次的特征，不需要手工构建特征，减少了医生的工作量。</li><li>虽然已经有一些研究对骨转移的自动诊断进行了研究，但这些研究中的大多数都是对首先需要从WBS图像分割的热点进行诊断，这一过程可能会引入额外的错误。</li></ol><h2 id="2-3-Multi-view-fusion——多视图融合">2.3 Multi-view fusion——多视图融合</h2><hr><h3 id="主要内容：-3">主要内容：</h3><ol><li>具有自然图像的图像分类任务通常一次只包含一个图像，而医学成像中的检查通常带有一组视图。</li><li>本文开发了一个使用多视图输入的自动化骨转移诊断框架。</li></ol><h1 id="3-Dataset">3. Dataset</h1><hr><p>数据标注后，将标注的考试分为训练集、验证集和测试集。</p><h2 id="3-1-Materials——资料">3.1 Materials——资料</h2><hr><h3 id="主要内容：-4">主要内容：</h3><ul><li>本研究所用WBS图像均来自四川大学华西医院核医学科。共收集记录16341份。</li><li>所有检查均使用两种设备中的一种进行，一种是分辨率为256×1024像素，另一种是分辨率为512×1024像素。</li><li>WBS图像被标记为恶性（malignant）或良性（benign）。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021007475.png" alt="image.png|400"></li></ul><h2 id="3-2-Data-annotation——数据批注">3.2 Data annotation——数据批注</h2><hr><h3 id="主要内容：-5">主要内容：</h3><ul><li>纳入13811例患者的15474项注释检查，包括9595例良性诊断和5879例恶性诊断。</li><li>与以往手动排除误导性示例的研究（Sadik等人，2006；2008）不同，本研究构建的数据集遵循真实世界分布，不排除任何案例，前提是在此数据集上训练的系统更适合常规临床应用。</li><li>表2和表3列出了数据集中主要病变的类型和发病率。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021007678.png" alt="image.png"></li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008015.png" alt="image.png"></li></ul><h2 id="3-3-Data-partition——数据分区">3.3 Data partition——数据分区</h2><hr><h3 id="主要内容：-6">主要内容：</h3><ul><li>分别使用12274、1600和1600个样本进行训练集、验证集和测试集。</li><li>开源</li></ul><h1 id="4-Methods">4. Methods</h1><hr><p>首先详细说明了所提出的WBS图像自动诊断体系结构，然后介绍了图像预处理方法。</p><h2 id="4-1-Overall-architecture——总体架构">4.1 Overall architecture——总体架构</h2><hr><h3 id="说明：">说明：</h3><ol><li>一个样本包含两个图像：一个后视图图像和一个前视图图像。</li><li>单个样本包含多个图像的数据集可以表示成为 $D = {(X_i, Y_i); i = 1, 2, \ldots, N}, \quad X_i = {x_{i,1}, \ldots, x_{i,j}, \ldots, x_{i,J}}$ ，Xi表示包含J个图像的数据集中的第i个样本，Xi,J是第i个样本中的第J个图像，Yi是样本Xi的对应标签。对于本研究使用的数据集，J=2，xi,1是后视图像，xi,2是前视图像，Yi∈{恶性，良性}。</li></ol><h3 id="总体架构概述">总体架构概述:</h3><ol><li>由于网络的输入不是单一的图像，而是J图像，因此我们开发了一个J路输入网络。</li><li>第一部分，采用 深度神经网络$N_{ex}$ 对输入的J图像进行特征提取，得到 $F_{i,J} = N_{ex}(x_{i,J})$ ，这里，xi,j是第i次检查中的第j幅图像，Fi,j表示通过网络Nex提取的xi,j的高级特征。</li><li>第二部分，使用 特征融合算子$N_{fu}$ ， $S_i = N_{fu}(F_{i,1}, \ldots, F_{i,j}, \ldots, F_{i,J})$ 融合第i个样本的高级特征。这里，Si表示融合特征。</li><li>最后，利用定制的 分类神经网络$N_{cl}$ 输出真实标签的预测值， $P_i = N_{cl}(S_i)$ ，Pi是应用与样本对应的softmax函数后的模型输出。</li><li>通过最小化训练集上的交叉熵代价函数，使用反向传播算法对所提出的体系结构进行训练，定义为 $L = - \sum_{i=1}^{N} Y_i^T \ln(P_i)$</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008888.png" alt="image.png"></p><h2 id="4-2-Part-one-feature-extraction-network——第一部分：特征提取网络">4.2 Part one: feature extraction network——第一部分：特征提取网络</h2><hr><h3 id="主要内容：-7">主要内容：</h3><p>探讨了几种经典CNN：Inception-V3，DenseNet和SENet</p><h2 id="4-3-Part-two-feature-fusion-operator——第二部分：特征融合算子">4.3 Part two: feature fusion operator——第二部分：特征融合算子</h2><hr><h3 id="主要内容：-8">主要内容：</h3><p>探讨了几种特征聚合策略：</p><ul><li>最大特征融合算子 $s_{c,w,h}^{i} = \max_{j=1,\ldots,J} (f_{c,w,h}^{i,j})$</li><li>平均特征融合算子 $s_{c,w,h}^{i} = \frac{1}{J} \sum_{j=1}^{J} f_{c,w,h}^{i,j}$</li><li>空间注意特征融合算子：高级特征在空间位置上加权，并通过求和算子进行聚合。</li></ul><p>空间注意特征融合算子：</p><ol><li>首先，将特征提取网络提取的高层特征Fi、j通过卷积层，产生空间位置描述符$Mi,j$，$m^{i,j}_{1,w,h}∈R^{1×W×H}$。这里，卷积层的核尺寸为1×1，输入通道和输出通道分别等于C和1。</li><li>然后，对Mi,j应用sigmoid函数，在空间位置上产生权重描述符$Qi,j$，$q^{i,j}<em>{1,w,h}∈R^{1×W ×H}$：$$q</em>{1,w,h}^{i,j} = \frac{1}{1 + e^{-m_{1,w,h}^{i,j}}}$$</li><li>最后，将Qi,j乘以Fi,j，并使用求和运算符聚合缩放嵌入：$$s_{c,w,h}^{i} = \sum_{j=1}^{J} \left( q_{1,w,h}^{i,j} \cdot f_{c,w,h}^{i,j} \right)$$</li><li>图4中描绘了该空间注意块的细节。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008874.png" alt="image.png"></p><h2 id="4-4-Part-three-classification-network——第三部分：分类网络">4.4 Part three: classification network——第三部分：分类网络</h2><hr><h3 id="主要内容：-9">主要内容：</h3><ol><li>设计了一个自定义的标准深度神经网络（SDNN）作为网络最后一部分的分量分类器，将第二部分产生的融合特征映射输入SDNN进行最终预测。</li><li>首先采用全局池层对特征地图的空间尺寸进行归一化。</li><li>在其后面添加一个全连接层，在全局池层之后添加一个dropout层，以缓解网络过度拟合。drop概率设置为0.7。</li><li>在dropout层之后是几个模块，每个模块是三个连续操作的组合：全连接层、批量归一化（BN）和泄漏校正线性单元（LeakyReLU）。所提出的SDNN的架构如图3所示。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008670.png" alt="image.png|200"></p><h2 id="4-5-Image-pre-processing——图像预处理">4.5 Image pre-processing——图像预处理</h2><hr><h3 id="主要内容：-10">主要内容：</h3><ol><li>在输入到模型之前，使用windowWidth设置为47，windowCenter设置为23.5，将HU值转换为范围为[0, 255]的灰度图像。</li><li>颜色均反转</li><li>提出了一种基于阈值分割的感兴趣区域（ROI）提取算法，从原始图像中提取有效区域。提取的图像分辨率为201×690～975×253像素，高宽比为2.7～4.1。</li><li>将所有图像填充到一个统一的高宽比$R_{hw}$来标准化分辨率，并将它们调整到一个统一的大小，以便图像的较小边缘等于256。</li><li>在本研究中，比率$R_{hw}$固定为3.4，即整个数据集的平均比率，阈值th固定为10。预处理后的图像分辨率为256×846，几乎没有黑边。</li></ol><h1 id="5-Experimental-and-results">5. Experimental and results</h1><hr><p>首先详细描述了实验配置，包括所提出方法的实现、评估策略和评估度量。然后给出了实验结果和分析。</p><h2 id="5-1-Experimental-configuration——实验配置">5.1 Experimental configuration——实验配置</h2><hr><h3 id="Implementation—实现：">Implementation—实现：</h3><p>所有特征提取网络都在ImageNet数据集上预先训练，然后使用adadelta作为优化器在数据集上进行微调，学习率为0.1，权重衰减率为$10^{-4}$，适用于200个epoch。</p><h3 id="Evaluation-Metrics—评价指标：">Evaluation Metrics—评价指标：</h3><p>在测试集上评估每个模型的总体性能，使用达到最高精度的验证集。以敏感性、特异性、准确性和F1评分作为评价指标。</p><h3 id="Evaluation-Strategy—评价策略：">Evaluation Strategy—评价策略：</h3><ol><li>探讨了预处理方法对模型的影响</li><li>比较了现有的图像网络预训练网络作为特征提取网络的性能</li><li>分析了空间注意块的有效性</li><li>将模型与三位有经验的医师进行了比较，进一步验证了方法的有效性</li></ol><h2 id="5-2-Input-methods——输入方法">5.2 Input methods——输入方法</h2><hr><h3 id="三种输入方法：">三种输入方法：</h3><ul><li>A）直接将预处理前后图像的大小调整为256×256。该模型输入两幅图像，分辨率为256×256。</li><li>B） 将经过预处理的前后图像直接输入到模型中，改变最终池层的核大小，使池层的输出宽度和高度等于1</li><li>C）直接将预处理的前后图像输入模型，并用空间金字塔池（SPP）层替换模型中的最终池层。</li><li>这三个实验都使用Inception-V3作为特征提取网络，并使用max特征融合算子。</li><li>实验结果如表5所示。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021012510.png" alt="image.png"></li></ul><h2 id="5-3-Feature-aggregation-methods——特征融合方法">5.3 Feature aggregation methods——特征融合方法</h2><hr><p>表6给出了特征提取网络的不同架构和不同特征聚合方法的比较。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021012303.png" alt="image.png"></p><p>从表中可以看出，结合空间注意算子的Inception-V3表现最好。<br>因此，本文使用了一个带有空间注意特征聚合算子的Inception-V3特征提取网络作为最终架构</p><h2 id="5-4-Multi-view-versus-single-view——多视图与单视图">5.4 Multi-view versus single view——多视图与单视图</h2><hr><p>结果如表7所示。<br>以后视图为输入的网络性能优于前视图，表明后视图比前视图包含更多的信息。<br>此外，多视点融合网络的性能比单纯的前后视点融合网络有了很大的提高。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021013413.png" alt="image.png"></p><h2 id="5-5-Visualization——可视化">5.5 Visualization——可视化</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021014534.png" alt="image.png"></p><ul><li>使用引导反向传播算法，引导反向传播算法计算梯度与最活跃的输出层有关的输入。</li><li>前两幅图像为真阳性病例及其相应的可视化结果。可以清楚地观察到网络中的最大输出神经元与输入图像中的热点区域高度相关。</li><li>后两幅图像均为假阳性病例及相应的可视化结果。大多数假阳性病例是非典型样本，尽管预测是错误的，但该模型仍然能够聚焦图像中的热点区域。</li></ul><h2 id="5-6-Model-ensemble-and-clinical-test——模型集成与临床试验">5.6 Model ensemble and clinical test——模型集成与临床试验</h2><hr><ul><li>集成学习是提高单个模型独立训练的性能的有效方法。</li><li>集成模型的最终预测得分为所有模型的平均softmax得分。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021014526.png" alt="image.png"></li></ul><h2 id="5-7-Comparisons-between-the-model-and-experts——模型与专家的比较">5.7 Comparisons between the model and experts——模型与专家的比较</h2><hr><ul><li>将其性能与三位核医学医师进行了比较。</li><li>这三位专家可分为三个层次：无经验（&lt;800 WBS解释）、中等经验（800-5000 WBS解释）和经验丰富（&gt;5000 WBS解释）</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021017879.png" alt="image.png"></li></ul><h2 id="5-8-Analytic-experiment——分析性实验">5.8 Analytic experiment——分析性实验</h2><hr><p>所提出的方法具有良好的性能，对于检查次数较少的类型，模型仍显示出显著的召回率。</p><h1 id="6-Conclusion">6. Conclusion</h1><hr><h3 id="主要内容：-11">主要内容：</h3><ul><li>网络结构基于深度卷积神经网络，由特征提取网络、特征聚合网络和特征分类网络三部分组成，并对几种数据输入方法进行了比较。</li><li>研究了三种最先进的图像网络预训练网络作为特征提取网络：Inception-V3、DenseNet-169和SE-ResNet-50。</li><li>构建了一个包含15474个检查项的大规模带注释WBS图像数据集来训练和评估所提出的模型，表现出优越的性能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 分割 </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPECT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world/"/>
      <url>/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
