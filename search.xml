<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【论文阅读】深度学习图像数据增广方法研究综述</title>
      <link href="/image-synthesis-review-02/"/>
      <url>/image-synthesis-review-02/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><hr><h3 id="背景：">背景：</h3><p>充足的训练数据不仅可以缓解模型在训练时的过拟合问题，而且可以进一步扩大参数搜索空间，帮助模型进一步朝着全局最优解优化。<br>然而，在许多领域或任务中，获取到充足训练样本的难度和代价非常高。因此，数据增广成为一种常用的增加训练样本的手段。</p><h3 id="本文贡献：">本文贡献：</h3><p>按照方法本质原理的不同，将其分为单数据变形、多数据混合、学习数据分布和学习增广策略等 4 类方法。<br>单数据变形方法主要可以分为几何变换、色域变换、清晰度变换、噪声注入和局部擦除等 5 种；<br>多数据混合可按照图像维度的混合和特征空间下的混合进行划分；<br>学习数据分布的方法主要基于生成对抗网络和图像风格迁移的应用进行划分；<br>学习增广策略的典型方法可以按照基于元学习和基于强化学习进行分类</p><h3 id="前景：">前景：</h3><p>根据数据和任务<br>基于强化学习探索最优的组合策略，<br>基于元学习自适应地学习最优数据变形和混合方式，<br>基于生成对抗网络进一步拟合真实数据分布以采样高质量的未知数据，<br>基于风格迁移探索多模态数据互相转换的应用</p><h1 id="0-引言">0. 引言</h1><hr><p>在许多研究领域，受限于数据获取难度大、标注成本高等原因，往往难以获得充足的训练数据，这样训练得到的深度学习模型往往存在过拟合的问题，进而导致模型泛化能力差、测试精度不高等，难以满足应用需求。</p><p><strong>数据增广</strong>，又称<strong>数据增强</strong>( data augmentation) ， 是一种增加有限数据的数量和多样性的策略，旨在从有限的数据中提炼出更多有用的信息，产生等价于更多数据的价值。数据增广方法试图从过拟合问题的根源———训练样本不足，去解决该问题。</p><p>数据增广可以分为<strong>数据变形</strong> (data warping) 和 <strong>数据过采样</strong> ( oversampling)两种方法。</p><p><strong>数据变形类</strong>：LeNet-5、AlexNet、VGGNet、GoogleNet、ResNet、DenseNet中都有用到。</p><p><strong>多幅图像信息混合</strong>：SamplePairing 、mixup、SMOTE等，这类方法本质上属于<strong>数据过采样</strong>。</p><p><strong>GAN</strong>：Frid-Adar 等</p><p><strong>元学习和强化学习的思想</strong>：训练一个模型去自适应地选用最优的数据增广策略，来实现模型性能提升的最大化。AutoAugment 和 RandAugment</p><p>本文从另外的角度，即从数据增广的生成方式综述，将数据扩增方法分为单数据变形、 多数据混合、学习数据分布规律生成新数据和学习增广策略等 4 类方法。</p><h1 id="1-单数据变形">1. 单数据变形</h1><hr><h2 id="1-1-几何变换">1.1 几何变换</h2><hr><ul><li>几何变换 ( geometric transformations)是最常见的图像数据增广方法，通过旋转、镜像、平移、裁剪、 缩放和扭曲等变换方式生成新样本。</li><li>在实际任务中，需要根据数据的特点选择合适 的几何变换方法才能进一步带来模型性能的提升， 否则可能适得其反。</li><li>虽然几何变换的方式简单易操作，但也存在对数据重复记忆、增加的信息量有限等缺点，这也导致几何变换在实际应用中为模型带来的精度提升十分有限。</li></ul><h2 id="1-2-色域变换">1.2 色域变换</h2><hr><ul><li>色域变换( color space transformations) 是一种在 图像各通道上进行亮度变换的新样本生成方式。</li><li>基于色域变换的数据增广本质上是通过对数据集增加各种各样的光照亮度偏差，增强模型在不同光照条件下的鲁棒性。</li><li>对于图像分类任务，空间几何信息相比色彩信息更加重要。色域变换与几何变换存在着同样的缺点， 同时还可能丢失一些重要的颜色信息，进而改变图像原有的语义信息，这也使得该方式的应用存在较 大的局限性。</li></ul><p>示例：</p><ul><li>颜色抖动（color jittering），通过几种颜色组合模拟出大范围内多色彩模式的图像增广方式</li><li>PCA抖动（fancy PCA），对原图像进行主成分分析( PCA) ，求得协方差矩阵，然后对主成分的特征值 施加一个均值为 0 的随机扰动，然后再反变换回去。</li><li>高斯抖动，本质上通过给协方差矩阵增加噪音实现一种图像在视觉表现上的滤镜效果。</li><li>实际应用中，甚至可以使用图像编辑软件进行颜色变换。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409061017167.png" alt="image.png"></p><h2 id="1-3-清晰度变换">1.3 清晰度变换</h2><hr><ul><li>清晰度变换是一种改变图像视觉清晰度的新样本生成方式。</li><li>“核滤波器( kernel filters) ”，核滤波器通过滑动的 n × m 矩阵对图像进行卷积操作，对图像进行锐化和模糊处理，实现图像的清晰度变换。</li><li>采用这种滤波方式对数据集进行增强，不如将其作为网络的一层，还可以训练获得最优的滤波操作。</li></ul><p>示例：</p><ul><li>高斯模糊滤波器( Gaussian blur filter)</li><li>边缘滤波器( edge filter)</li><li>PatchShuffle 正则化</li></ul><h2 id="1-4-噪声注入">1.4 噪声注入</h2><hr><ul><li>噪声注入( noise injection) 是一种在图像上叠加噪声的新样本生成方式，噪声可表示为一个服从某分布的随机矩阵。</li><li>通过人为地为图像施加噪声干扰，可为数据集引入冗余和干扰信息，模拟不同成像质量的图像，增强模型对噪声干扰和冗余信息的过滤能力，提高模型对不同质量图像的识别能力。</li><li>常见的噪声种类有高斯噪声、瑞利噪声、伽马噪声、均匀噪声和椒盐噪声等</li><li>在图像上增加噪声可以帮助 CNNs 学到更加鲁棒的特征</li><li>但对于更加复杂的数据集以及多分类问题，模型训练本质是在欠拟合的情况下，噪声注入的图像扩增方式并不能带来新的有效信息，因此不能为模型带来提升效果</li><li>对抗训练：为防御对抗攻击，采用对抗样本进行训练，可以视做一种数据增广方法，用以弥补模型自身的弱点</li></ul><p>示例：</p><ul><li>前向噪声调整方案( forward noise adjustment scheme)</li><li>30 类遥感图像场景数据集</li><li>噪声叠加在图像上，产生“对抗样本( adversarial examples) ”</li><li>DisturbLabel</li></ul><h2 id="1-5-局部擦除">1.5 局部擦除</h2><hr><ul><li>不同于噪声是对图像离散的像素值信息的干扰，局部擦除则是图像局部区域所有像素值信息的丢失。</li><li>迫使模型去学习图像中更宽广的具有描述性质的特征，从而防止模型过拟合于特定的视觉特征。</li><li>根据数据和任务的不同，这种方法有时需要人为干预以保证其有效性。</li></ul><p>示例：</p><ul><li>随即擦除（random erasing），可以视为一种在数据空间的dropout</li><li>Cutout 正则化 、Hide-and-Seek 、GridMask</li><li>不规则区域的局部擦除</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062140038.png" alt="image.png"></p><h1 id="2-多数据混合">2. 多数据混合</h1><hr><p>多数据混合的方式希望将多幅图像的信息进行混合以产生新的训练数据， 可以从图像空间或特征空间进行信息混合。</p><h2 id="2-1-图像空间的数据混合">2.1 图像空间的数据混合</h2><hr><ul><li>在图像空间进行数据混合的数据增广方法，可 以分为对多幅图像的线性叠加和非线性混合，是一 类与人类直觉不一致的数据增广方式</li><li>虽然这类混合图像的方法看似不合常理，缺乏可解释性，但是对于提升模型的分类精度却十分有效，可以取得非常具有竞争力的 结果。</li></ul><p>示例：</p><ul><li>基于线性混合图像：SamplePairing、mixup和Between-Class Learning</li><li>SamplePairing 对两幅图像求平均值的方式可以看做是在两个数据的中点进行插值，mixup 可以看做是拓展到线性插值得到新样本的版本</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062239130.png" alt="image.png"></p><ul><li>mixup 数据增广方法不仅可以提高深度神经网络模型的泛化能力，而且可以有效减少模型对错误标签的记忆，增加模型对于对抗样本的鲁棒性，甚至可以稳定生成对抗网络的训练。</li><li>“CNN 中的输入数据可以被视为 波形”，“波形混合”的角度解释了图像线性叠加数据增广方法的原理，类间学习方法( between-class learning，BC) 应用到图像上，随机的比例混合两幅图像，这类线性叠加图像的方法相当于一个正则项，希望模型尽可能向线性函数去拟合，以防止强非线性导致的过拟合问题<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062314809.png" alt="image.png"></li><li>非线性图像混合<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062316532.png" alt="image.png"></li><li>多图随机裁剪拼接混合<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062317706.png" alt="image.png"></li></ul><h2 id="2-2-特征空间数据混合">2.2 特征空间数据混合</h2><hr><ul><li>借助 CNN 提取的图像特征，在特征空间进行数据增广。</li><li>针对图像数据， 在特征空间进行数据混合的方法很少被采用。</li></ul><p>示例：</p><ul><li>SMOTE 方法，一种在特征空间上进行插值生成新样本的方法</li><li>在特征空间外插值</li><li>在数据空间进行图像变换的效果要优于特征空间变换</li></ul><h1 id="3-学习数据分布">3. 学习数据分布</h1><hr><p>机器学习中的生成式方法，可以通过训练，学习数据集的潜在概率分布，在数据分布中进行过采样生成新数据，由于将整个数据集作为先验知识，这种数据增广方法在理论上是一种更加优秀的方法。</p><h2 id="3-1-生成对抗网络">3.1 生成对抗网络</h2><hr><ul><li>GAN 的核心思想源自博弈论的二人零和博弈 ( zero-sum game)</li><li>在 GAN 中， 博弈的双方是生成器 G 和判别器 D，优化过程是一 个极小极大博弈( min-max game) 问题，使生成器和判别器在不断优化中各自提高自己的生成能力和判别能力。</li><li>生成器的目标是学习真实数据的潜在分布，并生成新的数据样本，使其看起来和真的一样，达到欺骗判别器的目的</li><li>判别器是一个二分类器，其目标是找到生成出的样本和真实数据分布之间的差异，判别输入的是真实数据还是生成的样本，并且计算并输出 一个样本是否来自于真实数据分布的概率值或者标量</li><li>GAN 生成的样本用于数据增广的有效性，并且相比图像变换这类经典的数据增广方法，可以取得更好的效果</li><li>需要较为大量的数据来训练 GAN 模型，样本并不是真实世界存在的，不能将生成的样本当做真实的样本来对待</li></ul><p>示例：</p><ul><li>PG-GANs</li><li>BigGANs</li><li>DCGAN</li><li>conditional GAN</li><li>SiftingGAN<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062337018.png" alt="image.png"></li><li>真假样本混合加权训练的数据增广方法</li></ul><h2 id="3-2-图像风格迁移">3.2 图像风格迁移</h2><hr><ul><li>风格迁移，或称为“图到图翻译( image-to-image translation) ”，可以视为一种广义上的图像变换，是 一类针对图像的领域迁移( domain transfer) 问题。</li><li>本质上是建立一种不同数据分布之间的相互映射。</li></ul><p>示例：</p><ul><li>基于 conditional GAN 提出 pix2pix 方法</li><li>循环一致性生成对抗网络( cycle-consistent adversarial networks，CycleGAN)）<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062342910.png" alt="image.png"></li><li>人体器官 MR( magnetic resonance) 影像和CT影像之间的转换</li><li>同一遥感场景下 SAR ( synthetic aperture rader) 和红外影像与可见光影像之间的转换</li><li>神经风格迁移（neural style transfer），类似于颜色空间的光照变换</li></ul><h1 id="4-学习增广策略">4. 学习增广策略</h1><hr><p>借助元学习( meta-learning) 和强化学习( reinforcement learning)训练一 个模型去自适应地选用最优的数据增广策略，来实现模型性能提升的最大化。</p><h2 id="4-1-基于元学习的策略">4.1 基于元学习的策略</h2><hr><ul><li>元学习的基本思想是希望模型像人一样学会“如何学习”，即基于过去学习的知识和经验总结学习方法，进而可以快速学习新知识、适应新任务和新环境。</li><li>元学习最直接的一种理解为“用神经网络去优化神经网络”，而在数据增广方面，可以用神经网络去替代确定的数据增广方法，训练模型去学习更好的增广策略。</li></ul><p>示例：</p><ul><li>输入随机选取的两幅同一类的图像，希望通过神经网络学习两幅图像共同的内容信息或者风格信息，进而得到一幅“增强图像”，再与原始图像一同输入到分类网络中进行分类模型的训练</li></ul><h2 id="4-2-基于强化学习的策略">4.2 基于强化学习的策略</h2><hr><ul><li>从给定的图像变换和混合方法中，搜索最优的组合策略</li><li>如何对给定任务定制一组图像变换策略， 以进一步提高给定模型的预测性能，仍然是一个悬而未决的问题</li></ul><p>示例：</p><ul><li>离散搜索问题</li><li>AutoAugment，选用了强化学习作为搜索算法，搜索最优策略</li><li>RandAugment，主要思想是随机选择变换并调解变换的强度</li></ul><h1 id="5-方法分析与研究展望">5. 方法分析与研究展望</h1><hr><h2 id="5-1-不同数据增广方法选用分析">5.1 不同数据增广方法选用分析</h2><hr><ul><li>采用不合适的变换方法则可能带来负面的效果。因此，方法的适用性成为使用数据增广时首先需要考虑的问题。</li><li>虽然在选用数据增广方法时需要考虑不同种类、不同领域图像各自的特点，但是都需要具备一个核心原则: 在不改变图像原有语义信息的同时尽可能多地增加变化。</li><li>由于自然图像和遥感图像在内容理解上都经常受到遮挡因素的影响，如 自然场景前景对背景的遮挡、遥感场景中云对地物的遮挡，裁剪和局部擦除的方法可以提高模型对遮 挡的鲁棒性，而对于医疗影像其成像方式的不同，不存在遮挡的问题，使用这类数据增广方法的有效性还有待验证。</li><li>虽然在一些研究工作中已经证明使用 GAN 进行数据增广可以更有效地提高模型的精度，但是训练 GAN 模型需要一定数量的样本，对于数据量非常小的任务，不适合采用这类基于学习的方法。<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062359953.png" alt="image.png"></li></ul><h2 id="5-2-未来研究展望">5.2 未来研究展望</h2><hr><ul><li>由于图像的维度很高，同时训练 GAN 的样本也非常有限，许多情况下 GAN 对图像数据的概率分布的拟合效果并不好，导致采样生成的图像质量难以保证，限制了 GAN 作为理论上最佳数据增广方法的发展。</li><li>对于 GAN 风格迁移方面的研究和应用， 本质上是建立一种不同数据分布之间的相互映射， 对于现实生活中普遍存在的跨场景、跨模态的多领域分布的数据，可以通过构建这种映射来实现数据的互补。</li></ul><h1 id="6-结语">6. 结语</h1><hr><ul><li>数据增广作为从数据层面提高机器学习模型性 能的一项重要手段，广泛应用于各个领域，尤其是那 些样本获取成本高、标注难度大的领域。</li><li>按照增广数据的生成方式划分为四类：单数据变形、多数据混合、学习数据分布和学习增广策略</li><li>基于学习的方法对于数据增 广同样具有广阔的发展前景，主要在于以下几个方面: 根据数据和任务基于强化学习探索最优的组合策略; 基于元学习自适应地学习最优数据变形和混合方式；基于生成对抗网络进一步拟合真实数据分布以采样高质量的未知数据；基于风格迁移探索多模态数据互相转换的应用</li></ul>]]></content>
      
      
      <categories>
          
          <category> 生成 </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】医学图像数据集扩充方法研究进展</title>
      <link href="/image-synthesis-review-01/"/>
      <url>/image-synthesis-review-01/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><hr><h3 id="背景：">背景：</h3><ul><li>计算机辅助诊断（CAD）</li><li>训练样本受成像成本、标记成本和涉及患者隐私等因素的影响，导致训练图像多样性不足且难以获取。</li></ul><h3 id="本文贡献：">本文贡献：</h3><p>对医学图像数据集扩充方法的研究进展进行综述。</p><ol><li>对比分析基于几何变换和基于生成对抗网络的扩充方法；</li><li>介绍基于生成对抗网络扩充方法的改进及其适用场景；</li><li>讨论医学图像数据集扩充领域的一些亟待解决的问题并对其未来发展趋势进行展望。</li></ol><h1 id="0-引言">0. 引言</h1><hr><p>医学图像成像模态：</p><ul><li>磁共振成像 （magnetic resonance imaging，MRI）</li><li>计算机断层扫描成像（computed tomography，CT）</li><li>正电子发射断层扫描成像（positron emission computed tomography，PET）</li></ul><p>诊断难点：医学图像信息量庞大以及部分疾病的病变部位细小</p><p>医学图像数据集扩充方法：基于几何变换和基于生成对抗网络（generative adversarial network，GAN）的扩充方法</p><h1 id="1-基于几何变换的医学图像数据集扩充方法">1. 基于几何变换的医学图像数据集扩充方法</h1><hr><p>两种操作方式：</p><ol><li>针对图像中像素点的灰度值进行操作，通过一系列变换函数的映射，改变像素点位置信息，使其纹理细节与原图保持一致；</li><li>通过将图像内容变形重组，使其病变区域、感兴趣区域产生形变进而使该图像拥有更多样化的特征信息。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409051723906.png" alt="image.png"></p><p>该扩充方法对数据集多样性的提升较少。</p><h1 id="2-基于GAN的医学图像数据集扩充方法">2. 基于GAN的医学图像数据集扩充方法</h1><hr><p><strong>GAN</strong> 是一种生成式模型，其目的是构建一个从真实图像到潜在特征分布的显式映射关系，构建过程中不需要额外构造复杂的概率密度函数即可实现该映射关系。<br>GAN的训练过程可以概括为一个零和博弈的过程，其生成器希望生成的图像尽可能真实从而欺骗鉴别器，鉴别器则尽力分辨出真实图像和生成图像。</p><p>基于GAN的医学图像数据集扩充方法可以分为四类，分别为：</p><ol><li>无条件数据集扩充方法；</li><li>条件数据集扩充方法；</li><li>跨模态数据集扩充方法；</li><li>与几何变换方法结合的数据集扩充方法</li></ol><h2 id="2-1-无条件数据集扩充方法">2.1 无条件数据集扩充方法</h2><hr><p><strong>无条件数据集扩充方法</strong>是指在没有任何额外信息的情况下，仅利用高斯噪声或者均匀噪声作为GAN的输入而生成医学图像的一类方法。<br>该方法早期生成的图像存在分辨率低、图像模糊、 图像特征单一的问题，但可优化解决，其性能逐步上升。</p><p>案例：</p><ul><li>肝脏病变CT图像</li><li>3D头部MRI图像</li><li>CT肺结节图像</li><li>大脑切片MRI图像</li><li>脑部MRI图像</li></ul><p>问题：<br>图像常出现一些与生理学相违背的现象，针对无条件数据集扩充方法的改进应考虑优化生成图像的结构。</p><h2 id="2-2-条件数据集扩充方法">2.2 条件数据集扩充方法</h2><hr><p><strong>条件数据集扩充方法</strong>是指在生成新的医学图像时，一些先验信息如标签、文字、图片等跟随噪声信息一同输入到GAN的生成器。<br><strong>先验信息</strong>能够起到指导模型生成的作用，在先验信息的约束之下生成的医学图像更符合人体的生理学构造。</p><p>案例：</p><ul><li>新冠感染者的CT肺部图像</li><li>肺结节数据</li><li>高分辨率的彩色视网膜图像</li><li>皮肤图像生成与皮肤病变生成</li></ul><p>优点：<br>在继承了无条件数据集扩充方法优点的同时，能够生成特定类型的图像。</p><p>问题：<br>该方法缩小了GAN的生成器样本空间从而对生成图像的多样性起到了限制作用，若想要获得更多样化的医学图像需要训练多个网络，这增加了额外的资源消耗。</p><h2 id="2-3-跨模态数据集扩充方法">2.3 跨模态数据集扩充方法</h2><hr><p>在跨模态数据集扩充方法中，使用最多的是<strong>有监督的像素到像素GAN</strong>（pixel-to-pixel GAN， Pix2PixGAN）和<strong>无监督的循环GAN</strong>（cycle GAN， CycleGAN）。<br><strong>Pix2PixGAN</strong> 在训练过程中需要成对按像素值对齐的图像，图像的获取成本高昂；<br><strong>CycleGAN</strong> 能够适用于非对齐的医学图像，但其生成效果不如Pix2PixGAN。</p><p>案例：</p><ul><li>未对齐的目标图像被视为噪声并使用附加的配准网络进行训练以自适应地拟合未对齐的噪声分布</li><li>基于迭代的多尺度特征融合 GAN，有效降低图像的对齐损失</li><li>联邦域翻译新基准方法，缓解图像域移</li><li>有效地去除图像中的噪声分布，有效地去除图像中的噪声分布</li><li>互信息约束GAN，减少模态迁移过程中医学图像细节的丢失</li><li>边缘感知GAN，整合了边缘信息并优化生成图像的内容及结构纹理信息</li><li>两阶段的模态迁移方法</li><li>以 CycleGAN为基准生成二维超声心动图的方法</li></ul><p>优点：<br>该方法在成像清晰度、图像多样性、特征多样性、网络收敛速度等方面都具有明显优势</p><p>问题：<br>该方法需要消耗更多的计算机算力资源以及对训练数据集有更高的要求</p><h2 id="2-4-与几何变换结合的数据集扩充方法">2.4 与几何变换结合的数据集扩充方法</h2><hr><p><strong>GAN 的生成器</strong>对输入图像生成变形场、强度变换、仿射变换等<strong>模拟几何变换方法</strong>的操作，可避免生成图像缺乏特征多样性。</p><p>案例：</p><ul><li>两步的基于GAN方法，生成脑部 MRI图像的纹理</li><li>对抗性数据集扩充方法</li><li>任务驱动型数据集扩充方法，三个医学数据集（心脏、前列腺和胰腺）</li><li>新的任务驱动数据集扩充方法，改变使用GAN模型随机增强训练示例却提升有限的现状，心脏MRI图像</li><li>新型联合强化学习方法，使用弱监督的GAN模型充当代理，并在给定样本作实例的情况下输出图像掩码，肌肉骨骼X光片</li><li>正则化对抗学习方法，人工指定生成范围和双层优化预定义操作，2D皮肤癌分类和3D腹部器官分割扩充</li></ul><p>优点：<br>既能产生如基于几何变换扩充方法的真实性又能兼顾GAN生成图像的多样化</p><p>问题：<br>基于GAN的扩充方法不稳定、难训练、缺乏可解释性<br>基于几何变换扩充方法生成的图像数据分布过于一致</p><h1 id="3-总结与展望">3. 总结与展望</h1><hr><p>问题：</p><ol><li>没有提出广泛接受的生成图像质量评价标准，多用：峰值信噪比（peak signal to noise ratio， PSNR）、IS、FID等来衡量图像质量</li><li>2D医学图像无法 完整表达人体器官的结构特性</li><li>仍致力于研究单模态医学图像生成，未充分利用医学图像的多模态信息</li><li>GAN生成的图像用于医学图像分析领域可能会带来不可预知的问题</li><li>对其他领域的优秀模型的吸收和借鉴非常有限</li><li>带标注医学图像数据难以获取</li></ol><p>展望：</p><ol><li>建立广泛接受的定性评价标准</li><li>医学图像分析对高维度的图像需求也将进一步增加</li><li>从多模态图像出发生成综合性单模态图像的研究</li><li>GAN扩充的医学图像缺乏可解释性</li><li>医学图像数据集扩充领域可以吸收其它领域的优秀成果</li><li>解决标注数据集获取困难的问题</li></ol>]]></content>
      
      
      <categories>
          
          <category> 生成 </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】Automated diagnosis of bone metastasis based on multi-view bone scans using attention-augmented deep neural networks</title>
      <link href="/spect-adob/"/>
      <url>/spect-adob/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="背景：">背景：</h3><p>手动分析骨显像图像需要丰富的经验，已有骨显像图像的自动或半自动诊断方法步骤复杂且在小数据集上验证不足，准确性和可靠性较低。</p><h3 id="本文贡献：">本文贡献：</h3><p>描述了一个深度卷积神经网络的方法，该方法有两个主要创新点：首先，通过联合分析前后视图进行诊断，从而获得较高的准确性。其次，提出了一种空间注意特征聚集算子来增强空间位置信息。</p><h3 id="结果：">结果：</h3><p>高分类准确率证明了所提出的体系结构对骨显像图像诊断的有效性，可作为临床决策支持工具应用。</p><h1 id="1-Introduction">1. Introduction</h1><hr><h3 id="背景：-2">背景：</h3><p>全身骨扫描（WBS）在骨转移的鉴别诊断中与MRI具有相似的性能，但其成本远低于MRI</p><ul><li>磁共振成像（magnetic resonance imaging，MRI）</li><li>计算机断层扫描（computed tomography，CT）</li><li>全身骨扫描（whole-body bone scan，WBS）</li></ul><p>WBS图像中的异常称为热点（hot spot），通常表现为比周围环境更高的信号强度。但没有骨转移的患者也可以在WBS图像上显示热点。</p><p>骨转移的自动诊断方法的发展面临以下几个障碍：</p><ol><li>各种非肿瘤性疾病在影像学表现上也表现出异常，导致高灵敏度和低特异性。</li><li>为了在不同场景下获得较强的泛化能力，需要一个大的带注释数据集来学习骨转移的特征。然而，以往骨扫描相关研究中使用的数据集均不能满足这一要求。</li><li>每个WBS检查包含两个显示前后视图的图像。要分析是否存在骨转移，模型必须将两种观点作为一个单一的检查进行联合分析。</li></ol><h3 id="解决方法：">解决方法：</h3><ol><li>使用深度卷积神经网络（CNNs）从数据中自动提取高级特征。</li><li>构建了一个由专业核医学医师注释的大规模WBS图像数据集。</li><li>提出了一种接收多个输入的新结构，用于联合分析来自前后视图的图像。</li></ol><h3 id="贡献：">贡献：</h3><ol><li>构建了一个包含15474个核医学专业医师标记的大规模WBS图像数据集。</li><li>提出了一种基于多视点图像的骨转移瘤自动诊断模型。</li><li>提出了一种由深度神经网络参数化的特征聚合算子，用于约束检查前后视图的特征。</li><li>分类和可视化结果表明，所提出的方法成功地掌握了骨转移瘤的WBS图像特征。</li></ol><h1 id="2-Related-work">2. Related work</h1><hr><ul><li>计算机辅助诊断系统（computer-aided diagnosis systems ，CAD）</li></ul><h2 id="2-1-Traditional-methods-for-bone-scan-analyzing——传统的骨扫描分析方法">2.1 Traditional methods for bone scan analyzing——传统的骨扫描分析方法</h2><hr><h3 id="主要内容：">主要内容：</h3><ol><li>WBS图像的分析主要集中在三个方面：骨扫描指数（BSI）的自动计算、骨转移瘤的自动诊断和热点分割。然而，BSI的计算只是半自动的，并且需要费力的手动过程。</li><li>这些方法的分类性能在很大程度上依赖于热点分割，这意味着分割错误可能导致后续分类失败。此外，传统方法依赖于手工特征和阈值，因此缺乏鲁棒性。此外，人工选择特征是一种累人的、主观的、难以提高性能的方法。</li></ol><h2 id="2-2-Deep-neural-networks-for-bone-scan-analysis——深度神经网络在骨扫描分析中的应用">2.2 Deep neural networks for bone scan analysis——深度神经网络在骨扫描分析中的应用</h2><hr><h3 id="主要内容：-2">主要内容：</h3><ol><li>与传统的图像处理方法相比，Deep-cnn具有许多优点：它们以数据驱动的方式自动提取不同层次的特征，不需要手工构建特征，减少了医生的工作量。</li><li>虽然已经有一些研究对骨转移的自动诊断进行了研究，但这些研究中的大多数都是对首先需要从WBS图像分割的热点进行诊断，这一过程可能会引入额外的错误。</li></ol><h2 id="2-3-Multi-view-fusion——多视图融合">2.3 Multi-view fusion——多视图融合</h2><hr><h3 id="主要内容：-3">主要内容：</h3><ol><li>具有自然图像的图像分类任务通常一次只包含一个图像，而医学成像中的检查通常带有一组视图。</li><li>本文开发了一个使用多视图输入的自动化骨转移诊断框架。</li></ol><h1 id="3-Dataset">3. Dataset</h1><hr><p>数据标注后，将标注的考试分为训练集、验证集和测试集。</p><h2 id="3-1-Materials——资料">3.1 Materials——资料</h2><hr><h3 id="主要内容：-4">主要内容：</h3><ul><li>本研究所用WBS图像均来自四川大学华西医院核医学科。共收集记录16341份。</li><li>所有检查均使用两种设备中的一种进行，一种是分辨率为256×1024像素，另一种是分辨率为512×1024像素。</li><li>WBS图像被标记为恶性（malignant）或良性（benign）。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021007475.png" alt="image.png|400"></li></ul><h2 id="3-2-Data-annotation——数据批注">3.2 Data annotation——数据批注</h2><hr><h3 id="主要内容：-5">主要内容：</h3><ul><li>纳入13811例患者的15474项注释检查，包括9595例良性诊断和5879例恶性诊断。</li><li>与以往手动排除误导性示例的研究（Sadik等人，2006；2008）不同，本研究构建的数据集遵循真实世界分布，不排除任何案例，前提是在此数据集上训练的系统更适合常规临床应用。</li><li>表2和表3列出了数据集中主要病变的类型和发病率。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021007678.png" alt="image.png"></li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008015.png" alt="image.png"></li></ul><h2 id="3-3-Data-partition——数据分区">3.3 Data partition——数据分区</h2><hr><h3 id="主要内容：-6">主要内容：</h3><ul><li>分别使用12274、1600和1600个样本进行训练集、验证集和测试集。</li><li>开源</li></ul><h1 id="4-Methods">4. Methods</h1><hr><p>首先详细说明了所提出的WBS图像自动诊断体系结构，然后介绍了图像预处理方法。</p><h2 id="4-1-Overall-architecture——总体架构">4.1 Overall architecture——总体架构</h2><hr><h3 id="说明：">说明：</h3><ol><li>一个样本包含两个图像：一个后视图图像和一个前视图图像。</li><li>单个样本包含多个图像的数据集可以表示成为 $D = {(X_i, Y_i); i = 1, 2, \ldots, N}, \quad X_i = {x_{i,1}, \ldots, x_{i,j}, \ldots, x_{i,J}}$ ，Xi表示包含J个图像的数据集中的第i个样本，Xi,J是第i个样本中的第J个图像，Yi是样本Xi的对应标签。对于本研究使用的数据集，J=2，xi,1是后视图像，xi,2是前视图像，Yi∈{恶性，良性}。</li></ol><h3 id="总体架构概述">总体架构概述:</h3><ol><li>由于网络的输入不是单一的图像，而是J图像，因此我们开发了一个J路输入网络。</li><li>第一部分，采用 深度神经网络$N_{ex}$ 对输入的J图像进行特征提取，得到 $F_{i,J} = N_{ex}(x_{i,J})$ ，这里，xi,j是第i次检查中的第j幅图像，Fi,j表示通过网络Nex提取的xi,j的高级特征。</li><li>第二部分，使用 特征融合算子$N_{fu}$ ， $S_i = N_{fu}(F_{i,1}, \ldots, F_{i,j}, \ldots, F_{i,J})$ 融合第i个样本的高级特征。这里，Si表示融合特征。</li><li>最后，利用定制的 分类神经网络$N_{cl}$ 输出真实标签的预测值， $P_i = N_{cl}(S_i)$ ，Pi是应用与样本对应的softmax函数后的模型输出。</li><li>通过最小化训练集上的交叉熵代价函数，使用反向传播算法对所提出的体系结构进行训练，定义为 $L = - \sum_{i=1}^{N} Y_i^T \ln(P_i)$</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008888.png" alt="image.png"></p><h2 id="4-2-Part-one-feature-extraction-network——第一部分：特征提取网络">4.2 Part one: feature extraction network——第一部分：特征提取网络</h2><hr><h3 id="主要内容：-7">主要内容：</h3><p>探讨了几种经典CNN：Inception-V3，DenseNet和SENet</p><h2 id="4-3-Part-two-feature-fusion-operator——第二部分：特征融合算子">4.3 Part two: feature fusion operator——第二部分：特征融合算子</h2><hr><h3 id="主要内容：-8">主要内容：</h3><p>探讨了几种特征聚合策略：</p><ul><li>最大特征融合算子 $s_{c,w,h}^{i} = \max_{j=1,\ldots,J} (f_{c,w,h}^{i,j})$</li><li>平均特征融合算子 $s_{c,w,h}^{i} = \frac{1}{J} \sum_{j=1}^{J} f_{c,w,h}^{i,j}$</li><li>空间注意特征融合算子：高级特征在空间位置上加权，并通过求和算子进行聚合。</li></ul><p>空间注意特征融合算子：</p><ol><li>首先，将特征提取网络提取的高层特征Fi、j通过卷积层，产生空间位置描述符$Mi,j$，$m^{i,j}_{1,w,h}∈R^{1×W×H}$。这里，卷积层的核尺寸为1×1，输入通道和输出通道分别等于C和1。</li><li>然后，对Mi,j应用sigmoid函数，在空间位置上产生权重描述符$Qi,j$，$q^{i,j}<em>{1,w,h}∈R^{1×W ×H}$：$$q</em>{1,w,h}^{i,j} = \frac{1}{1 + e^{-m_{1,w,h}^{i,j}}}$$</li><li>最后，将Qi,j乘以Fi,j，并使用求和运算符聚合缩放嵌入：$$s_{c,w,h}^{i} = \sum_{j=1}^{J} \left( q_{1,w,h}^{i,j} \cdot f_{c,w,h}^{i,j} \right)$$</li><li>图4中描绘了该空间注意块的细节。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008874.png" alt="image.png"></p><h2 id="4-4-Part-three-classification-network——第三部分：分类网络">4.4 Part three: classification network——第三部分：分类网络</h2><hr><h3 id="主要内容：-9">主要内容：</h3><ol><li>设计了一个自定义的标准深度神经网络（SDNN）作为网络最后一部分的分量分类器，将第二部分产生的融合特征映射输入SDNN进行最终预测。</li><li>首先采用全局池层对特征地图的空间尺寸进行归一化。</li><li>在其后面添加一个全连接层，在全局池层之后添加一个dropout层，以缓解网络过度拟合。drop概率设置为0.7。</li><li>在dropout层之后是几个模块，每个模块是三个连续操作的组合：全连接层、批量归一化（BN）和泄漏校正线性单元（LeakyReLU）。所提出的SDNN的架构如图3所示。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008670.png" alt="image.png|200"></p><h2 id="4-5-Image-pre-processing——图像预处理">4.5 Image pre-processing——图像预处理</h2><hr><h3 id="主要内容：-10">主要内容：</h3><ol><li>在输入到模型之前，使用windowWidth设置为47，windowCenter设置为23.5，将HU值转换为范围为[0, 255]的灰度图像。</li><li>颜色均反转</li><li>提出了一种基于阈值分割的感兴趣区域（ROI）提取算法，从原始图像中提取有效区域。提取的图像分辨率为201×690～975×253像素，高宽比为2.7～4.1。</li><li>将所有图像填充到一个统一的高宽比$R_{hw}$来标准化分辨率，并将它们调整到一个统一的大小，以便图像的较小边缘等于256。</li><li>在本研究中，比率$R_{hw}$固定为3.4，即整个数据集的平均比率，阈值th固定为10。预处理后的图像分辨率为256×846，几乎没有黑边。</li></ol><h1 id="5-Experimental-and-results">5. Experimental and results</h1><hr><p>首先详细描述了实验配置，包括所提出方法的实现、评估策略和评估度量。然后给出了实验结果和分析。</p><h2 id="5-1-Experimental-configuration——实验配置">5.1 Experimental configuration——实验配置</h2><hr><h3 id="Implementation—实现：">Implementation—实现：</h3><p>所有特征提取网络都在ImageNet数据集上预先训练，然后使用adadelta作为优化器在数据集上进行微调，学习率为0.1，权重衰减率为$10^{-4}$，适用于200个epoch。</p><h3 id="Evaluation-Metrics—评价指标：">Evaluation Metrics—评价指标：</h3><p>在测试集上评估每个模型的总体性能，使用达到最高精度的验证集。以敏感性、特异性、准确性和F1评分作为评价指标。</p><h3 id="Evaluation-Strategy—评价策略：">Evaluation Strategy—评价策略：</h3><ol><li>探讨了预处理方法对模型的影响</li><li>比较了现有的图像网络预训练网络作为特征提取网络的性能</li><li>分析了空间注意块的有效性</li><li>将模型与三位有经验的医师进行了比较，进一步验证了方法的有效性</li></ol><h2 id="5-2-Input-methods——输入方法">5.2 Input methods——输入方法</h2><hr><h3 id="三种输入方法：">三种输入方法：</h3><ul><li>A）直接将预处理前后图像的大小调整为256×256。该模型输入两幅图像，分辨率为256×256。</li><li>B） 将经过预处理的前后图像直接输入到模型中，改变最终池层的核大小，使池层的输出宽度和高度等于1</li><li>C）直接将预处理的前后图像输入模型，并用空间金字塔池（SPP）层替换模型中的最终池层。</li><li>这三个实验都使用Inception-V3作为特征提取网络，并使用max特征融合算子。</li><li>实验结果如表5所示。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021012510.png" alt="image.png"></li></ul><h2 id="5-3-Feature-aggregation-methods——特征融合方法">5.3 Feature aggregation methods——特征融合方法</h2><hr><p>表6给出了特征提取网络的不同架构和不同特征聚合方法的比较。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021012303.png" alt="image.png"></p><p>从表中可以看出，结合空间注意算子的Inception-V3表现最好。<br>因此，本文使用了一个带有空间注意特征聚合算子的Inception-V3特征提取网络作为最终架构</p><h2 id="5-4-Multi-view-versus-single-view——多视图与单视图">5.4 Multi-view versus single view——多视图与单视图</h2><hr><p>结果如表7所示。<br>以后视图为输入的网络性能优于前视图，表明后视图比前视图包含更多的信息。<br>此外，多视点融合网络的性能比单纯的前后视点融合网络有了很大的提高。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021013413.png" alt="image.png"></p><h2 id="5-5-Visualization——可视化">5.5 Visualization——可视化</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021014534.png" alt="image.png"></p><ul><li>使用引导反向传播算法，引导反向传播算法计算梯度与最活跃的输出层有关的输入。</li><li>前两幅图像为真阳性病例及其相应的可视化结果。可以清楚地观察到网络中的最大输出神经元与输入图像中的热点区域高度相关。</li><li>后两幅图像均为假阳性病例及相应的可视化结果。大多数假阳性病例是非典型样本，尽管预测是错误的，但该模型仍然能够聚焦图像中的热点区域。</li></ul><h2 id="5-6-Model-ensemble-and-clinical-test——模型集成与临床试验">5.6 Model ensemble and clinical test——模型集成与临床试验</h2><hr><ul><li>集成学习是提高单个模型独立训练的性能的有效方法。</li><li>集成模型的最终预测得分为所有模型的平均softmax得分。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021014526.png" alt="image.png"></li></ul><h2 id="5-7-Comparisons-between-the-model-and-experts——模型与专家的比较">5.7 Comparisons between the model and experts——模型与专家的比较</h2><hr><ul><li>将其性能与三位核医学医师进行了比较。</li><li>这三位专家可分为三个层次：无经验（&lt;800 WBS解释）、中等经验（800-5000 WBS解释）和经验丰富（&gt;5000 WBS解释）</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021017879.png" alt="image.png"></li></ul><h2 id="5-8-Analytic-experiment——分析性实验">5.8 Analytic experiment——分析性实验</h2><hr><p>所提出的方法具有良好的性能，对于检查次数较少的类型，模型仍显示出显著的召回率。</p><h1 id="6-Conclusion">6. Conclusion</h1><hr><h3 id="主要内容：-11">主要内容：</h3><ul><li>网络结构基于深度卷积神经网络，由特征提取网络、特征聚合网络和特征分类网络三部分组成，并对几种数据输入方法进行了比较。</li><li>研究了三种最先进的图像网络预训练网络作为特征提取网络：Inception-V3、DenseNet-169和SE-ResNet-50。</li><li>构建了一个包含15474个检查项的大规模带注释WBS图像数据集来训练和评估所提出的模型，表现出优越的性能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 分割 </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPECT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world/"/>
      <url>/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
