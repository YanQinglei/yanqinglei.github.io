<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【论文阅读】High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</title>
      <link href="/gan-hris/"/>
      <url>/gan-hris/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="方法：">方法：</h3><p>一种新的对抗性损失函数以及新颖的多尺度生成器和判别器架构，生成了2048×1024分辨率的图像。<br>加入了两个用于交互式视觉操作的新功能。</p><ol><li>整合了对象实例分割信息，使得能够进行对象的操作，例如移除/添加对象以及更改对象类别。</li><li>提出了一种方法，在相同输入下生成多样化的结果，允许用户交互式地编辑对象外观。</li></ol><h1 id="1-Introduction">1. Introduction</h1><hr><p>讨论了一种新的方法，产生高分辨率图像的语义标签映射。<br>首先仅通过对抗性训练获得结果，而不依赖任何手工制作的损失或预先训练的网络进行感知损失。<br>然后，如果预训练网络可用，则添加来自预训练网络的感知损失可以在某些情况下略微改善结果。<br>利用实例级对象分割信息，将同一类别内的不同对象实例进行分割。<br>提出了一种方法来生成不同的结果，给定相同的输入标签映射，允许用户交互编辑相同对象的外观。</p><h1 id="2-Related-Work">2. Related Work</h1><hr><h3 id="Generative-adversarial-networks（生成对抗网络）：">Generative adversarial networks（生成对抗网络）：</h3><p>旨在通过强制生成的样本与自然图像不可区分来模拟自然图像分布。</p><h3 id="Image-to-image-translation（图像到图像的转换）：">Image-to-image translation（图像到图像的转换）：</h3><p>在给定输入输出图像对作为训练数据的情况下，将输入图像从一个域翻译到另一个域。<br>基于感知损失的直接回归目标，并生成了第一个能够合成2048×1024图像的模型。（高分辨率）</p><h3 id="Deep-visual-manipulation（深度视觉操作）：">Deep visual manipulation（深度视觉操作）：</h3><p>基于GANs学习的先验知识的对象外观编辑优化方法。<br>专注于对象级语义编辑，允许用户与整个场景交互并操纵图像中的单个对象。<br>系统允许更灵活的操作，并产生实时高分辨率的结果。</p><h1 id="3-Instance-Level-Image-Synthesis">3. Instance-Level Image Synthesis</h1><hr><p>提出了一个条件对抗框架，用于从语义标签地图生成高分辨率照片真实感图像。</p><h2 id="3-1-The-pix2pix-Baseline——像素到像素基准模型">3.1 The pix2pix Baseline——像素到像素基准模型</h2><hr><p>生成器G的目标是将语义标签图转换为逼真的图像，而判别器D的目标是区分真实图像与生成的图像。<br>训练数据集是由对应图像对{(si, xi)}组成的，其中si是语义标签图，xi是对应的自然照片。<br>输入语义标签图的真实图像的条件分布：<br>$$\min_G \max_D L_{\text{GAN}}(G, D)$$<br>目标函数：<br>$$L_{\text{GAN}}(G, D) = \mathbb{E}_{(s, x)}[\log D(s, x)] + \mathbb{E}_s[\log(1 - D(s, G(s)))]$$<br>采用U-Net作为生成器，采用基于Patch的全卷积网络作为判别器。<br>输入判别器的是语义标签图和对应图像的逐通道拼接。</p><h2 id="3-2-Improving-Photorealism-and-Resolution——提高图片真实感和分辨率">3.2 Improving Photorealism and Resolution——提高图片真实感和分辨率</h2><hr><h3 id="Coarse-to-fine-generator（粗糙到精细的生成器）：">Coarse-to-fine generator（粗糙到精细的生成器）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409121843542.png" alt="image.png"><br>将生成器分解为两个子网络：G1 和 G2，生成器表示为一个元组 G = {G1, G2}。将 G1 称为全局生成器网络，将 G2 称为局部增强器网络。<br>全局生成器网络在 1024 × 512 的分辨率下运行，局部增强器网络输出的图像分辨率是前一网络输出的 4 倍（每个图像维度上增加 2 倍）。<br>为了生成更高分辨率的图像，可以使用额外的局部增强器网络。</p><p>全局生成器网络包含 3 个组件：卷积前端 G(F)1、一组残差块 G®1和反卷积后端 G(B)1。<br>局部增强器网络同样由 3 个组件组成：卷积前端 G(F)2、一组残差块 G®2 和反卷积后端 G(B)2。</p><p>G1 输入分辨率为 1024 × 512 的语义标签图，输出分辨率为 1024 × 512 的图像。<br>G2 输入分辨率为 2048 × 1024 的语义标签图，输入到残差块 G®2 的特征图是两个特征图的逐元素和：G(F)2 的输出特征图和全局生成网络 G(B)1 后端的最后一个特征图。这有助于将来自 G1 的全局信息整合到 G2 中，有效地整合全局和局部信息。</p><h3 id="Multi-scale-discriminators（多尺度判别器）：">Multi-scale discriminators（多尺度判别器）：</h3><p>使用 3 个判别器，它们的网络结构相同，但在不同的图像尺度上运行，称为 D1、D2 和 D3。<br>将真实和合成的高分辨率图像分别按 2 倍和 4 倍的比例进行下采样，创建一个包含 3 个尺度的图像金字塔。<br>判别器 D1、D2 和 D3 分别在这 3 个不同的尺度上训练，以区分真实图像和合成图像。<br>运行在最粗尺度的判别器具有最大的感受野，对图像有更全局的视角，能够引导生成器生成全局一致的图像。<br>运行在最细尺度的判别器则专注于引导生成器生成更精细的细节。<br>这使得训练粗到细生成器变得更容易，因为将低分辨率模型扩展到更高分辨率只需在最细级别添加一个额外的判别器，而不必从头开始训练。</p><p>加入判别器后的公式：<br>$$\min_G \max_{D_1, D_2, D_3} \sum_{k=1}^{3} \mathcal{L}_{GAN}(G, D_k)$$</p><h3 id="Improved-adversarial-loss（改进的对抗损失）：">Improved adversarial loss（改进的对抗损失）：</h3><p>在 GAN 损失中加入基于鉴别器的特征匹配损失来改进损失函数。<br>从判别器的多个层中提取特征，并学习匹配这些来自真实图像和合成图像的中间表示。<br>特征匹配损失 $L_{FM}(G, Dk)$ 为：<br>$$L_{FM}(G, D_k) = \mathbb{E}<em>{(s, x)} \sum</em>{i=1}^{T} \frac{1}{N_i} \left[ \left| D^{(i)}<em>k(s, x) - D^{(i)}<em>k(s, G(s)) \right|<em>1 \right]$$<br>判别器 Dk 的第 i 层特征提取器表示为 $D(i)<em>k$（从输入到 Dk 的第 i 层），T 是总层数，$N_i$​ 表示每层中的元素数量。<br>GAN 判别器特征匹配损失与感知损失相关。<br>整体目标将 GAN 损失和特征匹配损失结合为：<br>$$\min_G \left( \max</em>{D_1, D_2, D_3} \sum</em>{k=1,2,3} L</em>{GAN}(G, D_k) + \lambda \sum</em>{k=1,2,3} L_{FM}(G, D_k) \right)$$<br>λ 控制两个项的相对重要性。对于特征匹配损失 $L_{FM}$​，$D_k$​ 仅作为特征提取器，并不优化损失 $L_{FM}$​。</p><h2 id="3-3-Using-Instance-Maps——使用实例级映射">3.3 Using Instance Maps——使用实例级映射</h2><hr><p>现有的图像合成方法仅使用语义标签图，这种图像中每个像素值表示该像素所属的对象类别。这个图像不能区分同一类别的不同对象。<br>实例级语义标签图包含每个独立对象的唯一对象 ID。</p><p>结合实例图方法：<br>直接将其传递到网络中或将其编码为一个独热向量。难实现。<br>为每个类别预分配一个固定数量的通道。但当数量设置得过小或过大时都会出现问题：数量过小时无法覆盖所有对象，数量过大时则浪费内存。</p><p><strong>实例图提供的最重要的信息是对象边界。</strong><br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122035631.png" alt="image.png"><br>实例边界图中如果对象 ID 与任何 4 个邻域的对象 ID 不同，设像素为 1，否则为 0。<br>然后，将实例边界图与语义标签图的独热向量表示进行连接，并输入到生成器网络中。<br>同样，判别器的输入是实例边界图、语义标签图和真实/合成图像的通道级连接。</p><h2 id="3-4-Learning-an-Instance-level-Feature-Embedding——学习实例级特征嵌入">3.4 Learning an Instance-level Feature Embedding——学习实例级特征嵌入</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122051088.png" alt="image.png"><br>为图像中的每个实例添加额外的低维特征通道作为生成器的输入。<br>通过操控这些特征，可以对合成过程进行灵活控制。<br>由于添加的特征是连续量，模型在原则上能够生成无限多的图像。</p><p>训练一个编码器 E 来学习一个与真实图像对应的特征图。<br>为了使每个实例中的特征一致，在编码器的输出上添加了一个实例级的平均池化层。<br>然后，将平均特征广播到同一实例的所有像素位置。<br>获取特征图 E(x) 后，在 Eq. (4) 中用 G(s, E(x)) 替代 G(s)，通过将标签图 s 和 E(x) 连接起来，并将编码器与生成器一起端到端地训练，这使得编码器能够捕捉生成器使用的最具代表性的特征。</p><p>在推理时进行交互式编辑时，编码器训练完成后，首先在训练图像中的所有实例上运行它，并记录获得的特征。<br>对这些特征进行 K-means 聚类，以每个语义类别为单位，每个聚类编码了特定风格的特征。<br>在推理时，随机选择一个聚类中心并使用它作为编码特征。</p><h1 id="4-Results">4. Results</h1><hr><h3 id="Implementation-details（实现细节）：">Implementation details（实现细节）：</h3><p>权重 λ 设置为 10 ， K-means 的 K 设置为 10。<br>使用三维向量来编码每个对象实例的特征。<br>在目标函数中添加感知损失：<br>$$\lambda \sum_{i=1}^{N} \frac{1}{M_i} \left[ \lVert F^{(i)}(x) - F^{(i)}(G(s)) \rVert_1 \right]$$<br>λ = 10，F(i) 表示 VGG 网络的第 i 层，具有 Mi 个元素。<br>将这两种变体命名为“ours”和“ours (w/o VGG loss)”。</p><h3 id="Datasets（数据集）：">Datasets（数据集）：</h3><p>Cityscapes 数据集<br> NYU Indoor RGBD 数据集<br> ADE20K 数据集<br> Helen Face 数据集</p><h3 id="Baselines（基准模型）：">Baselines（基准模型）：</h3><p>pix2pix<br>CRN</p><h2 id="4-1-Quantitative-Comparisons——定量比较">4.1 Quantitative Comparisons——定量比较</h2><hr><p>对合成图像进行语义分割，并比较预测的分割结果与输入的匹配程度。<br>分割准确率：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122102655.png" alt="image.png"></p><h2 id="4-2-Human-Perceptual-Study——人的感知实验">4.2 Human Perceptual Study——人的感知实验</h2><hr><p>AMT平台<br>Cityscapes 数据集</p><h3 id="Unlimited-time（无限时间）：">Unlimited time（无限时间）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122106838.png" alt="image.png"></p><h3 id="Limited-time（有限时间）：">Limited time（有限时间）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122107772.png" alt="image.png"><br>合成结果示例：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122108820.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122109368.png" alt="image.png"></p><h3 id="Analysis-of-the-loss-function（损失函数分析）：">Analysis of the loss function（损失函数分析）：</h3><p>GAN 损失、基于鉴别器的特征匹配损失和 VGG 感知损失。<br>添加特征匹配损失显著提高了性能，而添加感知损失进一步增强了结果。</p><h3 id="Using-instance-maps（使用实例图）：">Using instance maps（使用实例图）：</h3><p>使用实例图提高了结果的真实感。</p><h3 id="Additional-datasets（额外数据集）：">Additional datasets（额外数据集）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122117206.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122116762.png" alt="image.png"></p><h3 id="Network-architectures（网络架构）：">Network architectures（网络架构）：</h3><p>粗糙到精细生成器和多尺度鉴别器都可以稳定 GAN 训练，并生成更具视觉吸引力的结果。</p><h2 id="4-3-Interactive-Object-Editing——交互式对象编辑">4.3 Interactive Object Editing——交互式对象编辑</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122120185.png" alt="image.png"><br>更改图像中的对象标签，以快速创建新场景。<br>更改单个汽车的颜色，或改变道路的纹理。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409122121334.png" alt="image.png"><br>更改面部颜色以模拟不同的化妆效果或在面部添加胡须。</p><h1 id="5-Discussion-and-Conclusion">5. Discussion and Conclusion</h1><hr><p>条件 GAN 能够合成高分辨率的逼真图像，而无需任何手工设计的损失函数或预训练网络。<br>加入感知损失可以略微改善结果。<br>在需要高分辨率结果但没有可用预训练网络的领域（如医学成像和生物学）中可能会非常有用。<br>图像到图像合成管道可以扩展以产生多样化的输出，并在适当的训练输入输出对下实现交互式图像操作。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</title>
      <link href="/gan-uiti/"/>
      <url>/gan-uiti/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="背景：">背景：</h3><p>成对训练数据缺乏。</p><h3 id="方法：">方法：</h3><p>学习一个映射 $G:X→Y$，使得来自G(X)的图像分布与使用对抗性损失的分布Y是不可区分的。<br>由于该映射是高度欠约束的，将其与逆映射 $F:Y→X$ 耦合，并引入循环一致性损失提出 $F(G(X))≈X$。</p><h1 id="1-Introduction">1. Introduction</h1><hr><p>提出系统：在没有任何成对的训练例子，捕捉一个图像采集的特殊特征，找出如何将这些特征转化为其他图像采集。</p><p>尽管缺乏成对示例形式的监督，但可以利用集合级别的监督：给定领域 X 中的一组图像和领域 Y 中的另一组图像。<br>训练一个映射 $G:X→Y$，使得 $x∈X$ 的输出 $\hat{y} = G(x)$ ，对于对抗器来说 $\hat{y}$ 与领域 Y 中的图像 $y$ 无法区分。<br>理论上，这一目标可以在 $\hat{y}$​ 上引导出一个输出分布，使其与经验分布 $p_Y(y)$ 相匹配（通常需要 G 是随机的）。<br>因此，最优的 G 将领域 X 转换为分布与领域 Y 完全一致的领域 $\hat{Y}$。<br>然而，这种转换并不能保证输入和输出 x 与 y 在个体层面上有有意义的配对——存在无限多个映射 G，它们可以诱导出相同的 $\hat{y}$ 分布。<br>此外，在实际操作中，单独优化对抗性目标是困难的：标准程序经常导致模式崩溃问题，即所有输入图像都映射到相同的输出图像，优化无法取得进展。</p><p>这些问题促使本文在目标函数中增加更多的结构：<br>一个转换器 $G:X→Y$ 和另一个转换器 $F:Y→X$，那么 G 和 F 应该互为逆函数，且两个映射应该是双映射的。<br>同时训练映射 G 和 F 来应用这一结构假设，并增加一个循环一致性损失，鼓励 $F(G(x))≈x$ 和 $G(F(y))≈y$ 。<br>将这一损失与域 XX 和 YY 上的对抗损失相结合，形成了本文用于无配对图像到图像转换的完整目标函数。</p><h1 id="2-Related-work">2. Related work</h1><hr><h3 id="Generative-Adversarial-Networks-GANs-——生成对抗网络：">Generative Adversarial Networks(GANs)——生成对抗网络：</h3><p>应用：</p><ul><li>image generation</li><li>image editing</li><li>representation learnin</li><li>text2image</li><li>image inpainting</li><li>future prediction</li></ul><p>GANs 关键在于对抗性损失，这迫使生成的图像在原则上与真实图像无法区分。<br>本文采用对抗性损失学习映射，使得翻译后的图像无法与目标域中的图像区分开来。</p><h3 id="Image-to-Image-Translation——图像到图像的转换：">Image-to-Image Translation——图像到图像的转换：</h3><p>追溯：</p><ul><li>Image Analogies</li><li>非参数化的纹理模型</li><li>通过卷积神经网络 (CNN) 学习一个参数化的转换函数</li></ul><p>本文方法基于“pix2pix”框架。<br>该框架使用条件生成对抗网络 (cGAN) 来学习从输入到输出图像的映射。<br>本文在没有成对训练示例的情况下学习映射。</p><h3 id="Unpaired-Image-to-Image-Translation——未成对图像到图像的转换：">Unpaired Image-to-Image Translation——未成对图像到图像的转换：</h3><p>近期：</p><ul><li>贝叶斯框架</li><li>CoupledGANs</li><li>跨模态场景网络</li><li>变分自动编码器</li><li>鼓励输入和输出共享某些“内容”特征</li><li>类标签空间</li><li>图像像素空间</li><li>图像特征空间</li></ul><p>本文的公式不依赖于任何特定于任务的、预定义的输入和输出之间的相似性函数，也不假设输入和输出必须位于同一低维嵌入空间中。</p><h3 id="Cycle-Consistency——循环一致性：">Cycle Consistency——循环一致性：</h3><p>近期：</p><ul><li>运动结构恢复</li><li>3D 形状匹配</li><li>共分割</li><li>密集语义对齐</li><li>深度估计</li></ul><p>使用传递性作为正则化结构化数据的一种方式由来已久。<br>本文类似使用循环一致性损失作为利用传递性来监督 CNN 训练的一种方式。</p><h3 id="Neural-Style-Transfer——神经风格转换：">Neural Style Transfer——神经风格转换：</h3><p>通过匹配预先训练的深度特征的Gram矩阵统计信息，将一个图像的内容与另一个图像（通常是绘画）的样式相结合，从而合成新图像。<br>通过尝试捕捉高层外观结构之间的对应关系，学习两个领域之间的映射，而不是两个特定的图像之间的映射。</p><h1 id="3-Formulation">3. Formulation</h1><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112008921.png" alt="image.png"></p><p><strong>目标</strong>：在给定训练样本 ${x_i}<em>{i=1}^N \in X$ 和 ${y_j}</em>{j=1}^M \in Y$ 的情况下，学习两个领域 X 和 Y 之间的映射函数。</p><p><strong>模型</strong>：包括两个映射 $G:X→Y$ 和 $F:Y→X$ 。两个对抗判别器 DX​ 和 DY。<br>DX​ 旨在区分图像 {x} 和转换后的图像 {F(y)}；DY​ 旨在区分 {y} 和转换后的图像 {G(x)}。</p><p><strong>目标包含两类项</strong>：<br>用于将生成图像的分布与目标领域中的数据分布相匹配的对抗性损失；<br>防止学习到的映射 G 和 F 彼此矛盾的循环一致性损失。</p><h2 id="3-1-Adversarial-Loss——对抗性损失">3.1 Adversarial Loss——对抗性损失</h2><hr><p>对两个映射函数应用对抗性损失。</p><p>对于映射函数 $G:X→Y$ 及其判别器 DY：<br>$$L_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}<em>{y \sim p</em>{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}<em>{x \sim p</em>{\text{data}}(x)}[\log(1 - D_Y(G(x)))]$$</p><p>$F:Y→X$ 和 DX 同理：<br>$$L_{\text{GAN}}(F, D_X, Y, X)$$</p><h2 id="3-2-Cycle-Consistency-Loss——循环一致性损失">3.2 Cycle Consistency Loss——循环一致性损失</h2><hr><p>前向循环一致性：对于来自领域 X 的每一张图像 x，图像转换循环应当能够将 x 带回原始图像。<br>$$x→G(x)→F(G(x))≈x$$<br>后向循环一致性：<br>$$y→F(y)→G(F(y))≈y$$<br>通过循环一致性损失来鼓励这种行为：<br>$$L_{\text{cyc}}(G, F) = \mathbb{E}<em>{x \sim p</em>{\text{data}}(x)}[|F(G(x)) - x|<em>1] + \mathbb{E}</em>{y \sim p_{\text{data}}(y)}[|G(F(y)) - y|_1]$$</p><h2 id="3-3-Full-Objective——完整目标">3.3 Full Objective——完整目标</h2><hr><p>完整目标：<br>$$L(G, F, D_X, D_Y) = L_{\text{GAN}}(G, D_Y, X, Y) + L_{\text{GAN}}(F, D_X, Y, X) + \lambda L_{\text{cyc}}(G, F)$$<br>λ 控制这两个目标的重要性比重。</p><p>希望解决以下优化问题：<br>$$G^<em>, F^</em> = \arg \min_{G,F} \max_{D_X, D_Y} L(G, F, D_X, D_Y)$$</p><p>本模型可以视为在训练两个“自编码器”：<br>自编码器 $F \circ G : X \to X$ 和自编码器 $G \circ F : Y \to Y$ 。<br>通过将图像转换到另一个领域的中间表示再映射回原始图像。<br>“对抗性自编码器”（adversarial autoencoder）</p><h1 id="4-Implementation">4. Implementation</h1><hr><h3 id="Network-Architecture（网络架构）：">Network Architecture（网络架构）：</h3><p>网络包含两个stride-2卷积、几个残差块和两个stride1/2卷积，使用instance normalization。<br>判别器使用70×70 PatchGANs。</p><h3 id="Training-details（训练细节）：">Training details（训练细节）：</h3><p>对于$L_{\text{GAN}}$ ，将负对数似然目标替换为最小二乘损失（least square loss）。<br>方程1变为：<br>$$L_{\text{LSGAN}}(G, D_Y, X, Y) = \mathbb{E}<em>{y \sim p</em>{\text{data}}(y)}\left[(D_Y(y) - 1)^2\right] + \mathbb{E}<em>{x \sim p</em>{\text{data}}(x)}\left[D_Y(G(x))^2\right].$$</p><p>设置一个图像缓冲区，存储之前生成的 50 张图像。<br>使用历史生成图像而非最新生成网络产生的图像来更新判别器 DX​ 和 DY。</p><h1 id="5-Results">5. Results</h1><hr><h2 id="5-1-Evaluation——评估">5.1 Evaluation——评估</h2><hr><h3 id="5-1-1-Baselines——基准模型">5.1.1 Baselines——基准模型</h3><hr><p><strong>CoGAN</strong><br>为领域 X 和领域 Y 各学习一个 GAN 生成器，并在前几层共享权重。<br>从 X 到 Y 的转换可以通过找到一个生成图像 X 的潜在表示，再将该潜在表示转换为 Y 风格来实现。</p><p><strong>Pixel loss + GAN</strong><br>使用对抗性损失来训练从 X 到 Y 的转换。正则项 $|X-Y|1$ 用于惩罚像素级别的大变化。</p><p><strong>Feature loss + GAN</strong><br>L1​ 损失是在预训练网络的深度图像特征上计算的，而不是在 RGB 像素值上计算的。</p><p><strong>BiGAN/ALI</strong><br>无条件 GANs 学习一个生成器 $G:Z→X$，将随机噪声 Z 映射为图像 X同时学习反向映射函数 $F:X→Z$ 。</p><p><strong>pix2pix</strong></p><h3 id="5-1-2-Comparison-against-baselines——与基准模型的比较">5.1.2 Comparison against baselines——与基准模型的比较</h3><hr><p>不同模型生成图片对比：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112117081.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112117465.png" alt="image.png"><br>本论文方法能够产生与完全监督的pix2pix具有相似质量的转换。<br>排除了图中的像素损失+GAN和特征损失+GAN，因为这两种方法都无法产生接近目标域的结果。</p><p>三个实验：<br>AMT 上进行“真实 vs 假”实验，评估感知逼真度。<br>Cityscapes 数据集上训练照片→标签任务，使用 Cityscapes 基准比较。<br>Cityscapes 数据集上训练标签→照片任务，分割网络对输出的照片进行评估。</p><p>AMT：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112137709.png" alt="image.png"><br>labels→photos：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112139548.png" alt="image.png"><br>photos→labels：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112139707.png" alt="image.png"></p><h3 id="5-1-3-Ablation-Study——消融实验">5.1.3 Ablation Study——消融实验</h3><hr><p>定性示例：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112142644.png" alt="image.png"><br>移除GAN损失会严重降低结果，移除循环一致性损失也是如此。<br>GAN+正向循环损耗 $\mathbb{E}<em>{x \sim p</em>{\text{data}}(x)}[|F(G(x)) - x|<em>1]$ 和 GAN+反向循环损耗 $\mathbb{E}</em>{y \sim p_{\text{data}}(y)}[|G(F(y)) - y|_1]$ 实验，<br>发现经常导致训练不稳定并导致模式崩溃，特别是对于移除的映射方向。</p><h2 id="5-2-Applications——应用">5.2 Applications——应用</h2><hr><p>训练数据上的转换结果通常比测试数据上的更加吸引人。</p><h3 id="Object-transfiguration（对象变换）：">Object transfiguration（对象变换）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112152050.png" alt="image.png"><br>本文方法侧重于两个视觉相似类别之间的对象变形。</p><h3 id="Season-transfer（季节转换）：">Season transfer（季节转换）：</h3><p>根据Flickr上优胜美地的冬夏照片训练。</p><h3 id="Collection-style-transfer-（收藏样式转换）：">Collection style transfer （收藏样式转换）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112154034.png" alt="image.png"><br>本文方法学习模仿一整套艺术品（如梵高）的风格，而不是转移单个选定艺术品（如《星夜》）的风格。</p><h3 id="Photo-generation-from-paintings（绘画生成照片）：">Photo generation from paintings（绘画生成照片）：</h3><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112156270.png" alt="image.png"><br>当将目标域的真实样本作为生成器的输入时，将生成器正则化为接近恒等映射：即<br>$$L_{\text{identity}}(G, F) = \mathbb{E}<em>{y \sim p</em>{\text{data}}(y)}[|G(y) - y|<em>1] + \mathbb{E}</em>{x \sim p_{\text{data}}(x)}[|F(x) - x|<em>1]$$<br>如果没有 $L</em>{identity}$ ，生成器 G 和 F 就可以在没有必要的情况下自由地改变输入图像的色调。因为在对抗性损失和循环一致性损失下，这样的映射可能同样有效。</p><h3 id="Photo-enhancement（照片增强）：">Photo enhancement（照片增强）：</h3><p>成功生成由智能手机拍摄的景深较浅的照片。</p><h1 id="6-Limitations-and-Discussion">6. Limitations and Discussion</h1><hr><p>失败案例：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409112213170.png" alt="image.png"><br>几何变化的任务，学习到的转换退化为对输入进行最小的修改。<br>处理更加多样化和极端的转换，特别是几何变化，是未来工作中的一个重要问题。<br>一些失败案例是由训练数据集的分布特征引起的。<br>使用成对训练数据与无配对方法之间的结果仍有差距。<br>结合弱监督或半监督数据可能会导致更强大的转换器，同时仍然只需付出全监督系统一小部分的标注成本。<br>这篇论文推动了在这种“无监督”环境下的可能性界限。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> CycleGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】Bone metastasis scintigram generation using generative adversarial learning with multi‐receptive field learning and two‐stage training</title>
      <link href="/gan-bmsg/"/>
      <url>/gan-bmsg/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="背景：">背景：</h3><p>需要一种能扩大SPECT骨显像数据集的数据生成方法。</p><h3 id="方法：">方法：</h3><p>一种基于深度学习的SPECT骨显像生成模型。<br>采用生成对抗学习结构，提出骨转移显像生成模型（bone metastasis scintigram generation model，BMS-Gen）。<br>BMS-Gen采用多输入条件和多感受野学习来确保生成样本的真实性。<br>BMS-Gen采用生成对抗学习来保持生成样本的多样性。<br>BMS-Gen采用两阶段训练策略来提高生成样本的质量。</p><h3 id="结果：">结果：</h3><p>在 SPECT 骨转移显像图的临床数据上进行的实验评估显示了BMS-Gen的性能。<br>在FID（Fréchet初始距离）、MSE（均方误差）和PSNR（峰值信噪比）指标上获得了最好的总分。<br>在图像分类和分割任务中，BMS-Gen生成的样本的引入使F-1得分最大（最小）增加了3.01%（0.15%），DSC得分最大（最小）增加了6.83%（2.21%）。</p><h1 id="1-Introduction">1. Introduction</h1><hr><ul><li>$^{99m}\text{Tc-MDP}$ (technetium-99 methylene phosphonic acid)——锝-99亚甲基二膦酸盐</li><li>$\text{SPECT}$ (single photon emission computed tomography)——单光子发射计算机断层扫描</li><li>$\text{PET}$ (Positron Emission Tomography )——正电子发射断层扫描</li><li>$\text{BM}$ (bone metastasis)——骨转移</li><li>$\text{BMS-Gen}$ (bone metastasis scintigram generation model)——骨转移显像生成模型</li></ul><p>使用 $^{99m}\text{Tc-MDP}$ 和 $\text{SPECT}$ 成像设备，可以在显像图中识别和显示 $\text{BM}$ 病变区域，该区域表现为放射性药物高摄取区域，通常称为“热点”。<br>SPECT骨显像的典型特征是空间分辨率低，骨转移病变在位置、大小和形状上不规则和不可预测。</p><p>两类CNN用于SPECT骨显像的自动分析：</p><ol><li>用于自动确定图像中是否存在骨转移，将其分类到相应的类别。</li><li>用于通过分割图像来自动分离BM病灶。<br>都需要大规模的SPECT骨显像样本。</li></ol><p>生成对抗网络（GAN）是深度生成学习中最强大的框架之一，包含一个将潜在噪声空间映射为真实图像的生成器和一个用于区分数据库中样本与生成器生成样本的判别器。</p><p>主要挑战有两点：</p><ol><li>SPECT骨显像较低的空间分辨率对从原始数据中生成细粒度的样本提出了很高的要求；</li><li>骨转移病灶在位置、大小和形状方面的不可预测性和不规则性，使得基于深度学习的模型很难表征低分辨率的骨显像图像。</li></ol><p>BMS-Gen特点：</p><ol><li>采用多输入条件和多感受野学习来确保生成样本的真实性。</li><li>采用生成对抗学习来保持生成样本的多样性。</li><li>采用两阶段训练策略来提高生成样本的质量。</li></ol><p>贡献：</p><ol><li>确定了BM显像自动生成这一研究问题，基于深度学习的医学图像分析领域中的首个相关研究。</li><li>提出了基于GAN的BMS-Gen模型。</li><li>评估并验证了生成样本在提升下游任务性能中的实用性。</li></ol><h1 id="2-Methods">2. Methods</h1><hr><h2 id="2-1-An-overview——概述">2.1 An overview——概述</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409092216234.png" alt="image.png"></p><ul><li>$\text{MRF-G}$ (multi-receptive field generator)——多感受野生成器</li></ul><p>组成：<br>生成网络和判别网络，生成网络使用定制的生成器——MRF-G。</p><p>样本：<br>病灶标签 $x_{GT}$ ，由经验丰富的专家使用开源工具LabelMe获取；<br>二值矩阵 $x_{BM}$ ，将x中的所有非零元素设为1、所有零元素设为0来创建二值矩阵；<br>3D高斯噪声 $x_{GN}$ ；<br>$x_{GT}$ 和 $x_{BM}$ 集成到 $x_{GN}$ 形成多通道输入 $x_{MCI}$ 。集成的 $x_{GT}$ 和 $x_{BM}$ 可视为3D噪声 $x_{GN}$ 的条件信息，代表多种条件。</p><p>初步训练阶段：<br>基于 $x_{MCI}$ 生成初步样本 $y_{Pre}$（即， $x_{MCI} \rightarrow y_{Pre}$）。</p><p>精炼训练阶段：<br>生成器MRF-G利用 $y_{Pre}$ 和 $x_{MCI}$ ，生成最终的精炼样本$y$（即，$x_{MCI} ; y_{Pre} \rightarrow y$）。</p><p>生成阶段：<br>判别网络通过分类判别器（CL-D）衡量生成样本的真实性;<br>通过分割判别器（SEG-D）计算生成像素与真实像素之间的语义相似度。<br>这种两阶段训练策略显著提高生成样本的质量。</p><h2 id="2-2-The-architecture-of-the-BMS-Gen-model——BMS-Gen模型的体系结构">2.2 The architecture of the BMS-Gen model——BMS-Gen模型的体系结构</h2><hr><p>增强生成器和判别器的能力对于开发高性能模型至关重要。</p><h3 id="2-2-1-The-generating-network——生成网络">2.2.1 The generating network——生成网络</h3><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101041252.png" alt="image.png"></p><ul><li>$\text{MRFS}$ (MRF synthesis)——多感受野生成</li><li>$\text{MRFF}$ (MRF fusion)——多感受野融合</li><li>$\text{IN}$ (Instance Normalization)——实例归一化</li></ul><p>MRF-G通过将病灶标签 $x_{GT}$ 输入到不同的层次，强化模型对病灶语义信息的理解，并在输出层使用 $x_{BM}$ 控制生成图像的整体结构。</p><p>MRFS模块是 MRF-G的核心，通过使用MRFF，concat 双路径输入来生成逼真的假样本。</p><p>MRFF模块，四个不同接收场（即卷积核）的并行分支生成了四个特征图，以捕捉大小各异的BM病灶。随后按各自的权重进行加权，通过逐像素加法操作进行融合。</p><p>MRFF模块中，<br>使用两个连续的3 × 3卷积来替代5 × 5卷积，使用三个连续的3 × 3卷积替代7 × 7卷积，减少计算开销。<br>使用逐像素加法而不是通道级联进行特征融合，以保持相关结构并改善信息的估计。<br>使用IN对每个特征图单独进行归一化操作，确保了不同特征不混合，保持输入条件与生成结果之间的映射关系。</p><p>MRFF模块具有通过MRF卷积提升MRF-G学习能力的潜力，能够同时捕捉不同尺度的图像特征，提取局部细节特征和全局结构特征。<br>MRF-G可以通过融合不同接收场的特征，在不同尺度上整合视觉和语义信息。<br>MRFF模块的并行结构也能增加网络的宽度，能够在一定程度上缓解过拟合问题，并防止合成图像与训练数据过于相似，从而保证多样性。</p><h3 id="2-2-2-The-discriminating-network——判别网络">2.2.2 The discriminating network——判别网络</h3><hr><ul><li>$\text{CL-D}$ (classification-based discrimination)——基于分类的判别器</li><li>$\text{SEG-D}$ (segmentation-based discrimination)——基于分割的判别器</li></ul><p>CL-D使用ResNet作为主干网络，对生成的样本进行分类，进行图像级别的分类判别，以确定生成的样本是否是真实的。无法提供细粒度的指导信息，可能导致模式崩溃的陷阱。</p><p>SEG-D使用U-Net作为主干网络，促使生成器专注于图像中的语义对象，能够保持真实图像和生成样本之间感兴趣区域（即病灶）的语义一致性。</p><h3 id="2-2-3-Loss-functions——损失函数">2.2.3 Loss functions——损失函数</h3><hr><ul><li>$\text{BCE}$ (Binary CrossEntropy)——二元交叉熵</li></ul><p>整个网络通过对标签预测损失的反向传播以极小化方式联合训练，生成器通过更新来最小化损失，判别器通过更新来最大化损失。</p><p>BMS-Gen采用了BCE损失来定义其损失函数：<br>$$\ell = -\mathbb{E}_{I,\varphi} \left[ \varphi \log(D(I)) + (1 - \varphi) \log(1 - D(I)) \right] \tag{1}$$<br>D 表示判别器，I 是输入到 D 的图像，φ 表示 I 的类别标签。</p><p>两个判别器的损失函数定义为：<br>$$\ell_{CL-D} = \ell(y, CL-D) + \ell(MRF-G, CL-D) \tag{2}$$<br>$$\ell_{SEG-D} = \ell(y, SEG-D) \tag{3}$$<br>y 为输入到 CL-D 和 SEG-D 的生成样本。</p><p>生成器的损失函数定义为：<br>$$\ell_{MRF-G} = \ell(MRF-G, CL-D) + \ell(MRF-G, SEG-D) + \lambda \cdot L1 \tag{4}$$<br>L1 用于衡量真实图像与生成样本之间的差异，λλ 的系数值设为100，可以有效减少伪影。</p><p>L1 损失：<br>$$L1 = \mathbb{E}<em>{x,x</em>{MCI}} \left[ |x - G(x_{MCI})|_1 \right] \tag{5}$$</p><h1 id="3-Experiments-and-Resaults">3. Experiments and Resaults</h1><hr><h2 id="3-1-Experimental-data——实验数据">3.1 Experimental data——实验数据</h2><hr><p>本研究的平面骨显像数据来自中国甘肃省肿瘤医院核医学科，2016年1月至2019年12月。</p><p>从410幅骨显像图像中挑选了286幅包含胸腔转移病灶的显像图，裁剪后提取出胸腔区域用于实验。<br>图像大小为256 × 256。</p><p>每幅图像的病灶标签（即 xGT​），由三位经验丰富的核医学医生（一名主任医师、一名副高级技师和一名主管技师）使用基于LabelMe系统手动标注转移病灶。</p><h2 id="3-2-Experimental-setup——实验设置">3.2 Experimental setup——实验设置</h2><hr><h3 id="BMS-Gen评价指标：">BMS-Gen评价指标：</h3><ul><li><strong>FID指标</strong>：用于衡量 xx 和 yy 之间的分布距离，得分越小，模型性能越高。</li><li><strong>MSE指标</strong>：计算 xx 和 yy 之间的像素级误差，MSE得分越低，生成质量越好。</li><li><strong>PSNR指标</strong>：衡量 xx 和 yy 之间的图像级相似度，PSNR得分越高，两图像越相似。</li><li><strong>SSIM指标</strong>：用于衡量两图像的结构相似性，取值介于0和1之间，值越高，表明两图像越相似。</li></ul><p><strong>FID（Fréchet Inception Distance）：</strong><br>$$FID(x, y) = |\mu’_x - \mu’_y|^2_2 + \text{Tr}\left(\Sigma_x + \Sigma_y - 2(\Sigma_x \Sigma_y)^{0.5}\right) \tag{6}$$</p><p>μ′ 是特征图的均值，Tr 是矩阵的迹，Σ 是对应特征图的协方差矩阵。</p><p><strong>MSE（均方误差）：</strong><br>$$MSE = \frac{1}{rc} \sum_{i=0}^{r-1} \sum_{j=0}^{c-1} (x(i, j) - y(i, j))^2 \tag{7}$$</p><p>r 和 c 是 x(y) 的行和列的数量。</p><p><strong>PSNR（峰值信噪比）：</strong><br>$$PSNR = 10 \cdot \log_{10} \left(\frac{Max^2_x}{MSE}\right) \tag{8}$$<br>Max 是 x 中元素的最大值。</p><p><strong>SSIM（结构相似性指数）：</strong><br>$$SSIM(x, y) = \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu^2_x + \mu^2_y + C_1)(\sigma^2_x + \sigma^2_y + C_2)} \tag{9}$$<br>μx​ 和 μy 是图像的均值，σxy​ 是协方差，σx​ 和 σy 是标准差，正数 C1 和 C2​ 用于避免分母为零。</p><h3 id="下游任务评价指标：">下游任务评价指标：</h3><p><strong>准确率：</strong><br>$$Accuracy = \frac{TP + TN}{TP + FP + TN + FN} \tag{10}$$</p><p><strong>精确率 / 类别像素准确率 (Precision / CPA)</strong>：<br>$$Precision = CPA = \frac{TP}{TP + FP} \tag{11}$$</p><p><strong>召回率 (Recall)</strong>：<br>$$Recall = \frac{TP}{TP + FN} \tag{12}$$</p><p><strong>F-1 分数 (F-1 score)</strong>：<br>$$F-1 \ score = 2 \times \frac{Precision \times Recall}{Precision + Recall} \tag{13}$$</p><p><strong>Dice 相似系数 (DSC)</strong>：<br>$$DSC = \frac{2TP}{FP + 2TP + FN} \tag{14}$$</p><p>TP = 真阳性（True Positive），FP = 假阳性（False Positive），FN = 假阴性（False Negative），TN = 真阴性（True Negative）。<br>所有训练好的模型都通过5次交叉验证在测试集上测试，以确保结果的可靠性。</p><p>参数设置：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101615355.png" alt="image.png"></p><h2 id="3-3-Experimental-results——实验结果">3.3 Experimental results——实验结果</h2><hr><h3 id="3-3-1-Performance-results——性能结果">3.3.1 Performance results——性能结果</h3><hr><p>BMS-Gen模型和其他三个基于GAN的经典生成模型的实验结果：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101618893.png" alt="image.png"></p><p>FID评分的显著降低和SSIM的显著增加表明BMS-Gen可以生成与真实图像相似的样本。<br>BMS-Gen的峰值信噪比比Pix2PixGAN略有下降，归因于SPECT骨显像的性质，其中SPECT BM显像中的元素通常具有比自然图像中0−255的像素值更宽的值范围。</p><h3 id="3-3-2-Performance-analysis-of-downstream-tasks——下游任务性能分析">3.3.2 Performance analysis of downstream tasks——下游任务性能分析</h3><hr><h4 id="Semantic-image-segmentation（图像语义分割）：">Semantic image segmentation（图像语义分割）：</h4><p>不同的数据扩充方法创建不同的数据集：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101635617.png" alt="image.png"><br>采用一对一的生成策略，分别从原始的286个SPECT骨闪烁图中生成286个样本。<br>随机抽取86个作为验证集（VAL），其余200个样本，结合原始200个样本，形成训练集（TRN）。</p><p>U-Net模型分割BM病变的实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101635582.png" alt="image.png"></p><p>BMS-Gen生成的样本对通过数据集DSeg-7上的几种分割模型获得的提示性能的影响：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101641088.png" alt="image.png"></p><h4 id="Image-classification（图像分类）：">Image classification（图像分类）：</h4><p>不同的数据扩充方法创建不同的数据集：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101646469.png" alt="image.png"><br>将502张正常SPECT骨扫描图中没有BM（骨转移）的286张图像与之前提到的286张样本结合，创建了实验数据集（n = 572），其余216张样本在使用生成策略时使用。<br>使用一对一的生成策略，BMS-Gen和几何变换分别从原始的286例SPECT骨显像中生成了286个样本。<br>在使用验证数据的情况下，几何变换生成了86个不含BM的样本。</p><ol><li>随机选择372个生成样本中的172个作为验证集（VAL），其余200个样本与原始的616个样本组合为训练集（TRN）。</li><li>随机选择286个生成样本中的200个样本，与原始的616个样本组合为训练集。</li></ol><p>两种情况都随机从原始样本中选择172个作为测试集（TST）。<br>对上述数据集进行实验。</p><p>ResNet-18模型在分类骨显像时的实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101649831.png" alt="image.png"></p><p>ResNet-18模型对不同生成模型扩充的数据集的分类性能：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101650409.png" alt="image.png"></p><p>BMS-Gen生成的样本对数据集DCL-7上几种分类模型获得的提示性能的影响：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101650860.png" alt="image.png"></p><h1 id="4-Discussion-and-Ablation-Study">4. Discussion and Ablation Study</h1><hr><h2 id="4-1-Effects-of-multiply-input-conditions-and-model-network-structure——多输入条件与模型网络结构的影响">4.1 Effects of multiply input conditions and model network structure——多输入条件与模型网络结构的影响</h2><hr><p>BMS-Gen模型消融研究的实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101654272.png" alt="image.png"><br>多输入条件和MRFF模块的引入，可以促进BMS-Gen模型聚焦于大小和位置具有高度随机性的病灶，生成逼真但不完全相同的样本。</p><p>两个案例进一步验证上述解释：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101656705.png" alt="image.png"><br>病灶区域在原始样本和生成样本之间的位置和形状保持高度一致。<br>病灶区域在原始样本和生成样本之间的强度和分布存在明显差异，显示了生成样本的多样性。</p><p>两个判别器对BMS-Gen总体性能的影响：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101659571.png" alt="image.png"><br>分割判别器被引入，帮助模型聚焦于骨显像中的关注区域（即BM病灶）。<br>SEG-D提供的额外语义信息可以有效地指导样本生成，并在生成任务中发挥积极作用。</p><p>SEG-D引入其他模型刻画病灶区域的能力：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101701802.png" alt="image.png"></p><h2 id="4-2-Effects-of-two-stage-training——两阶段训练的效果">4.2 Effects of two-stage training——两阶段训练的效果</h2><hr><p>两阶段训练策略对BMS-Gen整体性能的影响：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101705038.png" alt="image.png"></p><p>两个案例展示两阶段训练策略效果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101707675.png" alt="image.png"><br>两阶段训练使得模型能够更加关注骨骼细节，从而能够精细地刻画BM病灶。</p><p>在全身骨扫描上，BMS-Gen等经典生成模型在定义的评价指标上的实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101708898.png" alt="image.png"></p><p>两个案例展示全身骨扫描图像生成效果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409101713399.png" alt="image.png"></p><h2 id="4-3-Limitations-of-the-proposed-method-on-downstream-tasks——所提方法对下游任务的局限性">4.3 Limitations of the proposed method on downstream tasks——所提方法对下游任务的局限性</h2><hr><p>召回率指标下降，通常是由于下游任务模型自身的限制。</p><h3 id="Reasons-for-lower-Recall-scores-on-segmentation-task（分割任务召回率较低的原因）：">Reasons for lower Recall scores on segmentation task（分割任务召回率较低的原因）：</h3><p>在分割任务中，前景（病变区域）通常比背景（正常组织）要小得多。<br>召回率可能会高估分割性能，因为它没有全面考虑所有方面。</p><h3 id="Reasons-for-lower-CPA-scores-on-segmentation-task（分割任务CPA分数较低的原因）：">Reasons for lower CPA scores on segmentation task（分割任务CPA分数较低的原因）：</h3><p>CPA下降与分割模型自身的特性有关。</p><h3 id="Reasons-for-lower-Recall-scores-on-classification-task（分类任务召回率较低的原因）：">Reasons for lower Recall scores on classification task（分类任务召回率较低的原因）：</h3><p>召回率下降与分类模型自身的特性有关。</p><h1 id="5-Conclusion">5. Conclusion</h1><hr><p>结果表明，我们的模型优于其他比较模型。</p><p>展望：<br>收集更多SPECT骨显像图，以进一步测试该模型的性能。<br>尝试将模型应用于其他医学图像模式（如PET）的数据增强任务。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> BoneMetastasis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】Image-to-Image Translation with Conditional Adversarial Networks</title>
      <link href="/gan-itit/"/>
      <url>/gan-itit/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="本文贡献：">本文贡献：</h3><ul><li>研究条件对抗网络作为一个通用的解决图像到图像的翻译问题的方案。</li><li>不仅学习从输入图像到输出图像的映射，而且学习一个损失函数来训练这种映射。</li></ul><h3 id="结果：">结果：</h3><ul><li>使得对传统上需要不同的损失公式的问题采用相同的普遍方法成为可能。</li></ul><h1 id="1-Introduction">1. Introduction</h1><hr><ul><li>将自动的图像到图像翻译定义为，在给定足够训练数据的情况下，将场景的一种可能表示转换为另一种可能表示的问题。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072308137.png" alt="image.png"></p><ul><li>predict pixels from pixels</li><li>CNN 学习最小化损失函数，即对结果质量进行评分的目标——尽管学习过程是自动的，但仍需要大量手动工作来设计有效的损失。迫使 CNN 做我们真正想要的损失函数——例如，输出清晰、逼真的图像。</li><li>GANs学习一个损失，试图分类输出图像是真是假，同时训练生成模型，以尽量减少这种损失。由于GANs学习的是与数据相适应的损失，因此它们可以应用于传统上需要不同类型的损失函数的大量任务。</li><li>cGANs学习条件生成模型。这使得cGAN适合于图像到图像的转换任务，其中对输入图像进行条件处理并生成相应的输出图像。</li></ul><h1 id="2-Related-work">2. Related work</h1><hr><h3 id="Structured-losses-for-image-modeling——图像建模中的结构损失：">Structured losses for image modeling——图像建模中的结构损失：</h3><ul><li>传统Image-to-image translation方法将输出空间视为“非结构化”的，即每个输出像素被视为有条件地独立于所有其他像素。</li><li>cGANs是结构化损失。结构化损失对输出的联合配置进行惩罚。</li><li>大量文献考虑了此类损失，方法包括条件随机场、SSIM度量、特征匹配、非参数损失、卷积伪先验和基于匹配协方差统计的损失。</li><li>cGANs的不同之处在于，损失是学习的，理论上，GAN可以惩罚输出和目标之间任何可能存在差异的结构。</li></ul><h3 id="Conditional-GANs——条件GANs：">Conditional GANs——条件GANs：</h3><ul><li>本文框架的不同之处在于没有特定于应用程序的内容。这使得设置比大多数其他设置简单得多。</li><li>本文的生成器使用了基于“U-Net”的架构 ，判别器则使用了卷积“PatchGAN”分类器，它只对图像块级别的结构进行惩罚。</li><li>类似的PatchGAN架构曾被提出，旨在捕捉局部风格统计信息。</li><li>本文展示了这种方法在更广泛的问题上是有效的，并探讨了改变块大小的影响。</li></ul><h1 id="3-Method">3. Method</h1><hr><ul><li>GANs 学习从随机噪声向量z到输出图像y的映射，表示为 $G: z \rightarrow y$ 。</li><li>cGANs 学习从观测到的图像x和随机噪声向量z到图像y的映射，表示为 $G: {x, z} \rightarrow y$ 。</li><li>生成器G被训练生成的输出不能被对抗性训练的判别器D区分为“假”图像，而判别器D则被训练尽可能准确地检测出生成器的“伪造”图像。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409072345976.png" alt="image.png"></p><h2 id="3-1-Objective——目标">3.1 Objective——目标</h2><hr><p>cGANs的目标函数：<br>$$ L_{cGAN}(G, D) = \mathbb{E}<em>{x,y}[\log D(x, y)] + \mathbb{E}</em>{x,z}[\log(1 - D(x, G(x, z)))] $$</p><ul><li>判别器 <strong>D(x,y)</strong>：输出的值表示它认为这个图像对是否真实的概率。</li><li>生成器 <strong>G(x,y)</strong>：试图让 D(x,G(x,z)) 尽可能接近1。</li><li>判别器 D 的目标是最大化该损失函数，它希望尽量分清楚真实图像 y 和生成的伪造图像 G(x,z)。</li><li>生成器 G 的目标是最小化该损失函数，它希望生成的图像尽可能逼真，欺骗判别器。</li><li><strong>z</strong> 表示<strong>随机噪声向量</strong>，它是从某个预定义的概率分布（如高斯分布或均匀分布）中采样得到的。</li><li><strong>D(x, y) 值大</strong>：判别器认为输入的图像是真实的。</li><li><strong>D(x, G(x, z)) 值大</strong>：生成器生成的图像质量高，足以欺骗判别器。</li><li><strong>D(x, G(x, z)) 值小</strong>：生成器的图像质量差，判别器成功识别出它是伪造的。</li><li><strong>$L_{cGAN}(G, D)$越大</strong>： 判别器 D 在识别生成器 G 的假图像方面做得更好；生成器 G 还需要进一步改进，以生成更逼真的图像。</li><li><strong>$L_{cGAN}(G, D)$越小</strong>：生成器 G 的图像质量足够好，以至于判别器 D 很难区分真实图像和生成图像。</li><li>$\mathbb{E}$ 代表 <strong>期望值（Expectation）</strong>，即随机变量取不同值的加权平均。</li></ul><p>其中，生成器 G 尝试最小化这个目标函数，而判别器 D 尝试最大化它，即：<br>$$G^* = \arg\min_G \max_D L_{cGAN}(G, D)$$</p><ul><li>$\arg$ 表示取得某个函数最优值（最小值或最大值）时的输入参数。</li></ul><p>无条件GANs：<br>$$L_{GAN}(G, D) = \mathbb{E}<em>{y}[\log D(y)] + \mathbb{E}</em>{x,z}[\log(1 - D(G(x, z)))]$$</p><p>将GAN的目标与更传统的损失（如L2距离）结合起来是有益的。生成器不仅需要欺骗判别器，还需要在L2意义上接近真实输出：<br>$$L_{L1}(G) = \mathbb{E}<em>{x,y,z}[|y - G(x, z)|<em>1]$$<br>最终目标：<br>$$G^* = \arg \min_G \max_D \left( L</em>{cGAN}(G, D) + \lambda L</em>{L1}(G) \right)$$</p><p>没有随机变量 $z$ 会生成确定性输出，但生成器很容易学会忽略噪声。<br>本文模型中，噪声仅通过 dropout 的形式引入，应用于生成器的多个层级，且在训练和测试时都使用。<br>尽管存在 dropout 噪声，但网络输出的随机性较小。</p><h2 id="3-2-Network-architectures——网络体系结构">3.2 Network architectures——网络体系结构</h2><hr><p>生成器和判别器都使用卷积BatchNorm ReLu形式的模块。</p><h3 id="3-2-1-Generator-with-skips——带有跳跃连接的生成器">3.2.1 Generator with skips——带有跳跃连接的生成器</h3><hr><ul><li>借鉴 U-Net</li></ul><p>许多图像转换问题，输入和输出之间共享大量低级信息，因此直接传递这些信息会更为理想。<br>例如，在图像上色的情况下，输入和输出共享显著边缘的位置。</p><p>本文添加了跳跃连接，在每一层 i 和第 n - i 层之间添加了跳跃连接，其中 n 是总层数。<br>每个跳跃连接简单地将层 i 的所有通道与层 n - i 的通道进行级联。</p><h3 id="3-2-2-Markovian-discriminator-PatchGAN-——马尔可夫鉴别器（PatchGAN）">3.2.2 Markovian discriminator (PatchGAN)——马尔可夫鉴别器（PatchGAN）</h3><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409082343453.png" alt="image.png"></p><p>L2 损失以及 L1 损失（见图 3），能准确捕捉低频信息，无法捕捉高频信息。<br>所以，对于低频的正确性，L1 损失已经足够了。<br>这促使本文将 GAN 判别器限制为仅建模高频结构，并依赖 L1 项来强制低频的正确性（公式 5）。</p><p>对于高频信息，限制关注局部图像块中的结构是足够的。<br>本文设计了一种判别器架构——PatchGAN，它仅对图像块级别的结构进行惩罚。<br>这个判别器试图判别图像中的每个 N×N 块是否真实。在图像上以卷积方式运行这个判别器，对所有响应进行平均以提供最终的 D 输出。<br>N 可以比图像的全尺寸小得多，仍能产生高质量的结果。较小的 PatchGAN 参数更少，运行速度更快。</p><p>判别器有效地将图像建模为马尔可夫随机场，将图像的复杂性简化为局部结构的依赖。</p><blockquote><p><strong>马尔可夫性</strong>：图像中的像素只与其邻近像素直接相关，而与较远的像素是条件独立的。即一个像素的状态（如颜色或强度）只依赖于其局部邻域（Patch）中的像素，而不直接依赖于远处的像素。</p></blockquote><blockquote><p>局部依赖性和条件独立性</p></blockquote><h2 id="3-3-Optimization-and-inference——优化与推理">3.3 Optimization and inference——优化与推理</h2><hr><h3 id="优化：">优化：</h3><p>在判别器 D 上进行一次梯度下降步骤，然后在生成器 G 上进行一次该步骤。<br>用小批量的随机梯度下降（SGD）并应用 Adam 优化器 。</p><h3 id="推理：">推理：</h3><p>以与训练阶段完全相同的方式运行生成器网络。<br>在测试时应用了 dropout，并使用测试批次的统计数据进行批量归一化，而不是使用训练批次的汇总统计数据。<br>当批量大小设置为1时，这种批量归一化方法被称为“实例归一化”。<br>本文实验使用了1到10之间的批量大小。</p><h1 id="4-Experiments">4. Experiments</h1><hr><p>在所有情况下，输入和输出仅为1-3通道图像。</p><h2 id="4-1-Evaluation-metrics——评估指标">4.1 Evaluation metrics——评估指标</h2><hr><p>首先，本文在 Amazon Mechanical Turk (AMT) 上进行“真实 vs 假”感知研究。该方法更符合人类观察者的合理性。<br>其次，衡量合成的城市景观是否足够逼真，使得现成的识别系统能够识别其中的对象。</p><h3 id="AMT-perceptual-studies——AMT知觉研究">AMT perceptual studies——AMT知觉研究</h3><hr><p>Turkers 会被呈现一系列对比试验，展示由我们的算法生成的“假”图像与“真实”图像。<br>在每次试验中，每张图像显示1秒，之后图像消失，Turkers 会有无限时间来选择哪张是假的。每个会话的前10张图像是练习，Turkers 会收到反馈。在正式实验的40次试验中不再提供反馈。每次会话只测试一种算法，且 Turkers 不允许完成多个会话。<br>所有图像都以 256 × 256 分辨率展示。<br>没有设置警觉性试验。</p><h3 id="FCN-score——FCN评分">FCN-score——FCN评分</h3><hr><p>采用 FCN-8s 语义分割架构，并在 cityscapes 数据集上对其进行训练。<br>通过生成照片的分类准确率，来对这些照片根据其来源标签进行评分。</p><h2 id="4-2-Analysis-of-the-objective-function——目标函数分析">4.2 Analysis of the objective function——目标函数分析</h2><hr><ol><li>比较 L1 项和 GAN 项的影响；</li><li>比较基于输入条件的判别器与非条件判别器的影响。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409082343453.png" alt="image.png"></p><p>单独使用 L1 会得到合理但模糊的结果。<br>单独使用 cGAN（设置λ=0）会得到更清晰的结果，但在某些应用中会引入视觉伪影。<br>将这两个项相加（λ=100）可减少这些伪影。</p><p>在 cityscapes 标签→照片任务上使用 FCN 评分量化观察结果：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091106240.png" alt="image.png"></p><p>基于 GAN 的目标取得了更高的分数，表明合成的图像包含了更多可识别的结构。<br>测试去除判别器条件的效果（标记为 GAN）。在这种情况下，损失loss不会惩罚输入和输出之间的不匹配；它只关心输出是否看起来真实。这种变体表现非常差，结果显示，生成器在输入照片不同的情况下几乎生成相同的输出。<br>显然，损失衡量输入和输出之间的匹配质量非常重要，确实，cGAN的表现比GAN好得多。<br>添加 L1 项也鼓励输出符合输入，因为 L1 损失会惩罚 ground truth 输出（正确匹配输入）与 synthesis 输出（可能不匹配）的距离。因此，L1+GAN 在创建符合输入标签图的真实渲染方面也很有效。<br>结合所有项，L1+cGAN 的表现也同样优秀。</p><h3 id="Colorfulness——色彩度：">Colorfulness——色彩度：</h3><p>cGANs 的一个显著效果是，它们能生成清晰的图像，即使在输入标签图中不存在空间结构时也会进行“幻觉”生成。<br>人们可能会认为 cGANs 在光谱维度上也有类似的“锐化”效果——使图像更具色彩感。<br>对抗损失原则上可以意识到灰色输出不现实，并鼓励匹配真实的颜色分布。</p><p>L1 损失在不确定边缘具体位置时会鼓励模糊，当不确定像素应采用哪个可能的颜色值时，它也会鼓励选择一种平均的、灰色的颜色。L1 损失会通过选择可能颜色的条件概率密度函数的中位数来最小化。</p><p>图表显示了 Lab 色彩空间中输出颜色值的边际分布：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091641593.png" alt="image"><br>Ground truth 分布用虚线表示。<br>L1 损失导致的分布比真实分布更窄，确认了 L1 鼓励平均灰色颜色的假设。<br>cGANs 能将输出分布推向更接近真实分布。</p><h2 id="4-3-Analysis-of-the-generator-architecture——生成器结构分析">4.3 Analysis of the generator architecture——生成器结构分析</h2><hr><p>图 4 比较了 U-Net 和编码器-解码器在城市景观生成中的表现：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091121513.png" alt="image.png"></p><h2 id="4-4-From-PixelGANs-to-PatchGans-to-ImageGANs——从像素到PatchGans再到ImageGANs">4.4 From PixelGANs to PatchGans to ImageGANs——从像素到PatchGans再到ImageGANs</h2><hr><p>判别器接收场 patch 大小 N 的变化效果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091550758.png" alt="image.png"></p><p>FCN 评分量化了这些效果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091108553.png" alt="image.png"></p><p>本文中所有实验均使用 70 × 70 的 PatchGAN，而在本节中所有实验都使用了 L1+cGAN 损失。<br>PixelGAN 对空间清晰度没有影响，但确实增加了结果的色彩丰富性。<br>颜色直方图匹配是图像处理中的一个常见问题，PixelGAN 可能是一个有前景的轻量级解决方案。</p><h3 id="Fully-convolutional-translation——全卷积平移：">Fully-convolutional translation——全卷积平移：</h3><p>卷积式地应用生成器，然后在比训练时更大的图像上进行测试。<br>在 256×256 图像上训练生成器后，在 512 × 512 图像上进行了测试：</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091601190.png" alt="image.png"></p><h2 id="4-5-Perceptual-validation——感知验证">4.5 Perceptual validation——感知验证</h2><hr><p>地图↔照片任务上的 AMT 实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091610772.png" alt="image.png"></p><p>地图↔照片任务本模型显著高于 L1 基线。<br>照片→地图任务偏低的原因可能是在地图中，轻微的结构错误由于其严格的几何形状更加明显，而在更加混乱的航拍照片中则不太显眼。</p><p>彩色化任务实验结果：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091614552.png" alt="image.png"></p><p>cGANs 的得分与 L2 变体相似，但未能达到完整方法的水平，后者是专门为彩色化任务设计的。</p><h2 id="4-6-Semantic-segmentation——语义分割">4.6 Semantic segmentation——语义分割</h2><hr><p>在 cityscape 数据集上训练了一个 cGANs（有/无 L1 损失），用于从照片生成标签：<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091619370.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091620265.png" alt="image.png"></p><p>未使用 L1 损失训练的 cGANs 在一定程度上能够解决该问题。<br>这是第一次展示 GAN 成功生成“标签”而非“图像”，前者几乎是离散的，而后者具有连续值的变化。</p><h2 id="4-7-Community-driven-Research——社区推动研究">4.7 Community-driven Research——社区推动研究</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409091627825.png" alt="image.png"></p><h1 id="5-Conclusion">5. Conclusion</h1><hr><p>cGANs 对于许多图像到图像转换任务来说是一个很有前景的方法，尤其是涉及高度结构化图形输出的任务。<br>这些网络学习了一种适应特定任务和数据的损失函数，这使得它们可以应用于多种场景。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】深度学习图像数据增广方法研究综述</title>
      <link href="/image-synthesis-review-02/"/>
      <url>/image-synthesis-review-02/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><hr><h3 id="背景：">背景：</h3><p>充足的训练数据不仅可以缓解模型在训练时的过拟合问题，而且可以进一步扩大参数搜索空间，帮助模型进一步朝着全局最优解优化。<br>然而，在许多领域或任务中，获取到充足训练样本的难度和代价非常高。因此，数据增广成为一种常用的增加训练样本的手段。</p><h3 id="本文贡献：">本文贡献：</h3><p>按照方法本质原理的不同，将其分为单数据变形、多数据混合、学习数据分布和学习增广策略等 4 类方法。<br>单数据变形方法主要可以分为几何变换、色域变换、清晰度变换、噪声注入和局部擦除等 5 种；<br>多数据混合可按照图像维度的混合和特征空间下的混合进行划分；<br>学习数据分布的方法主要基于生成对抗网络和图像风格迁移的应用进行划分；<br>学习增广策略的典型方法可以按照基于元学习和基于强化学习进行分类</p><h3 id="前景：">前景：</h3><p>根据数据和任务<br>基于强化学习探索最优的组合策略，<br>基于元学习自适应地学习最优数据变形和混合方式，<br>基于生成对抗网络进一步拟合真实数据分布以采样高质量的未知数据，<br>基于风格迁移探索多模态数据互相转换的应用</p><h1 id="0-引言">0. 引言</h1><hr><p>在许多研究领域，受限于数据获取难度大、标注成本高等原因，往往难以获得充足的训练数据，这样训练得到的深度学习模型往往存在过拟合的问题，进而导致模型泛化能力差、测试精度不高等，难以满足应用需求。</p><p><strong>数据增广</strong>，又称<strong>数据增强</strong>(data augmentation)，是一种增加有限数据的数量和多样性的策略，旨在从有限的数据中提炼出更多有用的信息，产生等价于更多数据的价值。数据增广方法试图从过拟合问题的根源———训练样本不足，去解决该问题。</p><p>数据增广可以分为<strong>数据变形</strong> (data warping) 和 <strong>数据过采样</strong> (oversampling)两种方法。</p><p><strong>数据变形类</strong>：LeNet-5、AlexNet、VGGNet、GoogleNet、ResNet、DenseNet中都有用到。</p><p><strong>多幅图像信息混合</strong>：SamplePairing、mixup、SMOTE等，这类方法本质上属于<strong>数据过采样</strong>。</p><p><strong>GAN</strong>：Frid-Adar 等</p><p><strong>元学习和强化学习的思想</strong>：训练一个模型去自适应地选用最优的数据增广策略，来实现模型性能提升的最大化。AutoAugment 和 RandAugment</p><p>本文从另外的角度，即从数据增广的生成方式综述，将数据扩增方法分为单数据变形、多数据混合、学习数据分布规律生成新数据和学习增广策略等 4 类方法。</p><h1 id="1-单数据变形">1. 单数据变形</h1><hr><h2 id="1-1-几何变换">1.1 几何变换</h2><hr><ul><li>几何变换 (geometric transformations)是最常见的图像数据增广方法，通过旋转、镜像、平移、裁剪、缩放和扭曲等变换方式生成新样本。</li><li>在实际任务中，需要根据数据的特点选择合适的几何变换方法才能进一步带来模型性能的提升，否则可能适得其反。</li><li>虽然几何变换的方式简单易操作，但也存在对数据重复记忆、增加的信息量有限等缺点，这也导致几何变换在实际应用中为模型带来的精度提升十分有限。</li></ul><h2 id="1-2-色域变换">1.2 色域变换</h2><hr><ul><li>色域变换(color space transformations)是一种在图像各通道上进行亮度变换的新样本生成方式。</li><li>基于色域变换的数据增广本质上是通过对数据集增加各种各样的光照亮度偏差，增强模型在不同光照条件下的鲁棒性。</li><li>对于图像分类任务，空间几何信息相比色彩信息更加重要。色域变换与几何变换存在着同样的缺点，同时还可能丢失一些重要的颜色信息，进而改变图像原有的语义信息，这也使得该方式的应用存在较大的局限性。</li></ul><p>示例：</p><ul><li>颜色抖动（color jittering），通过几种颜色组合模拟出大范围内多色彩模式的图像增广方式</li><li>PCA抖动（fancy PCA），对原图像进行主成分分析(PCA)，求得协方差矩阵，然后对主成分的特征值施加一个均值为 0 的随机扰动，然后再反变换回去。</li><li>高斯抖动，本质上通过给协方差矩阵增加噪音实现一种图像在视觉表现上的滤镜效果。</li><li>实际应用中，甚至可以使用图像编辑软件进行颜色变换。</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409061017167.png" alt="image.png"></p><h2 id="1-3-清晰度变换">1.3 清晰度变换</h2><hr><ul><li>清晰度变换是一种改变图像视觉清晰度的新样本生成方式。</li><li>“核滤波器(kernel filters)”，核滤波器通过滑动的 n × m 矩阵对图像进行卷积操作，对图像进行锐化和模糊处理，实现图像的清晰度变换。</li><li>采用这种滤波方式对数据集进行增强，不如将其作为网络的一层，还可以训练获得最优的滤波操作。</li></ul><p>示例：</p><ul><li>高斯模糊滤波器(Gaussian blur filter)</li><li>边缘滤波器(edge filter)</li><li>PatchShuffle 正则化</li></ul><h2 id="1-4-噪声注入">1.4 噪声注入</h2><hr><ul><li>噪声注入(noise injection)是一种在图像上叠加噪声的新样本生成方式，噪声可表示为一个服从某分布的随机矩阵。</li><li>通过人为地为图像施加噪声干扰，可为数据集引入冗余和干扰信息，模拟不同成像质量的图像，增强模型对噪声干扰和冗余信息的过滤能力，提高模型对不同质量图像的识别能力。</li><li>常见的噪声种类有高斯噪声、瑞利噪声、伽马噪声、均匀噪声和椒盐噪声等</li><li>在图像上增加噪声可以帮助 CNNs 学到更加鲁棒的特征</li><li>但对于更加复杂的数据集以及多分类问题，模型训练本质是在欠拟合的情况下，噪声注入的图像扩增方式并不能带来新的有效信息，因此不能为模型带来提升效果</li><li>对抗训练：为防御对抗攻击，采用对抗样本进行训练，可以视做一种数据增广方法，用以弥补模型自身的弱点</li></ul><p>示例：</p><ul><li>前向噪声调整方案(forward noise adjustment scheme)</li><li>30 类遥感图像场景数据集</li><li>噪声叠加在图像上，产生“对抗样本(adversarial examples)”</li><li>DisturbLabel</li></ul><h2 id="1-5-局部擦除">1.5 局部擦除</h2><hr><ul><li>不同于噪声是对图像离散的像素值信息的干扰，局部擦除则是图像局部区域所有像素值信息的丢失。</li><li>迫使模型去学习图像中更宽广的具有描述性质的特征，从而防止模型过拟合于特定的视觉特征。</li><li>根据数据和任务的不同，这种方法有时需要人为干预以保证其有效性。</li></ul><p>示例：</p><ul><li>随即擦除（random erasing），可以视为一种在数据空间的dropout</li><li>Cutout 正则化 、Hide-and-Seek 、GridMask</li><li>不规则区域的局部擦除</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062140038.png" alt="image.png"></p><h1 id="2-多数据混合">2. 多数据混合</h1><hr><p>多数据混合的方式希望将多幅图像的信息进行混合以产生新的训练数据，可以从图像空间或特征空间进行信息混合。</p><h2 id="2-1-图像空间的数据混合">2.1 图像空间的数据混合</h2><hr><ul><li>在图像空间进行数据混合的数据增广方法，可以分为对多幅图像的线性叠加和非线性混合，是一类与人类直觉不一致的数据增广方式</li><li>虽然这类混合图像的方法看似不合常理，缺乏可解释性，但是对于提升模型的分类精度却十分有效，可以取得非常具有竞争力的结果。</li></ul><p>示例：</p><ul><li>基于线性混合图像：SamplePairing、mixup和Between-Class Learning</li><li>SamplePairing 对两幅图像求平均值的方式可以看做是在两个数据的中点进行插值，mixup 可以看做是拓展到线性插值得到新样本的版本</li></ul><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062239130.png" alt="image.png"></p><ul><li>mixup 数据增广方法不仅可以提高深度神经网络模型的泛化能力，而且可以有效减少模型对错误标签的记忆，增加模型对于对抗样本的鲁棒性，甚至可以稳定生成对抗网络的训练。</li><li>“CNN 中的输入数据可以被视为波形”，“波形混合”的角度解释了图像线性叠加数据增广方法的原理，类间学习方法(between-class learning，BC)应用到图像上，随机的比例混合两幅图像，这类线性叠加图像的方法相当于一个正则项，希望模型尽可能向线性函数去拟合，以防止强非线性导致的过拟合问题<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062314809.png" alt="image.png"></li><li>非线性图像混合<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062316532.png" alt="image.png"></li><li>多图随机裁剪拼接混合<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062317706.png" alt="image.png"></li></ul><h2 id="2-2-特征空间数据混合">2.2 特征空间数据混合</h2><hr><ul><li>借助 CNN 提取的图像特征，在特征空间进行数据增广。</li><li>针对图像数据，在特征空间进行数据混合的方法很少被采用。</li></ul><p>示例：</p><ul><li>SMOTE 方法，一种在特征空间上进行插值生成新样本的方法</li><li>在特征空间外插值</li><li>在数据空间进行图像变换的效果要优于特征空间变换</li></ul><h1 id="3-学习数据分布">3. 学习数据分布</h1><hr><p>机器学习中的生成式方法，可以通过训练，学习数据集的潜在概率分布，在数据分布中进行过采样生成新数据，由于将整个数据集作为先验知识，这种数据增广方法在理论上是一种更加优秀的方法。</p><h2 id="3-1-生成对抗网络">3.1 生成对抗网络</h2><hr><ul><li>GAN 的核心思想源自博弈论的二人零和博弈(zero-sum game)</li><li>在 GAN 中，博弈的双方是生成器 G 和判别器 D，优化过程是一个极小极大博弈(min-max game)问题，使生成器和判别器在不断优化中各自提高自己的生成能力和判别能力。</li><li>生成器的目标是学习真实数据的潜在分布，并生成新的数据样本，使其看起来和真的一样，达到欺骗判别器的目的</li><li>判别器是一个二分类器，其目标是找到生成出的样本和真实数据分布之间的差异，判别输入的是真实数据还是生成的样本，并且计算并输出一个样本是否来自于真实数据分布的概率值或者标量</li><li>GAN 生成的样本用于数据增广的有效性，并且相比图像变换这类经典的数据增广方法，可以取得更好的效果</li><li>需要较为大量的数据来训练 GAN 模型，样本并不是真实世界存在的，不能将生成的样本当做真实的样本来对待</li></ul><p>示例：</p><ul><li>PG-GANs</li><li>BigGANs</li><li>DCGAN</li><li>conditional GAN</li><li>SiftingGAN<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062337018.png" alt="image.png"></li><li>真假样本混合加权训练的数据增广方法</li></ul><h2 id="3-2-图像风格迁移">3.2 图像风格迁移</h2><hr><ul><li>风格迁移，或称为“图到图翻译(image-to-image translation)”，可以视为一种广义上的图像变换，是一类针对图像的领域迁移(domain transfer)问题。</li><li>本质上是建立一种不同数据分布之间的相互映射。</li></ul><p>示例：</p><ul><li>基于 conditional GAN 提出 pix2pix 方法</li><li>循环一致性生成对抗网络(cycle-consistent adversarial networks，CycleGAN)）<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062342910.png" alt="image.png"></li><li>人体器官MR(magnetic resonance)影像和CT影像之间的转换</li><li>同一遥感场景下 SAR (synthetic aperture rader)和红外影像与可见光影像之间的转换</li><li>神经风格迁移（neural style transfer），类似于颜色空间的光照变换</li></ul><h1 id="4-学习增广策略">4. 学习增广策略</h1><hr><p>借助元学习(meta-learning)和强化学习(reinforcement learning)训练一个模型去自适应地选用最优的数据增广策略，来实现模型性能提升的最大化。</p><h2 id="4-1-基于元学习的策略">4.1 基于元学习的策略</h2><hr><ul><li>元学习的基本思想是希望模型像人一样学会“如何学习”，即基于过去学习的知识和经验总结学习方法，进而可以快速学习新知识、适应新任务和新环境。</li><li>元学习最直接的一种理解为“用神经网络去优化神经网络”，而在数据增广方面，可以用神经网络去替代确定的数据增广方法，训练模型去学习更好的增广策略。</li></ul><p>示例：</p><ul><li>输入随机选取的两幅同一类的图像，希望通过神经网络学习两幅图像共同的内容信息或者风格信息，进而得到一幅“增强图像”，再与原始图像一同输入到分类网络中进行分类模型的训练</li></ul><h2 id="4-2-基于强化学习的策略">4.2 基于强化学习的策略</h2><hr><ul><li>从给定的图像变换和混合方法中，搜索最优的组合策略</li><li>如何对给定任务定制一组图像变换策略，以进一步提高给定模型的预测性能，仍然是一个悬而未决的问题</li></ul><p>示例：</p><ul><li>离散搜索问题</li><li>AutoAugment，选用了强化学习作为搜索算法，搜索最优策略</li><li>RandAugment，主要思想是随机选择变换并调解变换的强度</li></ul><h1 id="5-方法分析与研究展望">5. 方法分析与研究展望</h1><hr><h2 id="5-1-不同数据增广方法选用分析">5.1 不同数据增广方法选用分析</h2><hr><ul><li>采用不合适的变换方法则可能带来负面的效果。因此，方法的适用性成为使用数据增广时首先需要考虑的问题。</li><li>虽然在选用数据增广方法时需要考虑不同种类、不同领域图像各自的特点，但是都需要具备一个核心原则: 在不改变图像原有语义信息的同时尽可能多地增加变化。</li><li>由于自然图像和遥感图像在内容理解上都经常受到遮挡因素的影响，如自然场景前景对背景的遮挡、遥感场景中云对地物的遮挡，裁剪和局部擦除的方法可以提高模型对遮 挡的鲁棒性，而对于医疗影像其成像方式的不同，不存在遮挡的问题，使用这类数据增广方法的有效性还有待验证。</li><li>虽然在一些研究工作中已经证明使用 GAN 进行数据增广可以更有效地提高模型的精度，但是训练 GAN 模型需要一定数量的样本，对于数据量非常小的任务，不适合采用这类基于学习的方法。<br><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409062359953.png" alt="image.png"></li></ul><h2 id="5-2-未来研究展望">5.2 未来研究展望</h2><hr><ul><li>由于图像的维度很高，同时训练 GAN 的样本也非常有限，许多情况下 GAN 对图像数据的概率分布的拟合效果并不好，导致采样生成的图像质量难以保证，限制了 GAN 作为理论上最佳数据增广方法的发展。</li><li>对于 GAN 风格迁移方面的研究和应用，本质上是建立一种不同数据分布之间的相互映射，对于现实生活中普遍存在的跨场景、跨模态的多领域分布的数据，可以通过构建这种映射来实现数据的互补。</li></ul><h1 id="6-结语">6. 结语</h1><hr><ul><li>数据增广作为从数据层面提高机器学习模型性能的一项重要手段，广泛应用于各个领域，尤其是那些样本获取成本高、标注难度大的领域。</li><li>按照增广数据的生成方式划分为四类：单数据变形、多数据混合、学习数据分布和学习增广策略</li><li>基于学习的方法对于数据增广同样具有广阔的发展前景，主要在于以下几个方面: 根据数据和任务基于强化学习探索最优的组合策略; 基于元学习自适应地学习最优数据变形和混合方式；基于生成对抗网络进一步拟合真实数据分布以采样高质量的未知数据；基于风格迁移探索多模态数据互相转换的应用</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】医学图像数据集扩充方法研究进展</title>
      <link href="/image-synthesis-review-01/"/>
      <url>/image-synthesis-review-01/</url>
      
        <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><hr><h3 id="背景：">背景：</h3><ul><li>计算机辅助诊断（CAD）</li><li>训练样本受成像成本、标记成本和涉及患者隐私等因素的影响，导致训练图像多样性不足且难以获取。</li></ul><h3 id="本文贡献：">本文贡献：</h3><p>对医学图像数据集扩充方法的研究进展进行综述。</p><ol><li>对比分析基于几何变换和基于生成对抗网络的扩充方法；</li><li>介绍基于生成对抗网络扩充方法的改进及其适用场景；</li><li>讨论医学图像数据集扩充领域的一些亟待解决的问题并对其未来发展趋势进行展望。</li></ol><h1 id="0-引言">0. 引言</h1><hr><p>医学图像成像模态：</p><ul><li>磁共振成像（magnetic resonance imaging，MRI）</li><li>计算机断层扫描成像（computed tomography，CT）</li><li>正电子发射断层扫描成像（positron emission computed tomography，PET）</li></ul><p>诊断难点：医学图像信息量庞大以及部分疾病的病变部位细小</p><p>医学图像数据集扩充方法：基于几何变换和基于生成对抗网络（generative adversarial network，GAN）的扩充方法</p><h1 id="1-基于几何变换的医学图像数据集扩充方法">1. 基于几何变换的医学图像数据集扩充方法</h1><hr><p>两种操作方式：</p><ol><li>针对图像中像素点的灰度值进行操作，通过一系列变换函数的映射，改变像素点位置信息，使其纹理细节与原图保持一致；</li><li>通过将图像内容变形重组，使其病变区域、感兴趣区域产生形变进而使该图像拥有更多样化的特征信息。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409051723906.png" alt="image.png"></p><p>该扩充方法对数据集多样性的提升较少。</p><h1 id="2-基于GAN的医学图像数据集扩充方法">2. 基于GAN的医学图像数据集扩充方法</h1><hr><p><strong>GAN</strong> 是一种生成式模型，其目的是构建一个从真实图像到潜在特征分布的显式映射关系，构建过程中不需要额外构造复杂的概率密度函数即可实现该映射关系。<br>GAN的训练过程可以概括为一个零和博弈的过程，其生成器希望生成的图像尽可能真实从而欺骗鉴别器，鉴别器则尽力分辨出真实图像和生成图像。</p><p>基于GAN的医学图像数据集扩充方法可以分为四类，分别为：</p><ol><li>无条件数据集扩充方法；</li><li>条件数据集扩充方法；</li><li>跨模态数据集扩充方法；</li><li>与几何变换方法结合的数据集扩充方法</li></ol><h2 id="2-1-无条件数据集扩充方法">2.1 无条件数据集扩充方法</h2><hr><p><strong>无条件数据集扩充方法</strong>是指在没有任何额外信息的情况下，仅利用高斯噪声或者均匀噪声作为GAN的输入而生成医学图像的一类方法。<br>该方法早期生成的图像存在分辨率低、图像模糊、图像特征单一的问题，但可优化解决，其性能逐步上升。</p><p>案例：</p><ul><li>肝脏病变CT图像</li><li>3D头部MRI图像</li><li>CT肺结节图像</li><li>大脑切片MRI图像</li><li>脑部MRI图像</li></ul><p>问题：<br>图像常出现一些与生理学相违背的现象，针对无条件数据集扩充方法的改进应考虑优化生成图像的结构。</p><h2 id="2-2-条件数据集扩充方法">2.2 条件数据集扩充方法</h2><hr><p><strong>条件数据集扩充方法</strong>是指在生成新的医学图像时，一些先验信息如标签、文字、图片等跟随噪声信息一同输入到GAN的生成器。<br><strong>先验信息</strong>能够起到指导模型生成的作用，在先验信息的约束之下生成的医学图像更符合人体的生理学构造。</p><p>案例：</p><ul><li>新冠感染者的CT肺部图像</li><li>肺结节数据</li><li>高分辨率的彩色视网膜图像</li><li>皮肤图像生成与皮肤病变生成</li></ul><p>优点：<br>在继承了无条件数据集扩充方法优点的同时，能够生成特定类型的图像。</p><p>问题：<br>该方法缩小了GAN的生成器样本空间从而对生成图像的多样性起到了限制作用，若想要获得更多样化的医学图像需要训练多个网络，这增加了额外的资源消耗。</p><h2 id="2-3-跨模态数据集扩充方法">2.3 跨模态数据集扩充方法</h2><hr><p>在跨模态数据集扩充方法中，使用最多的是<strong>有监督的像素到像素GAN</strong>（pixel-to-pixel GAN， Pix2PixGAN）和<strong>无监督的循环GAN</strong>（cycle GAN， CycleGAN）。<br><strong>Pix2PixGAN</strong> 在训练过程中需要成对按像素值对齐的图像，图像的获取成本高昂；<br><strong>CycleGAN</strong> 能够适用于非对齐的医学图像，但其生成效果不如Pix2PixGAN。</p><p>案例：</p><ul><li>未对齐的目标图像被视为噪声并使用附加的配准网络进行训练以自适应地拟合未对齐的噪声分布</li><li>基于迭代的多尺度特征融合GAN，有效降低图像的对齐损失</li><li>联邦域翻译新基准方法，缓解图像域移</li><li>有效地去除图像中的噪声分布，有效地去除图像中的噪声分布</li><li>互信息约束GAN，减少模态迁移过程中医学图像细节的丢失</li><li>边缘感知GAN，整合了边缘信息并优化生成图像的内容及结构纹理信息</li><li>两阶段的模态迁移方法</li><li>以CycleGAN为基准生成二维超声心动图的方法</li></ul><p>优点：<br>该方法在成像清晰度、图像多样性、特征多样性、网络收敛速度等方面都具有明显优势</p><p>问题：<br>该方法需要消耗更多的计算机算力资源以及对训练数据集有更高的要求</p><h2 id="2-4-与几何变换结合的数据集扩充方法">2.4 与几何变换结合的数据集扩充方法</h2><hr><p><strong>GAN 的生成器</strong>对输入图像生成变形场、强度变换、仿射变换等<strong>模拟几何变换方法</strong>的操作，可避免生成图像缺乏特征多样性。</p><p>案例：</p><ul><li>两步的基于GAN方法，生成脑部MRI图像的纹理</li><li>对抗性数据集扩充方法</li><li>任务驱动型数据集扩充方法，三个医学数据集（心脏、前列腺和胰腺）</li><li>新的任务驱动数据集扩充方法，改变使用GAN模型随机增强训练示例却提升有限的现状，心脏MRI图像</li><li>新型联合强化学习方法，使用弱监督的GAN模型充当代理，并在给定样本作实例的情况下输出图像掩码，肌肉骨骼X光片</li><li>正则化对抗学习方法，人工指定生成范围和双层优化预定义操作，2D皮肤癌分类和3D腹部器官分割扩充</li></ul><p>优点：<br>既能产生如基于几何变换扩充方法的真实性又能兼顾GAN生成图像的多样化</p><p>问题：<br>基于GAN的扩充方法不稳定、难训练、缺乏可解释性<br>基于几何变换扩充方法生成的图像数据分布过于一致</p><h1 id="3-总结与展望">3. 总结与展望</h1><hr><p>问题：</p><ol><li>没有提出广泛接受的生成图像质量评价标准，多用：峰值信噪比（peak signal to noise ratio， PSNR）、IS、FID等来衡量图像质量</li><li>2D医学图像无法完整表达人体器官的结构特性</li><li>仍致力于研究单模态医学图像生成，未充分利用医学图像的多模态信息</li><li>GAN生成的图像用于医学图像分析领域可能会带来不可预知的问题</li><li>对其他领域的优秀模型的吸收和借鉴非常有限</li><li>带标注医学图像数据难以获取</li></ol><p>展望：</p><ol><li>建立广泛接受的定性评价标准</li><li>医学图像分析对高维度的图像需求也将进一步增加</li><li>从多模态图像出发生成综合性单模态图像的研究</li><li>GAN扩充的医学图像缺乏可解释性</li><li>医学图像数据集扩充领域可以吸收其它领域的优秀成果</li><li>解决标注数据集获取困难的问题</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 图像生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文阅读】Automated diagnosis of bone metastasis based on multi-view bone scans using attention-augmented deep neural networks</title>
      <link href="/spect-adob/"/>
      <url>/spect-adob/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract">Abstract</h1><hr><h3 id="背景：">背景：</h3><p>手动分析骨显像图像需要丰富的经验，已有骨显像图像的自动或半自动诊断方法步骤复杂且在小数据集上验证不足，准确性和可靠性较低。</p><h3 id="本文贡献：">本文贡献：</h3><p>描述了一个深度卷积神经网络的方法，该方法有两个主要创新点：首先，通过联合分析前后视图进行诊断，从而获得较高的准确性。其次，提出了一种空间注意特征聚集算子来增强空间位置信息。</p><h3 id="结果：">结果：</h3><p>高分类准确率证明了所提出的体系结构对骨显像图像诊断的有效性，可作为临床决策支持工具应用。</p><h1 id="1-Introduction">1. Introduction</h1><hr><h3 id="背景：-2">背景：</h3><p>全身骨扫描（WBS）在骨转移的鉴别诊断中与MRI具有相似的性能，但其成本远低于MRI</p><ul><li>磁共振成像（magnetic resonance imaging，MRI）</li><li>计算机断层扫描（computed tomography，CT）</li><li>全身骨扫描（whole-body bone scan，WBS）</li></ul><p>WBS图像中的异常称为热点（hot spot），通常表现为比周围环境更高的信号强度。但没有骨转移的患者也可以在WBS图像上显示热点。</p><p>骨转移的自动诊断方法的发展面临以下几个障碍：</p><ol><li>各种非肿瘤性疾病在影像学表现上也表现出异常，导致高灵敏度和低特异性。</li><li>为了在不同场景下获得较强的泛化能力，需要一个大的带注释数据集来学习骨转移的特征。然而，以往骨扫描相关研究中使用的数据集均不能满足这一要求。</li><li>每个WBS检查包含两个显示前后视图的图像。要分析是否存在骨转移，模型必须将两种观点作为一个单一的检查进行联合分析。</li></ol><h3 id="解决方法：">解决方法：</h3><ol><li>使用深度卷积神经网络（CNNs）从数据中自动提取高级特征。</li><li>构建了一个由专业核医学医师注释的大规模WBS图像数据集。</li><li>提出了一种接收多个输入的新结构，用于联合分析来自前后视图的图像。</li></ol><h3 id="贡献：">贡献：</h3><ol><li>构建了一个包含15474个核医学专业医师标记的大规模WBS图像数据集。</li><li>提出了一种基于多视点图像的骨转移瘤自动诊断模型。</li><li>提出了一种由深度神经网络参数化的特征聚合算子，用于约束检查前后视图的特征。</li><li>分类和可视化结果表明，所提出的方法成功地掌握了骨转移瘤的WBS图像特征。</li></ol><h1 id="2-Related-work">2. Related work</h1><hr><ul><li>计算机辅助诊断系统（computer-aided diagnosis systems ，CAD）</li></ul><h2 id="2-1-Traditional-methods-for-bone-scan-analyzing——传统的骨扫描分析方法">2.1 Traditional methods for bone scan analyzing——传统的骨扫描分析方法</h2><hr><h3 id="主要内容：">主要内容：</h3><ol><li>WBS图像的分析主要集中在三个方面：骨扫描指数（BSI）的自动计算、骨转移瘤的自动诊断和热点分割。然而，BSI的计算只是半自动的，并且需要费力的手动过程。</li><li>这些方法的分类性能在很大程度上依赖于热点分割，这意味着分割错误可能导致后续分类失败。此外，传统方法依赖于手工特征和阈值，因此缺乏鲁棒性。此外，人工选择特征是一种累人的、主观的、难以提高性能的方法。</li></ol><h2 id="2-2-Deep-neural-networks-for-bone-scan-analysis——深度神经网络在骨扫描分析中的应用">2.2 Deep neural networks for bone scan analysis——深度神经网络在骨扫描分析中的应用</h2><hr><h3 id="主要内容：-2">主要内容：</h3><ol><li>与传统的图像处理方法相比，Deep-cnn具有许多优点：它们以数据驱动的方式自动提取不同层次的特征，不需要手工构建特征，减少了医生的工作量。</li><li>虽然已经有一些研究对骨转移的自动诊断进行了研究，但这些研究中的大多数都是对首先需要从WBS图像分割的热点进行诊断，这一过程可能会引入额外的错误。</li></ol><h2 id="2-3-Multi-view-fusion——多视图融合">2.3 Multi-view fusion——多视图融合</h2><hr><h3 id="主要内容：-3">主要内容：</h3><ol><li>具有自然图像的图像分类任务通常一次只包含一个图像，而医学成像中的检查通常带有一组视图。</li><li>本文开发了一个使用多视图输入的自动化骨转移诊断框架。</li></ol><h1 id="3-Dataset">3. Dataset</h1><hr><p>数据标注后，将标注的考试分为训练集、验证集和测试集。</p><h2 id="3-1-Materials——资料">3.1 Materials——资料</h2><hr><h3 id="主要内容：-4">主要内容：</h3><ul><li>本研究所用WBS图像均来自四川大学华西医院核医学科。共收集记录16341份。</li><li>所有检查均使用两种设备中的一种进行，一种是分辨率为256×1024像素，另一种是分辨率为512×1024像素。</li><li>WBS图像被标记为恶性（malignant）或良性（benign）。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021007475.png" alt="image.png|400"></li></ul><h2 id="3-2-Data-annotation——数据批注">3.2 Data annotation——数据批注</h2><hr><h3 id="主要内容：-5">主要内容：</h3><ul><li>纳入13811例患者的15474项注释检查，包括9595例良性诊断和5879例恶性诊断。</li><li>与以往手动排除误导性示例的研究（Sadik等人，2006；2008）不同，本研究构建的数据集遵循真实世界分布，不排除任何案例，前提是在此数据集上训练的系统更适合常规临床应用。</li><li>表2和表3列出了数据集中主要病变的类型和发病率。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021007678.png" alt="image.png"></li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008015.png" alt="image.png"></li></ul><h2 id="3-3-Data-partition——数据分区">3.3 Data partition——数据分区</h2><hr><h3 id="主要内容：-6">主要内容：</h3><ul><li>分别使用12274、1600和1600个样本进行训练集、验证集和测试集。</li><li>开源</li></ul><h1 id="4-Methods">4. Methods</h1><hr><p>首先详细说明了所提出的WBS图像自动诊断体系结构，然后介绍了图像预处理方法。</p><h2 id="4-1-Overall-architecture——总体架构">4.1 Overall architecture——总体架构</h2><hr><h3 id="说明：">说明：</h3><ol><li>一个样本包含两个图像：一个后视图图像和一个前视图图像。</li><li>单个样本包含多个图像的数据集可以表示成为 $D = {(X_i, Y_i); i = 1, 2, \ldots, N}, \quad X_i = {x_{i,1}, \ldots, x_{i,j}, \ldots, x_{i,J}}$ ，Xi表示包含J个图像的数据集中的第i个样本，Xi,J是第i个样本中的第J个图像，Yi是样本Xi的对应标签。对于本研究使用的数据集，J=2，xi,1是后视图像，xi,2是前视图像，Yi∈{恶性，良性}。</li></ol><h3 id="总体架构概述">总体架构概述:</h3><ol><li>由于网络的输入不是单一的图像，而是J图像，因此我们开发了一个J路输入网络。</li><li>第一部分，采用 深度神经网络$N_{ex}$ 对输入的J图像进行特征提取，得到 $F_{i,J} = N_{ex}(x_{i,J})$ ，这里，xi,j是第i次检查中的第j幅图像，Fi,j表示通过网络Nex提取的xi,j的高级特征。</li><li>第二部分，使用 特征融合算子$N_{fu}$ ， $S_i = N_{fu}(F_{i,1}, \ldots, F_{i,j}, \ldots, F_{i,J})$ 融合第i个样本的高级特征。这里，Si表示融合特征。</li><li>最后，利用定制的 分类神经网络$N_{cl}$ 输出真实标签的预测值， $P_i = N_{cl}(S_i)$ ，Pi是应用与样本对应的softmax函数后的模型输出。</li><li>通过最小化训练集上的交叉熵代价函数，使用反向传播算法对所提出的体系结构进行训练，定义为 $L = - \sum_{i=1}^{N} Y_i^T \ln(P_i)$</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008888.png" alt="image.png"></p><h2 id="4-2-Part-one-feature-extraction-network——第一部分：特征提取网络">4.2 Part one: feature extraction network——第一部分：特征提取网络</h2><hr><h3 id="主要内容：-7">主要内容：</h3><p>探讨了几种经典CNN：Inception-V3，DenseNet和SENet</p><h2 id="4-3-Part-two-feature-fusion-operator——第二部分：特征融合算子">4.3 Part two: feature fusion operator——第二部分：特征融合算子</h2><hr><h3 id="主要内容：-8">主要内容：</h3><p>探讨了几种特征聚合策略：</p><ul><li>最大特征融合算子 $s_{c,w,h}^{i} = \max_{j=1,\ldots,J} (f_{c,w,h}^{i,j})$</li><li>平均特征融合算子 $s_{c,w,h}^{i} = \frac{1}{J} \sum_{j=1}^{J} f_{c,w,h}^{i,j}$</li><li>空间注意特征融合算子：高级特征在空间位置上加权，并通过求和算子进行聚合。</li></ul><p>空间注意特征融合算子：</p><ol><li>首先，将特征提取网络提取的高层特征Fi、j通过卷积层，产生空间位置描述符$Mi,j$，$m^{i,j}_{1,w,h}∈R^{1×W×H}$。这里，卷积层的核尺寸为1×1，输入通道和输出通道分别等于C和1。</li><li>然后，对Mi,j应用sigmoid函数，在空间位置上产生权重描述符$Qi,j$，$q^{i,j}<em>{1,w,h}∈R^{1×W ×H}$：$$q</em>{1,w,h}^{i,j} = \frac{1}{1 + e^{-m_{1,w,h}^{i,j}}}$$</li><li>最后，将Qi,j乘以Fi,j，并使用求和运算符聚合缩放嵌入：$$s_{c,w,h}^{i} = \sum_{j=1}^{J} \left( q_{1,w,h}^{i,j} \cdot f_{c,w,h}^{i,j} \right)$$</li><li>图4中描绘了该空间注意块的细节。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008874.png" alt="image.png"></p><h2 id="4-4-Part-three-classification-network——第三部分：分类网络">4.4 Part three: classification network——第三部分：分类网络</h2><hr><h3 id="主要内容：-9">主要内容：</h3><ol><li>设计了一个自定义的标准深度神经网络（SDNN）作为网络最后一部分的分量分类器，将第二部分产生的融合特征映射输入SDNN进行最终预测。</li><li>首先采用全局池层对特征地图的空间尺寸进行归一化。</li><li>在其后面添加一个全连接层，在全局池层之后添加一个dropout层，以缓解网络过度拟合。drop概率设置为0.7。</li><li>在dropout层之后是几个模块，每个模块是三个连续操作的组合：全连接层、批量归一化（BN）和泄漏校正线性单元（LeakyReLU）。所提出的SDNN的架构如图3所示。</li></ol><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021008670.png" alt="image.png|200"></p><h2 id="4-5-Image-pre-processing——图像预处理">4.5 Image pre-processing——图像预处理</h2><hr><h3 id="主要内容：-10">主要内容：</h3><ol><li>在输入到模型之前，使用windowWidth设置为47，windowCenter设置为23.5，将HU值转换为范围为[0, 255]的灰度图像。</li><li>颜色均反转</li><li>提出了一种基于阈值分割的感兴趣区域（ROI）提取算法，从原始图像中提取有效区域。提取的图像分辨率为201×690～975×253像素，高宽比为2.7～4.1。</li><li>将所有图像填充到一个统一的高宽比$R_{hw}$来标准化分辨率，并将它们调整到一个统一的大小，以便图像的较小边缘等于256。</li><li>在本研究中，比率$R_{hw}$固定为3.4，即整个数据集的平均比率，阈值th固定为10。预处理后的图像分辨率为256×846，几乎没有黑边。</li></ol><h1 id="5-Experimental-and-results">5. Experimental and results</h1><hr><p>首先详细描述了实验配置，包括所提出方法的实现、评估策略和评估度量。然后给出了实验结果和分析。</p><h2 id="5-1-Experimental-configuration——实验配置">5.1 Experimental configuration——实验配置</h2><hr><h3 id="Implementation—实现：">Implementation—实现：</h3><p>所有特征提取网络都在ImageNet数据集上预先训练，然后使用adadelta作为优化器在数据集上进行微调，学习率为0.1，权重衰减率为$10^{-4}$，适用于200个epoch。</p><h3 id="Evaluation-Metrics—评价指标：">Evaluation Metrics—评价指标：</h3><p>在测试集上评估每个模型的总体性能，使用达到最高精度的验证集。以敏感性、特异性、准确性和F1评分作为评价指标。</p><h3 id="Evaluation-Strategy—评价策略：">Evaluation Strategy—评价策略：</h3><ol><li>探讨了预处理方法对模型的影响</li><li>比较了现有的图像网络预训练网络作为特征提取网络的性能</li><li>分析了空间注意块的有效性</li><li>将模型与三位有经验的医师进行了比较，进一步验证了方法的有效性</li></ol><h2 id="5-2-Input-methods——输入方法">5.2 Input methods——输入方法</h2><hr><h3 id="三种输入方法：">三种输入方法：</h3><ul><li>A）直接将预处理前后图像的大小调整为256×256。该模型输入两幅图像，分辨率为256×256。</li><li>B） 将经过预处理的前后图像直接输入到模型中，改变最终池层的核大小，使池层的输出宽度和高度等于1</li><li>C）直接将预处理的前后图像输入模型，并用空间金字塔池（SPP）层替换模型中的最终池层。</li><li>这三个实验都使用Inception-V3作为特征提取网络，并使用max特征融合算子。</li><li>实验结果如表5所示。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021012510.png" alt="image.png"></li></ul><h2 id="5-3-Feature-aggregation-methods——特征融合方法">5.3 Feature aggregation methods——特征融合方法</h2><hr><p>表6给出了特征提取网络的不同架构和不同特征聚合方法的比较。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021012303.png" alt="image.png"></p><p>从表中可以看出，结合空间注意算子的Inception-V3表现最好。<br>因此，本文使用了一个带有空间注意特征聚合算子的Inception-V3特征提取网络作为最终架构</p><h2 id="5-4-Multi-view-versus-single-view——多视图与单视图">5.4 Multi-view versus single view——多视图与单视图</h2><hr><p>结果如表7所示。<br>以后视图为输入的网络性能优于前视图，表明后视图比前视图包含更多的信息。<br>此外，多视点融合网络的性能比单纯的前后视点融合网络有了很大的提高。</p><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021013413.png" alt="image.png"></p><h2 id="5-5-Visualization——可视化">5.5 Visualization——可视化</h2><hr><p><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021014534.png" alt="image.png"></p><ul><li>使用引导反向传播算法，引导反向传播算法计算梯度与最活跃的输出层有关的输入。</li><li>前两幅图像为真阳性病例及其相应的可视化结果。可以清楚地观察到网络中的最大输出神经元与输入图像中的热点区域高度相关。</li><li>后两幅图像均为假阳性病例及相应的可视化结果。大多数假阳性病例是非典型样本，尽管预测是错误的，但该模型仍然能够聚焦图像中的热点区域。</li></ul><h2 id="5-6-Model-ensemble-and-clinical-test——模型集成与临床试验">5.6 Model ensemble and clinical test——模型集成与临床试验</h2><hr><ul><li>集成学习是提高单个模型独立训练的性能的有效方法。</li><li>集成模型的最终预测得分为所有模型的平均softmax得分。</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021014526.png" alt="image.png"></li></ul><h2 id="5-7-Comparisons-between-the-model-and-experts——模型与专家的比较">5.7 Comparisons between the model and experts——模型与专家的比较</h2><hr><ul><li>将其性能与三位核医学医师进行了比较。</li><li>这三位专家可分为三个层次：无经验（&lt;800 WBS解释）、中等经验（800-5000 WBS解释）和经验丰富（&gt;5000 WBS解释）</li><li><img src="https://raw.githubusercontent.com/YanQinglei/blog-img/main/img202409021017879.png" alt="image.png"></li></ul><h2 id="5-8-Analytic-experiment——分析性实验">5.8 Analytic experiment——分析性实验</h2><hr><p>所提出的方法具有良好的性能，对于检查次数较少的类型，模型仍显示出显著的召回率。</p><h1 id="6-Conclusion">6. Conclusion</h1><hr><h3 id="主要内容：-11">主要内容：</h3><ul><li>网络结构基于深度卷积神经网络，由特征提取网络、特征聚合网络和特征分类网络三部分组成，并对几种数据输入方法进行了比较。</li><li>研究了三种最先进的图像网络预训练网络作为特征提取网络：Inception-V3、DenseNet-169和SE-ResNet-50。</li><li>构建了一个包含15474个检查项的大规模带注释WBS图像数据集来训练和评估所提出的模型，表现出优越的性能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 分割 </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPECT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world/"/>
      <url>/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
